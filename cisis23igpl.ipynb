{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAUa5zNamPFO"
   },
   "source": [
    "# Description of Experiment\n",
    "\n",
    "\n",
    "The most recent version of this notebook is at https://github.com/nickjeffrey/cisis23igpl\n",
    "\n",
    "This jupyter notebook builds on previous works at https://github.com/nickjeffrey/ensemble_learning\n",
    "\n",
    "This notebook explores the use of Deep Learning classifiers, which are then fed to an Ensemble Learning model to see if the accuracy can be improved.\n",
    "\n",
    "This notebook used the following hardware / software resources:  Intel Core i7-10710U (6 core, 12 thread), 64GB RAM, 1TB SSD, Windows 11, Sci-kit Learn, Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuA1Zf3RTJxL"
   },
   "source": [
    "# Definitions:\n",
    "\n",
    "In the context of neural network models, the test loss and test accuracy are performance metrics used to evaluate the model's performance on unseen data, specifically the test set.\n",
    "\n",
    "Test Loss:\n",
    "\n",
    "- The test loss measures how well the model is performing on the test set. It represents the average loss (e.g., cross-entropy loss) incurred by the model when making predictions on the test data.\n",
    "- Lower test loss indicates better performance, as it means that the model's predictions are closer to the actual labels.\n",
    "However, it's important to consider the scale and nature of the loss function used. For instance, a test loss of 0.1 might be good for one problem but poor for another, depending on the context.\n",
    "\n",
    "\n",
    "Test Accuracy:\n",
    "\n",
    "- The test accuracy measures the proportion of correctly classified samples in the test set.\n",
    "It is calculated by dividing the number of correctly classified samples by the total number of samples in the test set.\n",
    "- Higher test accuracy indicates better performance, as it means that the model is making more correct predictions.\n",
    "However, accuracy alone might not provide a complete picture, especially if the classes are imbalanced or if different types of errors have different costs.\n",
    "\n",
    "\n",
    "In summary, test loss and test accuracy are two important metrics used to assess the performance of a SimpleRNN model on unseen data. Lower test loss and higher test accuracy generally indicate better performance, but it's essential to consider other factors such as the nature of the problem, class imbalance, and potential costs associated with different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WgmmRvd78t_"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5pKfxMf7-VV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing   import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "# Miscellaneous packages\n",
    "import time                                           #for calculating elapsed time for training tasks\n",
    "import os                                             #for checking if file exists\n",
    "import socket                                         #for getting FQDN of local machine\n",
    "import math                                           #square root function\n",
    "import sys\n",
    "\n",
    "\n",
    "# Packages from scikit-learn\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV       #for hyperparameter optimization\n",
    "from sklearn.model_selection import cross_val_score    #for cross fold validation\n",
    "from sklearn.metrics         import make_scorer        #used by GridSearchCV\n",
    "from sklearn.metrics         import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.naive_bayes     import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "from sklearn.tree            import DecisionTreeClassifier\n",
    "from sklearn.ensemble        import RandomForestClassifier\n",
    "from sklearn.neural_network  import MLPClassifier      #neural network classifier\n",
    "from sklearn.ensemble        import BaggingClassifier, VotingClassifier, StackingClassifier, AdaBoostClassifier, GradientBoostingClassifier   #Packages for Ensemble Learning\n",
    "\n",
    "# packages for balancing classes\n",
    "from imblearn.under_sampling import RandomUnderSampler  #may need to install with: conda install -c conda-forge imbalanced-learn\n",
    "from imblearn.over_sampling  import SMOTE               #may need to install with: conda install -c conda-forge imbalanced-learn\n",
    "\n",
    "# Deep Learning classifiers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models     import Sequential\n",
    "from tensorflow.keras.layers     import Dense, Dropout, LSTM, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses     import binary_crossentropy\n",
    "from tensorflow.keras.metrics    import Accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "of8Ey5Z34mhL"
   },
   "outputs": [],
   "source": [
    "# # KerasClassifier was moved to scikeras in version 2.13.0, so you will need to install the package, but this will break other things!\n",
    "\n",
    "# import importlib.util\n",
    "\n",
    "# # Check if scikeras is installed\n",
    "# if importlib.util.find_spec(\"scikeras\") is None:\n",
    "#   print(\"scikeras is not installed, attempting installation now.\")\n",
    "#   !pip install scikeras\n",
    "# else:\n",
    "#   print(\"scikeras is already installed.\")\n",
    "\n",
    "\n",
    "# # after confirming the scikeras package was installed, you can now import KerasClassifier,\n",
    "# #  which is used for SimpleRNN hyperparameter optimization\n",
    "# from scikeras.wrappers import KerasClassifier, KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNNJ8GQLL0oj"
   },
   "outputs": [],
   "source": [
    "# WARNING: do not use tensorflow.keras.wrappers.scikit_learn\n",
    "# DEPRECATED. Use [Sci-Keras](https://github.com/adriangb/scikeras) instead.\n",
    "# See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
    "\n",
    "\n",
    "# import pkg_resources\n",
    "\n",
    "# # Get a list of installed packages and their versions\n",
    "# installed_packages = {package.key: package.version for package in pkg_resources.working_set}\n",
    "\n",
    "# # Uninstall TensorFlow if installed version is greater than 2.12.0\n",
    "# if 'tensorflow' in installed_packages and installed_packages['tensorflow'] > '2.12.0':\n",
    "#   pip.main(['uninstall', '-y', 'tensorflow'])\n",
    "#   print(\"TensorFlow uninstalled successfully\")\n",
    "# else:\n",
    "#   print(\"Did not find a version of TensorFlow greater than 2.12.0\")\n",
    "\n",
    "\n",
    "\n",
    "# # Check if TensorFlow is installed and its version is greater than 2.12.0\n",
    "# if 'tensorflow' in installed_packages and installed_packages['tensorflow'] == '2.12.0':\n",
    "#   print(\"TensorFlow 2.12.0 is already installed\")\n",
    "# else:\n",
    "#   print(\"Installing TensorFlow 2.12.0\")\n",
    "#   pip.main(['install', 'tensorflow==2.12.0'])\n",
    "\n",
    "\n",
    "\n",
    "# # At this point, tensorflow 2.12.0 is installed, so import the package we want\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# # Print the installed version of KerasClassifier\n",
    "# #print(\"Installed version of KerasClassifier:\", KerasClassifier.__version__)\n",
    "# KerasClassifier\n",
    "\n",
    "# #!pip uninstall -y tensorflow\n",
    "# #!pip install tensorflow==2.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl0E2qi1j-L_"
   },
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuu4rrIgkAnW"
   },
   "outputs": [],
   "source": [
    "# function to show missing values in dataset\n",
    "\n",
    "def get_type_missing(df):\n",
    "    df_types = pd.DataFrame()\n",
    "    df_types['data_type'] = df.dtypes\n",
    "    df_types['missing_values'] = df.isnull().sum()\n",
    "    return df_types.sort_values(by='missing_values', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vuds3_rnkAvm"
   },
   "outputs": [],
   "source": [
    "# function to create a confusion matrix\n",
    "\n",
    "def visualize_confusion_matrix(y_test, y_pred):\n",
    "    #\n",
    "    ## Calculate accuracy\n",
    "    #accuracy = accuracy_score(y_test, y_pred)\n",
    "    #print(\"Accuracy:\", accuracy)\n",
    "    #\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #\n",
    "    # visualize confusion matrix with more detailed labels\n",
    "    # https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "    #\n",
    "    group_names = ['True Negative','False Positive','False Negative','True Positive']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure(figsize=(3.5, 2.0))  #default figsize is 6.4\" wide x 4.8\" tall, shrink to 3.5\" wide 2.0\" tall\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', cbar=False)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # use the .ravel function to pull out TN,TP,FN,TP\n",
    "    # https://analytics4all.org/2020/05/07/python-confusion-matrix/\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "    # calculate different metrics\n",
    "    Accuracy = (( TP + TN) / ( TP + TN + FP + FN))\n",
    "    Sensitivity = TP / (TP + FN)\n",
    "    Specificity = TN / (TN + FP)\n",
    "    GeometricMean = math.sqrt(Sensitivity * Specificity)\n",
    "\n",
    "    # Precision is the ratio of true positive predictions to the total number of positive predictions made by the model\n",
    "    # average=binary for  binary classification models, average=micro for multiclass classification, average=weighted to match classification_report\n",
    "    Precision = precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Recall is the ratio of true positive predictions to the total number of actual positive instances in the data.\n",
    "    # average=binary for  binary classification models, average=micro for multiclass classification, average=weighted to match classification_report\n",
    "    Recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # F1-score is a metric that considers both precision and recall, providing a balance between the two.\n",
    "    # average=binary for  binary classification models, average=micro for multiclass classification, average=weighted to match classification_report\n",
    "    F1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # add details below graph to help interpret results\n",
    "    print('\\n\\n')\n",
    "    print('Confusion matrix\\n\\n', cm)\n",
    "    print('\\nTrue Negatives  (TN) = ', TN)\n",
    "    print('False Positives (FP) = ', FP)\n",
    "    print('False Negatives (FN) = ', FN)\n",
    "    print('True Positives  (TP) = ', TP)\n",
    "    print ('\\n')\n",
    "    print (\"Accuracy:       \", Accuracy)\n",
    "    print (\"Sensitivity:    \", Sensitivity)\n",
    "    print (\"Specificity:    \", Specificity)\n",
    "    print (\"Geometric Mean: \", GeometricMean)\n",
    "    print ('\\n')\n",
    "    print (\"Precision:       \", Precision)\n",
    "    print (\"Recall:          \", Recall)\n",
    "    print (\"f1-score:        \", F1)\n",
    "\n",
    "    print('\\n------------------------------------------------\\n')\n",
    "    # We want TN and TP to be approximately equal, because this indicates the dataset is well balanced.\n",
    "    # If TN and TP are very different, it indicates imbalanced data, which can lead to low accuracy due to overfitting\n",
    "    #if (TN/TP*100 < 40 or TN/TP*100 > 60):   #we want TN and TP to be approximately 50%, if the values are below 40% or over 60%, generate a warning\n",
    "    #    print(\"WARNING: the confusion matrix shows that TN and TP are very imbalanced, may lead to low accuracy!\")\n",
    "    #\n",
    "    return cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cw4bBmYtkAx-"
   },
   "outputs": [],
   "source": [
    "# function to report on model accuracy (TP, FP, FN, FP), precision, recall, f1-score\n",
    "# this function does not provide anything additional to the results from the previous function\n",
    "\n",
    "def model_classification_report(cm, y_test, y_pred):\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "    print('\\n')\n",
    "    print(\"Classification Report: \\n\", report)\n",
    "    print('\\n\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DHEnelKtkA0V",
    "outputId": "bb87d07d-10f5-46d4-c09a-35f8efc721be"
   },
   "outputs": [],
   "source": [
    "# function to show elapsed time for running notebook\n",
    "\n",
    "# start a timer so we can calculate the total runtime of this notebook\n",
    "notebook_start_time = time.time()  #seconds since epoch\n",
    "\n",
    "def show_elapsed_time():\n",
    "    #\n",
    "    # Get the current time as a struct_time object\n",
    "    current_time_struct = time.localtime()\n",
    "\n",
    "    # Format the struct_time as a string (yyyy-mm-dd HH:MM:SS format)\n",
    "    current_time_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", current_time_struct)\n",
    "\n",
    "    # Display the current time in HH:MM:SS format\n",
    "    print(\"Current Time:\", current_time_str)\n",
    "\n",
    "    # show a running total of elapsed time for the entire notebook\n",
    "    notebook_end_time = time.time()  #seconds since epoch\n",
    "    print(f\"The entire notebook runtime so far is {(notebook_end_time-notebook_start_time)/60:.0f} minutes\")\n",
    "\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_NPoDEZb8sc"
   },
   "outputs": [],
   "source": [
    "# create a function to plot Training Loss and Validation Loss for Neural Network classifiers\n",
    "\n",
    "def plot_loss_history(train_loss_history_unoptimized, train_loss_history_optimized, val_loss_history_unoptimized, val_loss_history_optimized, epoch_count):\n",
    "    if len(train_loss_history_unoptimized) == 0 or len(train_loss_history_optimized) == 0 or len(val_loss_history_unoptimized) == 0 or len(val_loss_history_optimized) == 0:\n",
    "        print(\"ERROR: cannot find loss history, using all zeros\")\n",
    "        raise ValueError (\"ERROR: Cannot find loss history, please investigate!\")\n",
    "    #\n",
    "    # sanity check to confirm the same number of epochs were used before and after hyperparameter optimization\n",
    "    if len(train_loss_history_unoptimized) != len(train_loss_history_optimized) or len(val_loss_history_unoptimized) != len(val_loss_history_optimized):\n",
    "        raise ValueError (\"ERROR: different number of epochs used before and after hyperparameter optimziation\")\n",
    "\n",
    "    # Create two subplots side by side\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))  # Create two subplots side by side\n",
    "\n",
    "    # Plot for loss before optimization\n",
    "    axs[0].plot(train_loss_history_unoptimized, label='Train Loss before optimizing', color='blue', linestyle='--')\n",
    "    axs[0].plot(val_loss_history_unoptimized, label='Validation Loss before optimizing', color='red', linestyle='--')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].set_title('Train / Val Loss before optimization')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Plot for loss after optimization\n",
    "    axs[1].plot(train_loss_history_optimized, label='Train Loss after optimizing', color='blue', linestyle='-')\n",
    "    axs[1].plot(val_loss_history_optimized, label='Validation Loss after optimizing', color='red', linestyle='-')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_title('Train / Val Loss after optimization')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "\n",
    "    # the lines should be very close to converging by the final epoch\n",
    "    # since list indexes start at 0 instead of 1, use epoch_count-1 as the index for the last value\n",
    "    # maximum acceptable divergence\n",
    "    divergence_threshold = 0.01\n",
    "    divergence_unoptimized = abs(train_loss_history_unoptimized[epoch_count-1] - val_loss_history_unoptimized[epoch_count-1])\n",
    "    divergence_optimized   = abs(train_loss_history_optimized[epoch_count-1]   - val_loss_history_optimized[epoch_count-1])\n",
    "    if divergence_optimized > divergence_threshold:\n",
    "        print(f\"WARNING, after optimization, loss divergence is {divergence_optimized}, which greater than threshold of {divergence_threshold}, please continue tuning this model.\")\n",
    "    else:\n",
    "        print(f\"After optimization, loss divergence is {divergence_optimized:.3f}, which is <= threshold of {divergence_threshold}, this model has acceptable loss.\")\n",
    "\n",
    "# Example usage:\n",
    "#epoch_count = 5\n",
    "#train_loss_history_unoptimized = [1, 2, 3, 4, 5]\n",
    "#train_loss_history_optimized = [1, 2, 3, 4, 5]\n",
    "#val_loss_history_unoptimized = [1, 2, 3, 4, 5]\n",
    "#val_loss_history_optimized = [1, 2, 3, 4, 5]\n",
    "#plot_loss_history(train_loss_history_unoptimized, train_loss_history_optimized, val_loss_history_unoptimized, val_loss_history_optimized, epoch_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpHC4O9Gb80p"
   },
   "outputs": [],
   "source": [
    "# create a function to plot Training Accuracy and Validation Accuracy for Neural Network classifiers\n",
    "\n",
    "def plot_accuracy_history(train_accuracy_history_unoptimized, train_accuracy_history_optimized, val_accuracy_history_unoptimized, val_accuracy_history_optimized, epoch_count):\n",
    "    # sanity check to confirm the required values have already been collected\n",
    "    if len(train_accuracy_history_unoptimized) == 0 or len(train_accuracy_history_optimized) == 0 or len(val_accuracy_history_unoptimized) == 0 or len(val_accuracy_history_optimized) == 0:\n",
    "        print(\"ERROR: cannot find accuracy history\")\n",
    "        raise ValueError (\"ERROR: Cannot find accuracy history, please investigate!\")\n",
    "    # sanity check to confirm the same number of epochs were used before and after hyperparameter optimization\n",
    "    if len(train_accuracy_history_unoptimized) != len(train_accuracy_history_optimized) or len(val_accuracy_history_unoptimized) != len(val_accuracy_history_optimized):\n",
    "        raise ValueError (\"ERROR: different number of epochs used before and after hyperparameter optimziation\")\n",
    "\n",
    "\n",
    "    # Create two subplots side by side\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))  # Create two subplots side by side\n",
    "\n",
    "    # Plot for accuracy before optimization\n",
    "    axs[0].plot(train_accuracy_history_unoptimized, label='Train Accuracy before optimizing',      color='blue', linestyle='--')\n",
    "    axs[0].plot(val_accuracy_history_unoptimized,   label='Validation Accuracy before optimizing', color='red',  linestyle='--')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_title('Train / Val Accuracy before optimization')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Plot for accuracy after optimization\n",
    "    axs[1].plot(train_accuracy_history_optimized, label='Train Accuracy after optimizing',      color='blue', linestyle='-')\n",
    "    axs[1].plot(val_accuracy_history_optimized,   label='Validation Accuracy after optimizing', color='red',  linestyle='-')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].set_title('Train / Val Accuracy after optimization')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "    plt.show()\n",
    "\n",
    "    # the lines should be very close to converging by the final epoch\n",
    "    # since list indexes start at 0 instead of 1, use epoch_count-1 as the index for the last value\n",
    "    # maximum acceptable divergence\n",
    "    divergence_threshold = 0.01\n",
    "    divergence_unoptimized = abs(train_accuracy_history_unoptimized[epoch_count-1] - val_accuracy_history_unoptimized[epoch_count-1])\n",
    "    divergence_optimized   = abs(train_accuracy_history_optimized[epoch_count-1]   - val_accuracy_history_optimized[epoch_count-1])\n",
    "    if divergence_optimized > divergence_threshold:\n",
    "        print(f\"WARNING, after optimization, accuracy divergence is {divergence_optimized}, which greater than threshold of {divergence_threshold}, please continue tuning this model.\")\n",
    "    else:\n",
    "        print(f\"After optimization, accuracy divergence is {divergence_optimized:.3f}, which is <= threshold of {divergence_threshold}, this model has acceptable accuracy.\")\n",
    "\n",
    "# Example usage:\n",
    "#epoch_count = 10\n",
    "#train_accuracy_history_unoptimized = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0.5]\n",
    "#train_accuracy_history_optimized = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0.4]\n",
    "#val_accuracy_history_unoptimized = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0.5]\n",
    "#val_accuracy_history_optimized = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0.4]\n",
    "#plot_accuracy_history(train_accuracy_history_unoptimized, train_accuracy_history_optimized, val_accuracy_history_unoptimized, val_accuracy_history_optimized, epoch_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhqIJe_Lkbtj"
   },
   "source": [
    "# Initialize variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ioPpjRGmkpVg"
   },
   "outputs": [],
   "source": [
    "# initialize variables to avoid undef errors\n",
    "\n",
    "accuracy_lr_unoptimized               = 0\n",
    "accuracy_lr_optimized                 = 0\n",
    "accuracy_nb_unoptimized               = 0\n",
    "accuracy_nb_optimized                 = 0\n",
    "accuracy_knn_unoptimized              = 0\n",
    "accuracy_knn_optimized                = 0\n",
    "accuracy_svm_unoptimized              = 0\n",
    "accuracy_svm_optimized                = 0\n",
    "accuracy_dt_unoptimized               = 0\n",
    "accuracy_dt_optimized                 = 0\n",
    "accuracy_rf_unoptimized               = 0\n",
    "accuracy_rf_optimized                 = 0\n",
    "accuracy_gb_unoptimized               = 0\n",
    "accuracy_gb_optimized                 = 0\n",
    "accuracy_mlp_unoptimized              = 0\n",
    "accuracy_mlp_optimized                = 0\n",
    "accuracy_fnn_unoptimized              = 0\n",
    "accuracy_fnn_optimized                = 0\n",
    "accuracy_sequential_unoptimized       = 0\n",
    "accuracy_sequential_optimized         = 0\n",
    "accuracy_cnn_unoptimized              = 0\n",
    "accuracy_cnn_optimized                = 0\n",
    "accuracy_rnn_unoptimized              = 0\n",
    "accuracy_rnn_optimized                = 0\n",
    "accuracy_lstm_unoptimized             = 0\n",
    "accuracy_lstm_optimized               = 0\n",
    "accuracy_gru_unoptimized              = 0\n",
    "accuracy_gru_optimized                = 0\n",
    "\n",
    "\n",
    "\n",
    "test_accuracy_sequential_unoptimized       = 0\n",
    "test_loss_sequential_unoptimized           = 0\n",
    "train_accuracy_sequential_unoptimized      = 0\n",
    "train_loss_sequential_unoptimized          = 0\n",
    "test_accuracy_sequential_optimized         = 0\n",
    "test_loss_sequential_optimized             = 0\n",
    "train_accuracy_sequential_optimized        = 0\n",
    "train_loss_sequential_optimized            = 0\n",
    "val_accuracy_sequential_unoptimized        = 0\n",
    "val_loss_sequential_unoptimized            = 0\n",
    "val_accuracy_sequential_optimized          = 0\n",
    "val_loss_sequential_optimized              = 0\n",
    "\n",
    "test_accuracy_lstm_unoptimized             = 0\n",
    "test_loss_lstm_unoptimized                 = 0\n",
    "train_accuracy_lstm_unoptimized            = 0\n",
    "train_loss_lstm_unoptimized                = 0\n",
    "test_accuracy_lstm_optimized               = 0\n",
    "test_loss_lstm_optimized                   = 0\n",
    "train_accuracy_lstm_optimized              = 0\n",
    "train_loss_lstm_optimized                  = 0\n",
    "val_accuracy_lstm_unoptimized              = 0\n",
    "val_loss_lstm_unoptimized                  = 0\n",
    "val_accuracy_lstm_optimized                = 0\n",
    "val_loss_lstm_optimized                    = 0\n",
    "\n",
    "test_accuracy_simplernn_unoptimized        = 0\n",
    "test_loss_simplernn_unoptimized            = 0\n",
    "train_accuracy_simplernn_unoptimized       = 0\n",
    "train_loss_simplernn_unoptimized           = 0\n",
    "test_accuracy_simplernn_optimized          = 0\n",
    "test_loss_simplernn_optimized              = 0\n",
    "train_accuracy_simplernn_optimized         = 0\n",
    "train_loss_simplernn_optimized             = 0\n",
    "val_accuracy_simplernn_unoptimized         = 0\n",
    "val_loss_simplernn_unoptimized             = 0\n",
    "val_accuracy_simplernn_optimized           = 0\n",
    "val_loss_simplernn_optimized               = 0\n",
    "\n",
    "test_accuracy_simplernn_lstm_unoptimized   = 0\n",
    "test_loss_simplernn_lstm_unoptimized       = 0\n",
    "train_accuracy_simplernn_lstm_unoptimized  = 0\n",
    "train_loss_simplernn_lstm_unoptimized      = 0\n",
    "test_accuracy_simplernn_lstm_optimized     = 0\n",
    "test_loss_simplernn_lstm_optimized         = 0\n",
    "train_accuracy_simplernn_lstm_optimized    = 0\n",
    "train_loss_simplernn_lstm_optimized        = 0\n",
    "val_accuracy_simplernn_lstm_unoptimized    = 0\n",
    "val_loss_simplernn_lstm_unoptimized        = 0\n",
    "val_accuracy_simplernn_lstm_optimized      = 0\n",
    "val_loss_simplernn_lstm_optimized          = 0\n",
    "\n",
    "\n",
    "test_accuracy_gru_unoptimized              = 0\n",
    "test_loss_gru_unoptimized                  = 0\n",
    "train_accuracy_gru_unoptimized             = 0\n",
    "train_loss_gru_unoptimized                 = 0\n",
    "test_accuracy_gru_optimized                = 0\n",
    "test_loss_gru_optimized                    = 0\n",
    "train_accuracy_gru_optimized               = 0\n",
    "train_loss_gru_optimized                   = 0\n",
    "val_accuracy_gru_unoptimized               = 0\n",
    "val_loss_gru_unoptimized                   = 0\n",
    "val_accuracy_gru_optimized                 = 0\n",
    "val_loss_gru_optimized                     = 0\n",
    "\n",
    "best_params_mlp                       = \"\"\n",
    "best_params_sequential                = \"\"\n",
    "best_params_lstm                      = \"\"\n",
    "best_params_simplernn                 = \"\"\n",
    "best_params_simplernn_lstm            = \"\"\n",
    "best_params_gru                       = \"\"\n",
    "\n",
    "\n",
    "accuracy_ensemble_voting              = 0\n",
    "accuracy_ensemble_stacking            = 0\n",
    "accuracy_ensemble_boosting            = 0\n",
    "accuracy_ensemble_bagging             = 0\n",
    "\n",
    "\n",
    "cv_count                              = 10  #number of cross-validation folds\n",
    "epoch_count                           = 25  #number of epochs for neural network classifiers\n",
    "epoch_verbosity                       = 0   #For NN classifiers with epochs. 0=silent training, 1 will show progress bars, 2 will provide a concise summary per epoch.\n",
    "verbosity                             = 0   # for MLP, LR, DT classifiers.\n",
    "\n",
    "train_loss_history_unoptimized     = [0] * epoch_count\n",
    "train_loss_history_optimized       = [0] * epoch_count\n",
    "val_loss_history_unoptimized       = [0] * epoch_count\n",
    "val_loss_history_optimized         = [0] * epoch_count\n",
    "\n",
    "train_accuracy_history_unoptimized = [0] * epoch_count\n",
    "train_accuracy_history_optimized   = [0] * epoch_count\n",
    "val_accuracy_history_unoptimized   = [0] * epoch_count\n",
    "val_accuracy_history_optimized     = [0] * epoch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rko_Sv99RJoB"
   },
   "outputs": [],
   "source": [
    "# # Load pickled datasets\n",
    "\n",
    "# # Determine the best location to obtain the source *.pkl file\n",
    "\n",
    "# # define *.pkl source file\n",
    "# filename = 'Edge-IIoTset2023_scaled_data_tuple.pkl'\n",
    "# LAN_location = 'http://datasets.nyx.local:80/datasets/Edge-IIoTset2023/Selected_dataset_for_ML_and_DL'  #high speed local copy on LAN\n",
    "# WAN_location = 'http://datasets.nyx.ca:8081/datasets/Edge-IIoTset2023/Selected_dataset_for_ML_and_DL'   #accessible to entire internet\n",
    "\n",
    "# #filename = 'CIC_IOT_Dataset2023_scaled_data_tuple.pkl'\n",
    "# #LAN_location = 'http://datasets.nyx.local:80/datasets/CIC_IOT_Dataset2023/csv'  #high speed local copy on LAN\n",
    "# #WAN_location = 'http://datasets.nyx.ca:8081/datasets/CIC_IOT_Dataset2023/csv'   #accessible to entire internet\n",
    "\n",
    "# # Get the FQDN of the local machine\n",
    "# fqdn = socket.getfqdn()\n",
    "# ipv4_address = socket.gethostbyname(socket.gethostname())\n",
    "# print(f\"Fully Qualified Domain Name (FQDN):{fqdn}, IPv4 address:{ipv4_address}\")\n",
    "# if ( \"nyx.local\" in fqdn ):\n",
    "#     # If inside the LAN, grab the local copy of the dataset\n",
    "#     print(f\"Detected Fully Qualified Domain Name of {fqdn}, dataset source is:\\n{LAN_location}/{filename}\")\n",
    "#     dataset = f\"{LAN_location}/{filename}\"\n",
    "# else:\n",
    "#     # If not inside the LAN, grab the dataset from an internet-accessible URL\n",
    "#     print(f\"Detected Fully Qualified Domain Name of {fqdn}, dataset source is:\\n{WAN_location}/{filename}\")\n",
    "#     dataset = f\"{WAN_location}/{filename}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Load pickle file from dataset that has already been labeled, scaled, randomly undersampled, and split into train/test/val\n",
    "\n",
    "# if not os.path.exists(filename):\n",
    "#   print(f\"Retrieving pickle file\", dataset)\n",
    "#   #!wget {dataset}          #wget typically exists on Linux but not Windows\n",
    "#   !curl -O {dataset}        #curl typically exists on both Linux and Windows\n",
    "# else:\n",
    "#   print(f\"Pickle file {filename} already exists\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Open the pickle file\n",
    "\n",
    "# # uncomment the filename you want to load\n",
    "# pickle_file = \"Edge-IIoTset2023_scaled_data_tuple.pkl\"\n",
    "# #pickle_file = \"CIC_IOT_Dataset2023_scaled_data_tuple.pkl\"\n",
    "\n",
    "# # Load the tuple using pickle\n",
    "# with open(pickle_file, 'rb') as f:\n",
    "#     #data_tuple = pickle.load(f)               #syntax for pandas <  version 2.0\n",
    "#     data_tuple = pd.read_pickle(pickle_file)   #syntax for pandas >= version 2.0\n",
    "\n",
    "# # split the pickle file into the lists from the source dataset\n",
    "# X_train, X_test, X_val, y_train, y_test, y_val = data_tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xTdi1Hwemk4"
   },
   "source": [
    "# Load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask user which dataset they want to work with\n",
    "\n",
    "# Ask for user input\n",
    "print(f\"There are two available datasets to work with:\")\n",
    "print(f\"1. Edge-IIoTset2023\")\n",
    "print(f\"2. CIC_IOT_Dataset2023\")\n",
    "get_user_input = input(\"Please enter 1 or 2 to select the dataset you wish to work with: \")\n",
    "\n",
    "# Check for user respond\n",
    "if get_user_input == \"1\":\n",
    "    dataset_name = \"Edge-IIoTset2023\"\n",
    "elif get_user_input == \"2\":\n",
    "    dataset_name = \"CIC_IOT_Dataset2023\"\n",
    "else:\n",
    "    raise ValueError (\"Please select 1 or 2 for the appropriate dataset\")\n",
    "\n",
    "    \n",
    "print(f\"You selected dataset name {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WbkL_Okyed3m",
    "outputId": "0ef8f6d9-5543-46b6-eb64-d00acdead8e4"
   },
   "outputs": [],
   "source": [
    "# define CSV source file\n",
    "print(f\"Working with dataset {dataset_name}\")\n",
    "if dataset_name == \"Edge-IIoTset2023\":\n",
    "    filename     = 'DNN-EdgeIIoT-dataset.csv'\n",
    "    LAN_location = 'http://datasets.nyx.local:80/datasets/Edge-IIoTset2023/Selected_dataset_for_ML_and_DL'  #high speed local copy on LAN\n",
    "    WAN_location = 'http://datasets.nyx.ca:8081/datasets/Edge-IIoTset2023/Selected_dataset_for_ML_and_DL'   #accessible to entire internet\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\":\n",
    "    filename     = 'merged_filtered.csv'\n",
    "    LAN_location = 'http://datasets.nyx.local:80/datasets/CIC_IOT_Dataset2023/csv'  #high speed local copy on LAN\n",
    "    WAN_location = 'http://datasets.nyx.ca:8081/datasets/CIC_IOT_Dataset2023/csv'   #accessible to entire internet\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Could not determine dataset name, please set appropriate variable.\")\n",
    "    \n",
    "\n",
    "# Get the FQDN of the local machine\n",
    "fqdn = socket.getfqdn()\n",
    "ipv4_address = socket.gethostbyname(socket.gethostname())\n",
    "print(f\"Fully Qualified Domain Name (FQDN):{fqdn}, IPv4 address:{ipv4_address}\")\n",
    "if ( \"nyx.local\" in fqdn ):\n",
    "    # If inside the LAN, grab the local copy of the dataset\n",
    "    print(f\"Detected Fully Qualified Domain Name of {fqdn}, dataset source is:\\n{LAN_location}/{filename}\")\n",
    "    dataset = f\"{LAN_location}/{filename}\"\n",
    "else:\n",
    "    # If not inside the LAN, grab the dataset from an internet-accessible URL\n",
    "    print(f\"Detected Fully Qualified Domain Name of {fqdn}, dataset source is:\\n{WAN_location}/{filename}\")\n",
    "    dataset = f\"{WAN_location}/{filename}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JbmRPXRiXP5",
    "outputId": "0c6a5cda-7552-4701-edd9-6f41652b4239"
   },
   "outputs": [],
   "source": [
    "# check to see if the dataset has already been retrieved from the remote web server\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "  print(f\"Retrieving dataset\", dataset)\n",
    "  #!wget {dataset}          #wget typically exists on Linux but not Windows\n",
    "  !curl -O {dataset}        #curl typically exists on both Linux and Windows\n",
    "else:\n",
    "  print(f\"File {filename} already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWwVGaGqiXU2",
    "outputId": "4d1ca8d9-1056-455c-b859-2e357d8eac9e"
   },
   "outputs": [],
   "source": [
    "# Confirm the source datafile exists locally, just in case the previous cell failed to load the CSV file\n",
    "if not os.path.exists(filename):\n",
    "    raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Confirmed existence of filename {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_s2qZH5iXjD",
    "outputId": "de06c5bf-405c-471a-aac1-c254ab93bcd6"
   },
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "print(f\"Loading dataset from {filename}\")\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MVMhjiOXdB5",
    "outputId": "69dba265-16e2-4f3a-c7e7-d9b4089ec1c2"
   },
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V__jmKh-eeeg",
    "outputId": "88a2a556-8d98-4871-a906-7d35d5132c98"
   },
   "outputs": [],
   "source": [
    "print(f\"Dropping rows from the dataset during debugging to speed up this notebook - turn this off when finished debugging!\")\n",
    "\n",
    "# cut dataset in half if > 2 million rows\n",
    "if ( len(df) > 2000000):\n",
    "    print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "    df.drop(df.index[::2], inplace=True)\n",
    "    print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "# cut dataset in half if > 1 million rows\n",
    "if ( len(df) > 1000000):\n",
    "    print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "    df.drop(df.index[::2], inplace=True)\n",
    "    print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "# # cut dataset in half if > 0.5 million rows\n",
    "# if ( len(df) > 500000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "# # cut dataset in half if > 0.5 million rows\n",
    "# if ( len(df) > 500000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "# # cut dataset in half if > 250,000 rows\n",
    "# if ( len(df) > 250000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "\n",
    "# # cut dataset in half if > 100,000 rows\n",
    "# if ( len(df) > 100000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "\n",
    "#  # cut dataset in half if > 50,000 rows\n",
    "# if ( len(df) > 50000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
    "\n",
    "\n",
    "# # cut dataset in half if > 25,000 rows\n",
    "# if ( len(df) > 25000):\n",
    "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
    "#     df.drop(df.index[::2], inplace=True)\n",
    "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEjUyQMieeid",
    "outputId": "92851f91-fe3f-4aeb-96b8-dc7e1e005f49"
   },
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yCIR_Tweemv",
    "outputId": "9fbab5bd-96d0-4406-ac31-ace108e7b54e"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuvrO_1ae0bY"
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "DwF-dYr4ezmj",
    "outputId": "152360f4-15d5-4de2-cdf2-0f42fd518d5b"
   },
   "outputs": [],
   "source": [
    "# take a quick look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hEO6Svveeqz"
   },
   "outputs": [],
   "source": [
    "# Display all the data rather than just a portion\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtkIZa6Zeeu1",
    "outputId": "edb3fee4-ae7a-448d-f48b-497f0aaa736f"
   },
   "outputs": [],
   "source": [
    "# check for any missing values in dataset\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "mSYrTeCWee2b",
    "outputId": "4bbacd23-f7ef-4f09-ce57-66f8b3b7f146"
   },
   "outputs": [],
   "source": [
    "# check for any missing datatypes\n",
    "get_type_missing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "eVqK7ARKee7B",
    "outputId": "6c41ddca-728a-4003-f39e-ee33499f08dc"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "49MyoC5_ee_k",
    "outputId": "6324a306-8d9f-49f1-fee1-4b63d408659a"
   },
   "outputs": [],
   "source": [
    "# look at all the datatypes of that are objects, in case any can be converted to integers\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3wccJ22efDi",
    "outputId": "37bf7d91-908d-4097-aa98-7153c995c9eb"
   },
   "outputs": [],
   "source": [
    "# look at the values in all of the features\n",
    "\n",
    "feature_names = df.columns.tolist()\n",
    "\n",
    "for feature_name in feature_names:\n",
    "    if feature_name in df.columns:\n",
    "        print('\\n')\n",
    "        print(f\"------------------\")\n",
    "        print(f\"{feature_name}\")\n",
    "        print(f\"------------------\")\n",
    "        print(df[feature_name].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_e5sF2pefHf",
    "outputId": "a5d84e97-46aa-4ebe-b7aa-dafb61f229b6"
   },
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4kaTp5defLr",
    "outputId": "4814b323-f848-4a76-cb82-f7e835e1c118"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "698aTJzVfQjH"
   },
   "source": [
    "# Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR0zfa_qfQvU"
   },
   "source": [
    "## Fix up feature names for Edge-IIoTset2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"dataset_name is {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nl0dAQclfM49",
    "outputId": "81684e7c-c507-49fa-f8cf-96eff650520e"
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"Edge-IIoTset2023\":\n",
    "    # look at the column names\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_GpHzBMXefPW",
    "outputId": "3ea5dbd5-8fe0-43e8-b77f-e95218aa375e"
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"Edge-IIoTset2023\":\n",
    "    print(df['frame.time'].value_counts().head())\n",
    "    print(\"\\nNull Values:\")\n",
    "    print(df['frame.time'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94R61fcjefTG"
   },
   "outputs": [],
   "source": [
    "## converting to datetime\n",
    "#def convert_to_datetime(value):\n",
    "#    try:\n",
    "#         return pd.to_datetime(value)\n",
    "#    except:\n",
    "#        return np.nan\n",
    "\n",
    "# skip the time-consuming conversion because we drop this feature later\n",
    "#df['frame.time'] = df['frame.time'].apply(convert_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Muw-GgRefW7",
    "outputId": "5b987987-de79-465e-fee8-01eb68d6a39b"
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"Edge-IIoTset2023\":\n",
    "    # Validating IP address\n",
    "    print(df['ip.src_host'].value_counts().head())\n",
    "    print('_________________________________________________________')\n",
    "    print(df['ip.dst_host'].value_counts().head())\n",
    "    print('_________________________________________________________')\n",
    "    print(df['arp.src.proto_ipv4'].value_counts().head())\n",
    "    print('_________________________________________________________')\n",
    "    print(df['arp.dst.proto_ipv4'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2rlHdGTefav"
   },
   "outputs": [],
   "source": [
    "# just for fun explore these values in the http.file_data column\n",
    "#if dataset_name == \"Edge-IIoTset2023\":\n",
    "    #df[df['Attack_label'] == 1]['http.file_data'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DHn2dUlefei",
    "outputId": "af8e549b-921f-45ac-f64e-4a9b475aaa65"
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"Edge-IIoTset2023\":\n",
    "    print(df['mqtt.topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmTH8HwAefiU",
    "outputId": "23505a40-decd-4df7-d7ed-aec5602d493e"
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"Edge-IIoTset2023\":\n",
    "    print(df['mqtt.protoname'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2q26stJefme",
    "outputId": "02760918-06f1-4532-d8f1-aeb5344cdb5a"
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"Edge-IIoTset2023\":\n",
    "    print(df['dns.qry.name.len'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZZcjk9uefqd",
    "outputId": "ada3e5d9-4b9f-4667-a0d0-c972037757b1"
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"Edge-IIoTset2023\":\n",
    "    print(df['http.request.method'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxGUxjsNefuY",
    "outputId": "b51e91e7-188a-4f03-fada-dac2d5fc0d4a"
   },
   "outputs": [],
   "source": [
    "if dataset_name == \"Edge-IIoTset2023\":\n",
    "    # how many 0 (normal) and 1 (attack) values do we have?\n",
    "    print(df['Attack_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix up feature names for CIC_IOT_Dataset2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"dataset_name is {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"CIC_IOT_Dataset2023\":\n",
    "    #look at the column names\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"CIC_IOT_Dataset2023\":\n",
    "    # Rename the column \"label\" to \"Attack_type\"\n",
    "    # the final column contains the text-based labels for the data (1=normal -1=attack)\n",
    "    # the column name is this ugly value:  Attack LABLE (1:No Attack, -1:Attack)\n",
    "    # let's rename that column to: LABEL\n",
    "    if 'label' in df.columns:\n",
    "        df.rename(columns={'label' : 'Attack_type' }, inplace=True)\n",
    "    \n",
    "    # This feature has an embedded space, let's replace that with an underscore\n",
    "    if 'Protocol Type' in df.columns:\n",
    "        df.rename(columns={'Protocol Type' : 'Protocol_Type' }, inplace=True)\n",
    "\n",
    "    # This feature has an embedded space, let's replace that with an underscore\n",
    "    if 'Tot sum' in df.columns:\n",
    "        df.rename(columns={'Tot sum' : 'Tot_sum' }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"CIC_IOT_Dataset2023\":\n",
    "    # confirm the column was renamed from \"label\" to \"Attack_type\"\n",
    "    print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"CIC_IOT_Dataset2023\":\n",
    "    # The final column in the dataset is Attack_type, and will contain one of these values:\n",
    "\n",
    "    # Display unique values in the \"Attack_type\" column\n",
    "    unique_attack_types = df['Attack_type'].unique()\n",
    "    print(\"Unique Attack Types:\")\n",
    "    print(unique_attack_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"CIC_IOT_Dataset2023\":\n",
    "    # Let's change the text \"BenignTraffic\" in the Attack_type column to \"Normal\"\n",
    "    df['Attack_type'] = df['Attack_type'].replace('BenignTraffic', 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"CIC_IOT_Dataset2023\":\n",
    "    # Add a column called \"Attack_label\", which will be 0 if the Attack_type is BenignTraffic, and 1 if anything else\n",
    "    # This column will be used to separate the data into 2 classes (normal + abnormal)\n",
    "\n",
    "    # Add a new column 'Label' where Normal is 0, anything else is 1\n",
    "    df['Attack_label'] = df['Attack_type'].apply(lambda x: 0 if x == 'Normal' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"CIC_IOT_Dataset2023\":\n",
    "    # confirm there is a new column called \"Attack_label\" which contains 0 for BenignTraffic, 1 for any type of attack\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"CIC_IOT_Dataset2023\":\n",
    "    # how many 0 (normal) and 1 (attack) values do we have?\n",
    "    print(df['Attack_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Clxq2KZgf00K"
   },
   "source": [
    "# Visualization of raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "XzcVmv8Gefyb",
    "outputId": "daf4b3c8-ae57-4067-f963-7ea870b8ab29"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "sns.countplot(data=df, x='Attack_label', hue='Attack_type', edgecolor='black', linewidth=1)\n",
    "plt.title('Attack Label vs Attack Type', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "F58oMBX-fyO7",
    "outputId": "c09a26c1-ba6a-477f-b110-a4d606c98181"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.pie(df, names='Attack_label', title='Distribution of Attack Labels')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "17hirQgUfyTu",
    "outputId": "80499dfc-3809-4c5c-cec0-efef2445830e"
   },
   "outputs": [],
   "source": [
    "fig = px.pie(df, names='Attack_type', title='Distribution of Attack Type')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEqQvYWcgEyL"
   },
   "source": [
    "- class imbalance issue - this can cause the machine learning model to result in biased results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4stS-wJ8gLBP"
   },
   "source": [
    "# Drop features\n",
    "Now using our domain knowledge we will only select useful features from our dataset and drop the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdYvIq0afyYr",
    "outputId": "38d5f254-004b-43ec-c920-611614c2c9bd"
   },
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2CsQ1Yd8fycl",
    "outputId": "0514d313-16f9-4e97-9395-c1eed9f8d0ae"
   },
   "outputs": [],
   "source": [
    "# Identifying columns that are entirely NaN (empty) or have all zero values\n",
    "empty_or_zero_columns = df.columns[(df.isnull().all())\n",
    "| (df == 0).all()   | (df == 1).all() | (df == 1.0).all()\n",
    "| (df == 0.0).all() | (df == 2).all() | (df == 2.0).all()]\n",
    "\n",
    "# Displaying the identified columns\n",
    "empty_features = empty_or_zero_columns.tolist()\n",
    "\n",
    "print(\"These columns are all empty features:\")\n",
    "print(empty_features)\n",
    "\n",
    "\n",
    "for feature in empty_features:\n",
    "  if feature in df.columns:\n",
    "    df.drop(feature, axis=1, inplace=True)\n",
    "    print(\"Dropping empty feature:\", feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "0dNdzfp3fygz",
    "outputId": "59b951fb-1b99-4760-b278-56c6dc6fc25c"
   },
   "outputs": [],
   "source": [
    "# show the columns to confirm the features have been dropped\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geVfCA4Ffyk1",
    "outputId": "e7b53fb2-b00f-444f-8374-070a5c2bf2c7"
   },
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29JrX58Cfyor",
    "outputId": "027f1476-1e78-4f2f-b415-af9778723b04"
   },
   "outputs": [],
   "source": [
    "# drop these features\n",
    "\n",
    "if dataset_name == \"Edge-IIoTset2023\":\n",
    "    feature_names = [\"frame.time\", \"ip.src_host\", \"ip.dst_host\", \"arp.src.proto_ipv4\",\"arp.dst.proto_ipv4\",\n",
    "                \"http.file_data\",\"http.request.full_uri\",\"icmp.transmit_timestamp\",\n",
    "                \"http.request.uri.query\", \"tcp.options\",\"tcp.payload\",\"tcp.srcport\",\n",
    "                \"tcp.dstport\", \"udp.port\", \"mqtt.msg\", \"icmp.unused\", \"http.tls_port\", 'dns.qry.type',\n",
    "                'dns.retransmit_request_in', \"mqtt.msg_decoded_as\", \"mbtcp.trans_id\", \"mbtcp.unit_id\", \"http.request.method\", \"http.referer\",\n",
    "                \"http.request.version\", \"dns.qry.name.len\", \"mqtt.conack.flags\", \"mqtt.protoname\", \"mqtt.topic\"]\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\":\n",
    "   feature_names = [\"fin_flag_number\", \"syn_flag_number\", \"rst_flag_number\", \"psh_flag_number\"]\n",
    "\n",
    "for feature_name in feature_names:\n",
    "  if feature_name in df.columns:\n",
    "    df.drop(feature_name, axis=1, inplace=True)\n",
    "    print(\"Dropping feature:\", feature_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHskP5hgfysr",
    "outputId": "64883dda-6b41-457d-d2cd-70f785c86c81"
   },
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset after dropping features:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1RbX_b0fywk",
    "outputId": "6ddaf344-f3e3-4cc3-8fd8-ea914ebb51cb"
   },
   "outputs": [],
   "source": [
    "# print(df[df['tcp.flags.ack'] == 1]['Attack_label'].value_counts(normalize=True))\n",
    "# print(df[df['tcp.flags.ack'] == 0]['Attack_label'].value_counts(normalize=True))\n",
    "\n",
    "#df['Attack_label'].groupby(df['tcp.flags.ack']).value_counts(normalize=True)\n",
    "# hence we group by is prefered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q20MJDLDfzVe",
    "outputId": "515e0006-32b4-42d1-e6f0-4d2f9c38757e"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NErKgVOkfzZz",
    "outputId": "6a9f59fb-1a7e-45a4-bce2-cf1b1057cd60"
   },
   "outputs": [],
   "source": [
    "#view dimensions of dataset (rows and columns)\n",
    "print (\"Rows,columns in dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5w_HYYW7ghKI"
   },
   "source": [
    "# Label encoding\n",
    "- Problem: if we use a machine learning model to predict the Attack label, it could predict it as 0.1, 0.2 or 0.99 which is not a valid Attack label\n",
    "- Solution: Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VBKrY0CnuXT",
    "outputId": "3f6348c7-1049-490b-9202-a17f9de82ed2"
   },
   "outputs": [],
   "source": [
    "# The final column in the dataset is Attack_type, and will contain either 0 or 1\n",
    "\n",
    "# Display unique values in the \"Attack_type\" column\n",
    "unique_attack_types = df['Attack_type'].unique()\n",
    "print(\"Unique Attack Types:\")\n",
    "print(unique_attack_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "galIuRn0fzea",
    "outputId": "df7b7464-8579-4b82-d30d-932884fa2bba"
   },
   "outputs": [],
   "source": [
    "# add a column to the dataset called \"Attack_label\"\n",
    "# this column will only contain 0 or 1, and an integer representation of the text-based \"Attack_type\" column\n",
    "# if Attack_type=Normal, then Attack_label=0, otherwise, Attack_level=1\n",
    "\n",
    "le = LabelEncoder()    #assumes \"from sklearn.preprocessing import LabelEncoder\"\n",
    "df['Attack_label'] = le.fit_transform(df['Attack_label'])\n",
    "\n",
    "print(f\"Converting text-based Attack_type feature to integer-baesd Attack_label feature\")\n",
    "df['Attack_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiCfNraMnBIs"
   },
   "outputs": [],
   "source": [
    "# Now that we have encoded the text-based \"Attack_type\" column into the integer-based \"Attack_label\" column, we can drop the \"Attack_type\" column\n",
    "df.drop('Attack_type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "nWmIB3SCmPQy",
    "outputId": "11f68ef8-0069-4e02-c7fb-cdd007f292eb"
   },
   "outputs": [],
   "source": [
    "# confirm that the Attack_label column has been added, and the Attack_type column has been removed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQz2nMiBgmiT"
   },
   "outputs": [],
   "source": [
    "# separate X and y variables (independent and dependent variables)\n",
    "\n",
    "X = df.drop(['Attack_label'], axis=1)\n",
    "y = df['Attack_label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "USwKs1Q3n6k0",
    "outputId": "93b89016-688c-4ee9-f00d-c7afc56570a3"
   },
   "outputs": [],
   "source": [
    "# Sanity check to confirm X and y have equal number of samples\n",
    "print(f\"X has\", len(X), \"samples\")\n",
    "print(f\"y has\", len(y), \"samples\")\n",
    "if ( len(X) != len(y) ):\n",
    "  raise ValueError (\"X and y are different lengths, please investigate!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSEpgbDmg7Cb"
   },
   "source": [
    "# Split data into train / test / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCZM1U2Vg5lZ"
   },
   "outputs": [],
   "source": [
    "# Split X and y into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyS8i7qBgmnF"
   },
   "outputs": [],
   "source": [
    "# Now further split test set into testing and validation sets because Deep Learning models also have validation data\n",
    "# In this example, the train/test split in the previous cell was 80/20, so the 0.5 split you see in this cell splits the 20% of test data evenly into test and validation\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bE-Svztiqen3",
    "outputId": "965f1135-cc1f-4fe7-f2ad-7bd5e25f7a2e"
   },
   "outputs": [],
   "source": [
    "# Sanity check to confirm X_train and y_train have equal number of samples\n",
    "print(f\"X_train has\", len(X_train), \"samples\")\n",
    "print(f\"y_train has\", len(y_train), \"samples\")\n",
    "if ( len(X_train) != len(y_train) ):\n",
    "  raise ValueError (\"X_train and y_train are different lengths, please investigate!\")\n",
    "\n",
    "# Sanity check to confirm X_test and y_test have equal number of samples\n",
    "print('\\n')\n",
    "print(f\"X_test has\", len(X_test), \"samples\")\n",
    "print(f\"y_test has\", len(y_test), \"samples\")\n",
    "if ( len(X_test) != len(y_test) ):\n",
    "  raise ValueError (\"X_test and y_test are different lengths, please investigate!\")\n",
    "\n",
    "# Sanity check to confirm X_val and y_val have equal number of samples\n",
    "print('\\n')\n",
    "print(f\"X_val has\", len(X_val), \"samples\")\n",
    "print(f\"y_val has\", len(y_val), \"samples\")\n",
    "if ( len(X_val) != len(y_val) ):\n",
    "  raise ValueError (\"X_val and y_val are different lengths, please investigate!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "ex3XJ1WF79fA",
    "outputId": "9fc55436-d3c1-487b-b1ee-6b9b25e30927"
   },
   "outputs": [],
   "source": [
    "# create a pie chart showing relative sizes of X_train, X_test, X_val\n",
    "\n",
    "\n",
    "# Labels for the pie chart\n",
    "labels = ['Training', 'Test', 'Validation']\n",
    "\n",
    "# Number of rows in each dataset split\n",
    "sizes = [len(X_train), len(X_test), len(X_val)]\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Dataset Split prior to class balancing')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n",
    "\n",
    "print(f\"X_train contains {len(X_train)} rows, y_train contains {len(y_train)} rows\")\n",
    "print(f\"X_test  contains {len(X_test)} rows, y_test  contains {len(y_test)} rows\")\n",
    "print(f\"X_val   contains {len(X_val)} rows, y_val   contains {len(y_val)} rows\")\n",
    "\n",
    "if (len(X_train) < len(X_test)):\n",
    "  print(f\"\\nWARNING: You will notice in the above chart that X_train has fewer rows than X_test or X_val\")\n",
    "  print(f\"This should not be the case, because the dataset has not yet undergone any reduction in the size of the training set.\")\n",
    "  print(f\"Please confirm that you are working on a clean dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "h5FCqwO_7n-P",
    "outputId": "bfd1fcb8-a210-4590-de6e-2218bd1916e0"
   },
   "outputs": [],
   "source": [
    "# create a pie chart showing the class balance in the training data\n",
    "\n",
    "print(f\"This pie chart shows the class balance in the training data.\")\n",
    "print(f\"The y_train data is labeled as 0=normal 1=attack \\n\")\n",
    "\n",
    "# Count the occurrences of each unique value\n",
    "normal_class   = sum(1 for value in y_train if value == 0)\n",
    "abnormal_class = sum(1 for value in y_train if value == 1)\n",
    "print(f\"  normal class contains {normal_class} samples\")\n",
    "print(f\"abnormal class contains {abnormal_class} samples\")\n",
    "if (normal_class == abnormal_class): print(\"WARNING: This dataset is not expected to be balanced yet.  Please investigate.\")\n",
    "if (normal_class != abnormal_class): print(\"This dataset is currently imbalanced, will be balanced in next section.\")\n",
    "\n",
    "# Extract labels and sizes for the pie chart\n",
    "labels = [\"Normal class\", \"Abnormal class\"]\n",
    "values = [normal_class, abnormal_class]\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Class distribution prior to balancing')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okhEEwpeNbIw"
   },
   "source": [
    "# Balance data classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds-pbwdTMt7v"
   },
   "source": [
    "## SMOTE\n",
    "This section is only shown as an example, this notebook balances the classes with random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEEuQKj0gmvS"
   },
   "outputs": [],
   "source": [
    "# If you wanted to balance the classes with SMOTE instead, sample code shown below:\n",
    "\n",
    "## Create an instance of the SMOTE class\n",
    "#smote = SMOTE(sampling_strategy='auto')\n",
    "\n",
    "## Apply SMOTE to the training data\n",
    "#X_train_resampled, y_train_type_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8vjRUZHM98r"
   },
   "source": [
    "## sequential undersampling\n",
    "This section is only shown as an example, this notebook balances the classes with random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwQ486-HxXQE"
   },
   "outputs": [],
   "source": [
    "# # sample code to perform sequential undersampling instead of random undersampling\n",
    "\n",
    "# def sequential_undersample(X, y, minority_class_label, desired_ratio):\n",
    "#     # Separate majority and minority class samples\n",
    "#     majority_X = X[y != minority_class_label]\n",
    "#     majority_y = y[y != minority_class_label]\n",
    "#     minority_X = X[y == minority_class_label]\n",
    "#     minority_y = y[y == minority_class_label]\n",
    "\n",
    "#     print(f\"Percentage of minority class samples in y: {sum(y == minority_class_label) / len(y) * 100:.2f}%\")\n",
    "#     print(f\"Percentage of minority class samples in minority_y: {sum(minority_y == minority_class_label) / len(minority_y) * 100:.2f}%\")\n",
    "\n",
    "#     # Calculate the number of majority class samples to keep\n",
    "#     num_minority_samples = len(minority_X)\n",
    "#     #num_majority_samples = int(num_minority_samples * desired_ratio)\n",
    "#     num_majority_samples = num_minority_samples\n",
    "\n",
    "#     # Keep a portion of the majority class samples\n",
    "#     majority_X_subset = majority_X[:num_majority_samples]\n",
    "#     majority_y_subset = majority_y[:num_majority_samples]\n",
    "\n",
    "#     # Combine minority and subset of majority class samples\n",
    "#     X_balanced = np.concatenate((minority_X, majority_X_subset))\n",
    "#     y_balanced = np.concatenate((minority_y, majority_y_subset))\n",
    "\n",
    "#     return X_balanced, y_balanced\n",
    "\n",
    "# # Usage example\n",
    "# X_train_balanced, y_train_balanced = sequential_undersample(X_train, y_train, minority_class_label=1, desired_ratio=0.5)\n",
    "\n",
    "\n",
    "# # Count the occurrences of each unique value\n",
    "# normal_class   = sum(1 for value in y_train_balanced if value == 0)\n",
    "# abnormal_class = sum(1 for value in y_train_balanced if value == 1)\n",
    "# print(f\"  normal class contains {normal_class} samples\")\n",
    "# print(f\"abnormal class contains {abnormal_class} samples\")\n",
    "\n",
    "# # save the resampled values back to the original variable names so we can use consistent names throughout this notebook\n",
    "# X_train = X_train_balanced\n",
    "# y_train = y_train_balanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1ppbkz_hDOt"
   },
   "source": [
    "## random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLOlGp6DgmrO",
    "outputId": "22770dcc-c17a-451d-9468-ee4feaecae89"
   },
   "outputs": [],
   "source": [
    "# Initialize RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=1, random_state=42)\n",
    "\n",
    "# Apply Random Under Sampling\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Class balance before resampling\")\n",
    "print(y_train.value_counts())\n",
    "print('\\n')\n",
    "print(\"Class balance after resampling\")\n",
    "print(y_train_resampled.value_counts())\n",
    "\n",
    "# save the resampled values back to the original variable names so we can use consistent names throughout this notebook\n",
    "X_train = X_train_resampled\n",
    "y_train = y_train_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7efaqGJ8sV-M",
    "outputId": "aeb1a738-ee22-41f1-bc9a-ac719a4ae5b4"
   },
   "outputs": [],
   "source": [
    "# confirm the classes are balanced\n",
    "# Figure out how many rows of each class exist in y_train (0=normal, 1=abnormal)\n",
    "\n",
    "# Count occurrences of 0 and 1\n",
    "normal_class   = sum(1 for value in y_train if value == 0)\n",
    "abnormal_class = sum(1 for value in y_train if value == 1)\n",
    "\n",
    "print(f\"Count of   normal class: {normal_class}\")\n",
    "print(f\"Count of abnormal class: {abnormal_class}\")\n",
    "\n",
    "total_rows = abnormal_class + normal_class\n",
    "print(f\"Total Number of rows (normal+abnormal): {total_rows}\" )\n",
    "\n",
    "balance = abnormal_class / total_rows * 100\n",
    "balance = round(balance,2)\n",
    "\n",
    "print(f\"Percentage of abnormal class in dataset (abnormal/total*100): {balance}%\")\n",
    "if (balance  < 10): print(\"This dataset is very imbalanced, please beware of overfitting.\")\n",
    "if (balance != 50): print(\"WARNING: This dataset is supposed to be balanced.  Please investigate.\")\n",
    "if (balance == 50): print(\"This dataset is perfectly balanced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VG-D5SOlrhX-",
    "outputId": "5cbf8f19-2647-4156-e5b3-4e25a95b48ed"
   },
   "outputs": [],
   "source": [
    "# Sanity check to confirm X_train and y_train have equal number of samples\n",
    "print(f\"X_train has\", len(X_train), \"samples\")\n",
    "print(f\"y_train has\", len(y_train), \"samples\")\n",
    "if ( len(X_train) != len(y_train) ):\n",
    "  raise ValueError (\"X_train and y_train are different lengths, please investigate!\")\n",
    "\n",
    "# Sanity check to confirm X_test and y_test have equal number of samples\n",
    "print('\\n')\n",
    "print(f\"X_test has\", len(X_test), \"samples\")\n",
    "print(f\"y_test has\", len(y_test), \"samples\")\n",
    "if ( len(X_test) != len(y_test) ):\n",
    "  raise ValueError (\"X_test and y_test are different lengths, please investigate!\")\n",
    "\n",
    "# Sanity check to confirm X_val and y_val have equal number of samples\n",
    "print('\\n')\n",
    "print(f\"X_val has\", len(X_val), \"samples\")\n",
    "print(f\"y_val has\", len(y_val), \"samples\")\n",
    "if ( len(X_val) != len(y_val) ):\n",
    "  raise ValueError (\"X_val and y_val are different lengths, please investigate!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x6fJPskkwu0b",
    "outputId": "e484e91b-4686-4623-8216-4828732ab0e6"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9g5DPtbhRtm"
   },
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvpTZEY9gm_4",
    "outputId": "c1aa8195-e619-4c39-a70f-673f4376182d"
   },
   "outputs": [],
   "source": [
    "# perform feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)  # Only transform the test       set, don't fit\n",
    "X_val_scaled   = scaler.transform(X_val)   # Only transform the validation set, don't fit\n",
    "\n",
    "# Save the values under original names so we can use consistent names in subsequent sections\n",
    "X_train = X_train_scaled\n",
    "X_test  = X_test_scaled\n",
    "X_val   = X_val_scaled\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXmU6ZD1haWi"
   },
   "source": [
    "# Save progress in a pickle file\n",
    "We don't actually use this pickle file anywhere, but it is nice to have available for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxvwDfv4gnH9",
    "outputId": "d9488cee-f00a-43c2-c9bf-6db8cad49867"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    output_file = \"Edge-IIoTset2023_scaled_data_tuple.pkl\"\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": output_file = \"CIC_IOT_Dataset2023_scaled_data_tuple.pkl\"\n",
    "\n",
    "print(f\"Saving progress to pickle file: \", output_file)\n",
    "\n",
    "# Create a tuple\n",
    "data_tuple = (X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "\n",
    "# Save the tuple using pickle\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(data_tuple, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yND7gPnIgnMM",
    "outputId": "0c36b0a8-5c28-403d-908e-5902c8ae006f"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HLhMICYUN_O"
   },
   "source": [
    "# Visualization after processing raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imfwqbN1Uqzf",
    "outputId": "27bfd718-d3ca-458d-f021-2ea42e0911bd"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "print(f\"X_train contains {len(X_train)} rows, y_train contains {len(y_train)} rows\")\n",
    "print(f\"X_test  contains {len(X_test)} rows, y_test  contains {len(y_test)} rows\")\n",
    "print(f\"X_val   contains {len(X_val)} rows, y_val   contains {len(y_val)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6a6o1dtetLU",
    "outputId": "dfc529a6-d25b-4ffa-ab13-47038e8162b8"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pINZyEvogaWW",
    "outputId": "20d9f562-ba10-4963-bc41-ff40d80234da"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEr6tv8Vgag1",
    "outputId": "f2dcb84e-6059-4151-f453-9213c3eb3e08"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ce8-The9DSF",
    "outputId": "52329b3d-8934-4ae4-f7c8-e67f5ceb5595"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7QWVKohE1H9",
    "outputId": "99433a60-d110-48f9-93a5-ad212abef387"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "ZufQNzMjWdr6",
    "outputId": "7ff46f0b-1571-4708-d8ca-1db06824355d"
   },
   "outputs": [],
   "source": [
    "# create a pie chart showing relative sizes of X_train, X_test, X_val\n",
    "\n",
    "\n",
    "# Labels for the pie chart\n",
    "labels = ['Training', 'Test', 'Validation']\n",
    "\n",
    "# Number of rows in each dataset split\n",
    "sizes = [len(X_train), len(X_test), len(X_val)]\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Dataset Split after balancing classes by undersampling majority class in X_train,y_train')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n",
    "\n",
    "print(f\"X_train contains\", len(X_train), \"rows, y_train contains\", len(y_train), \" rows\")\n",
    "print(f\"X_test  contains\", len(X_test), \"rows, y_test  contains\", len(y_test), \" rows\")\n",
    "print(f\"X_val   contains\", len(X_val), \"rows, y_val   contains\", len(y_val), \" rows\")\n",
    "print(f\"Please note that this data is after undersampling the majority class for balancing, so it is expected that the 80/10/10 split is changed here.\")\n",
    "\n",
    "if (len(X_train) < len(X_test)):\n",
    "  print(f\"\\nWARNING: You will notice in the above chart that X_train has fewer rows than X_test or X_val\")\n",
    "  print(f\"This should not be the case, because the dataset has not yet undergone any reduction in the size of the training set.\")\n",
    "  print(f\"Please confirm that you are working on a clean dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "rXIu74uS0E5S",
    "outputId": "0688b99d-c685-4bd1-8d69-dc35e05073e8"
   },
   "outputs": [],
   "source": [
    "# create a pie chart showing the class balance in the training data\n",
    "\n",
    "print(f\"This pie chart shows the class balance in the training data.\")\n",
    "print(f\"The y_train data is labeled as 0=normal 1=attack \\n\")\n",
    "\n",
    "# Count the occurrences of each unique value\n",
    "normal_class   = sum(1 for value in y_train if value == 0)\n",
    "abnormal_class = sum(1 for value in y_train if value == 1)\n",
    "if (normal_class != abnormal_class): print(\"WARNING: This dataset is supposed to be balanced.  Please investigate.\")\n",
    "if (normal_class == abnormal_class): print(\"This dataset is perfectly balanced.\")\n",
    "\n",
    "# Extract labels and sizes for the pie chart\n",
    "labels = [\"Normal class\", \"Abnormal class\"]\n",
    "values = [normal_class, abnormal_class]\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Class balance in training data labels')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNfsuc9u3PoC",
    "outputId": "5e8b2b94-8eae-49eb-f7bc-bfad07a110b0"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3c6_ZILWSr3"
   },
   "source": [
    "# Reduce dataset size to speed up analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUK7Pvii9cZC"
   },
   "source": [
    "NOTE: When reducing the size of your dataset to speed up training, it's generally recommended to sample only from the training data and leave the validation and test data untouched. Here's why:\n",
    "\n",
    "Training Data:\n",
    "- Sampling from the training data allows you to create a smaller subset that can be used for training the model.\n",
    "- Since the training data is used to update the model's parameters during training, reducing its size can significantly speed up the training process without affecting the evaluation of the model.\n",
    "\n",
    "Validation Data:\n",
    "- The validation data is used to tune hyperparameters and monitor the model's performance during training.\n",
    "- It's important to keep the validation data separate from the training data to ensure an unbiased evaluation of the model's performance.\n",
    "- Sampling from the validation data could lead to overfitting on the validation set and biased performance estimates.\n",
    "\n",
    "Test Data:\n",
    "- Similarly, the test data serves as an unbiased evaluation of the model's performance on unseen data.\n",
    "- Sampling from the test data could lead to overly optimistic performance estimates, as the model is evaluated on a different distribution than it will encounter in real-world scenarios.\n",
    "\n",
    "In summary, while it's common to reduce the size of the training data to speed up training, it's important to keep the validation and test data separate and unchanged to ensure unbiased evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rFWvt5AWZN3",
    "outputId": "2e0b5eb9-4135-4099-9133-fef2358d9269"
   },
   "outputs": [],
   "source": [
    "# save these values for comparison at the end of this section\n",
    "X_train_len = len(X_train)  #re-calculate after subsampling\n",
    "X_test_len  = len(X_test)   #re-calculate after subsampling\n",
    "X_val_len   = len(X_val)    #re-calculate after subsampling\n",
    "y_train_len = len(y_train)  #re-calculate after subsampling\n",
    "y_test_len  = len(y_test)   #re-calculate after subsampling\n",
    "y_val_len   = len(y_val)    #re-calculate after subsampling\n",
    "\n",
    "\n",
    "print(f\"X_train contains\", len(X_train), \"rows, y_train contains\", len(y_train), \" rows\")\n",
    "print(f\"X_test  contains\", len(X_test), \"rows, y_test  contains\", len(y_test), \" rows\")\n",
    "print(f\"X_val   contains\", len(X_val), \"rows, y_val   contains\", len(y_val), \" rows\")\n",
    "\n",
    "print(f\"\\nThe objective of this section is to see if we can speed up the training process by reducing the size of the dataset, but not losing too much accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DI2ZFmW-WZR2",
    "outputId": "219f6746-e910-4af6-bfd6-d3611d17d05c"
   },
   "outputs": [],
   "source": [
    "# Define a list of fractions to keep\n",
    "#fractions_to_keep = [0.01, 0.02, 0.05, 0.10, 0.25, 0.50, 0.75, 1.0]\n",
    "fractions_to_keep = [0.25, 0.50, 0.75, 1.0]\n",
    "\n",
    "\n",
    "#initialize variables\n",
    "best_accuracy         = 0\n",
    "best_fraction_to_keep = 0\n",
    "accuracy_001          = 0\n",
    "accuracy_002          = 0\n",
    "accuracy_005          = 0\n",
    "accuracy_010          = 0\n",
    "accuracy_025          = 0\n",
    "accuracy_050          = 0\n",
    "accuracy_075          = 0\n",
    "accuracy_100          = 0\n",
    "\n",
    "# Iterate through different fractions\n",
    "for fraction_to_keep in fractions_to_keep:\n",
    "    # Randomly subsample the training set\n",
    "    num_samples_to_keep = int(len(X_train) * fraction_to_keep)\n",
    "    random_indices = np.random.choice(len(X_train), num_samples_to_keep, replace=False)\n",
    "\n",
    "    X_train_subsampled = X_train[random_indices]\n",
    "    y_train_subsampled = y_train.iloc[random_indices]   #use .iloc becaue y_train is a 1-dimensional array\n",
    "\n",
    "    # Train your model on the subsampled data\n",
    "    #clf = LogisticRegression(max_iter=800, random_state=42)\n",
    "    clf = MLPClassifier(random_state=42)\n",
    "    clf.fit(X_train_subsampled, y_train_subsampled)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate accuracy on the test set\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy on the test set (fraction_to_keep={fraction_to_keep:.4f}): {accuracy:.4f}\")\n",
    "\n",
    "    # Save the accuracy levels for later comparison\n",
    "    if fraction_to_keep == 0.01: accuracy_001 = accuracy\n",
    "    if fraction_to_keep == 0.02: accuracy_002 = accuracy\n",
    "    if fraction_to_keep == 0.05: accuracy_005 = accuracy\n",
    "    if fraction_to_keep == 0.10: accuracy_010 = accuracy\n",
    "    if fraction_to_keep == 0.25: accuracy_025 = accuracy\n",
    "    if fraction_to_keep == 0.50: accuracy_050 = accuracy\n",
    "    if fraction_to_keep == 0.75: accuracy_075 = accuracy\n",
    "    if fraction_to_keep == 1.0:  accuracy_100 = accuracy\n",
    "\n",
    "    # keep track of the best accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_fraction_to_keep = fraction_to_keep\n",
    "\n",
    "\n",
    "print(f\"The highest accuracy is {best_accuracy:.4f} using the {best_fraction_to_keep} fraction of the dataset\\n\")\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "RRQyYu9FWZV3",
    "outputId": "e1873f29-9dfb-4df2-c2ea-8fc144830a17"
   },
   "outputs": [],
   "source": [
    "# Visualize the results from the previous cell\n",
    "\n",
    "# Data extracted from the image\n",
    "data = {\n",
    "    'fraction_to_keep': [0.01, 0.02, 0.05, 0.10, 0.25, 0.50, 0.75, 1.00],\n",
    "    'accuracy': [accuracy_001, accuracy_002, accuracy_005, accuracy_010, accuracy_025, accuracy_050, accuracy_075, accuracy_100]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['fraction_to_keep'], df['accuracy'], marker='o')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Accuracy on the Test Set by Fraction of Data Kept')\n",
    "plt.xlabel('Fraction of Data Kept')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Adding text for each data point\n",
    "for i in range(len(df)):\n",
    "    plt.text(df['fraction_to_keep'][i], df['accuracy'][i], f\"{df['fraction_to_keep'][i]*100}%\", ha='right')\n",
    "\n",
    "# Adding grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the figure with texts\n",
    "fig_path_with_text = 'accuracy_vs_data_fraction_with_text.png'\n",
    "plt.savefig(fig_path_with_text)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OyynLMUWZZl",
    "outputId": "cc1e2651-4397-42d0-9ae5-1ca77a78a240"
   },
   "outputs": [],
   "source": [
    "# This cell will programnmatically determine the best_fraction_to_keep, by sacrificing some (small) amount of accuracy for speed.\n",
    "# Exactly how small?  Let's go with an acceptable loss of 1% of accuracy for better speed.\n",
    "\n",
    "acceptable_loss_of_accuracy = 0.0100  # 0.01*100= 1%  Tweak this value depending on how much accuracy you are willing to sacrifice\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_100):\n",
    "    print(f\"Using 100% of the dataset gives {accuracy_100*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 1.0\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_075):\n",
    "    print(f\"Using  75% of the dataset gives {accuracy_075*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.75\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_050):\n",
    "    print(f\"Using  50% of the dataset gives {accuracy_050*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.50\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_025):\n",
    "    print(f\"Using  25% of the dataset gives {accuracy_025*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.25\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_010):\n",
    "    print(f\"Using  10% of the dataset gives {accuracy_010*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.10\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_005):\n",
    "    print(f\"Using   5% of the dataset gives {accuracy_005*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.05\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_002):\n",
    "    print(f\"Using   2% of the dataset gives {accuracy_002*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.02\n",
    "\n",
    "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_001):\n",
    "    print(f\"Using   1% of the dataset gives {accuracy_001*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
    "    best_fraction_to_keep = 0.01\n",
    "\n",
    "print(f\"\\nBased on the above calculations, we will keep {best_fraction_to_keep*100:.0f}% of the dataset, which will still provide acceptable accuracy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61P-uoy8Wxlw",
    "outputId": "39dbc864-98aa-4264-cf12-50d197fb9f74"
   },
   "outputs": [],
   "source": [
    "# Based on the accuracy calculations in the previous cell, decide how much of the dataset to keep\n",
    "fraction_to_keep = best_fraction_to_keep\n",
    "\n",
    "# Randomly subsample the training set\n",
    "num_samples_to_keep = int(len(X_train) * fraction_to_keep)\n",
    "random_indices = np.random.choice(len(X_train), num_samples_to_keep, replace=False)\n",
    "\n",
    "#save the sub-sampled data to temporary variable names\n",
    "X_train_subsampled = X_train[random_indices]\n",
    "y_train_subsampled = y_train.iloc[random_indices]   #use .iloc becaue y_train is a 1-dimensional array\n",
    "\n",
    "#save the sub-sampled data back to the original variable names that are used in subsequent sections\n",
    "X_train = X_train_subsampled\n",
    "y_train = y_train_subsampled\n",
    "\n",
    "print(f\"\\nPrior to downsampling the dataset sizes were:\")\n",
    "print(f\"---------------------------------------------\")\n",
    "print(f\"X_train previously contained {X_train_len} rows, y_train previously contained {y_train_len} rows\")  #these values were calculated prior to subsampling\n",
    "print(f\"X_test  previously contained {X_test_len} rows, y_test  previously contained {y_test_len} rows\")\n",
    "print(f\"X_val   previously contained {X_val_len} rows, y_val   previously contained {y_val_len} rows\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nAfter downsampling the training data without losing too much accuracy, the new size of the dataset is:\")\n",
    "print(f\"------------------------------------------------------------------------------------------------------\")\n",
    "X_train_len = len(X_train)  #re-calculate after subsampling\n",
    "X_test_len  = len(X_test)   #re-calculate after subsampling\n",
    "X_val_len   = len(X_val)    #re-calculate after subsampling\n",
    "y_train_len = len(y_train)  #re-calculate after subsampling\n",
    "y_test_len  = len(y_test)   #re-calculate after subsampling\n",
    "y_val_len   = len(y_val)    #re-calculate after subsampling\n",
    "\n",
    "print(f\"X_train now contains {X_train_len} rows, y_train now contains {y_train_len} rows\")  #these values were calculated prior to subsampling\n",
    "print(f\"X_test  now contains {X_test_len} rows, y_test  now contains {y_test_len} rows\")\n",
    "print(f\"X_val   now contains {X_val_len} rows, y_val   now contains {y_val_len} rows\")\n",
    "\n",
    "if (len(X_train) < len(X_test)):\n",
    "  print(f\"\\nWARNING: You have reduced the size of X_train by too much!  X_train should not be smaller than X_test\")\n",
    "  print(f\"This is because the training data was reduced via subsampling to speed up processing, but the test and validation data was not reduced in size.\")\n",
    "  print(f\"Please go back to the dataset reduction setting and adjust the sizes of of the fractions_to_keep list\")\n",
    "  raise ValueError (\"X_train has been reduced by too much, please investigate!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "MCrIItQfIyuA",
    "outputId": "b4dcdce2-f6b2-4914-d51e-a5d7c2a195d8"
   },
   "outputs": [],
   "source": [
    "# create a pie chart showing relative sizes of X_train, X_test, X_val\n",
    "\n",
    "\n",
    "# Labels for the pie chart\n",
    "labels = ['Training', 'Test', 'Validation']\n",
    "\n",
    "# Number of rows in each dataset split\n",
    "sizes = [len(X_train), len(X_test), len(X_val)]\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Dataset Split after subsampling training data')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n",
    "\n",
    "print(f\"X_train contains\", len(X_train), \"rows, y_train contains\", len(y_train), \" rows\")\n",
    "print(f\"X_test  contains\", len(X_test), \"rows, y_test  contains\", len(y_test), \" rows\")\n",
    "print(f\"X_val   contains\", len(X_val), \"rows, y_val   contains\", len(y_val), \" rows\")\n",
    "\n",
    "if (len(X_train) < len(X_test)):\n",
    "  print(f\"\\nWARNING: You have reduced the size of X_train by too much!  X_train should not be smaller than X_test\")\n",
    "  print(f\"This is because the training data was reduced via subsampling to speed up processing, but the test and validation data was not reduced in size.\")\n",
    "  print(f\"Please go back to the dataset reduction setting and adjust the sizes of of the fractions_to_keep list\")\n",
    "  raise ValueError (\"X_train has been reduced by too much, please investigate!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "BcDY72YWPeME",
    "outputId": "95166448-0cd0-4259-b56e-b54dd87afeec"
   },
   "outputs": [],
   "source": [
    "# create a pie chart showing the class balance in the training data\n",
    "\n",
    "print(f\"This pie chart shows the class balance in the training data.\")\n",
    "print(f\"The y_train data is labeled as 0=normal 1=attack \\n\")\n",
    "\n",
    "# Count the occurrences of each unique value\n",
    "value_counts = Counter(y_train)    #assumes \"from collections import Counter\"\n",
    "\n",
    "# Extract labels and sizes for the pie chart\n",
    "labels = list(value_counts.keys())\n",
    "sizes = list(value_counts.values())\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Class balance in training data labels')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9J5L7el-Pexs",
    "outputId": "801ca877-bf85-4558-fe44-586c9a095247"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcB2_DRHt5Q9"
   },
   "source": [
    "# Model training with traditional classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBD4TM5Tt_cI"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jND7EdOUckhI"
   },
   "source": [
    "### default params\n",
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 970
    },
    "id": "XROyny8YuC-0",
    "outputId": "5c8bd533-5dad-4b7b-db5e-b15001c5f69f"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the LogisticRegression model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_lr_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "# We want to see approximately equal results from TN and TP\n",
    "cm = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_whb7hJuEqL"
   },
   "source": [
    "### optimized params\n",
    "LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDSWt1gaZ1eH"
   },
   "source": [
    "The LogisticRegression() class in scikit-learn provides several parameters that can be adjusted to customize the logistic regression model. Here are some of the commonly used parameters:\n",
    "- penalty: Specifies the norm used in the penalization. It can take values like 'l1' (L1 regularization), 'l2' (L2 regularization), or 'none' (no regularization). The default is 'l2'.\n",
    "- C: Inverse of regularization strength. Smaller values specify stronger regularization. The default value is 1.0.\n",
    "- solver: Algorithm to use in the optimization problem. Options include 'liblinear', 'newton-cg', 'lbfgs', 'sag', and 'saga'. The default is 'lbfgs'.\n",
    "- max_iter: Maximum number of iterations taken for the solvers to converge. The default is 100.\n",
    "- multi_class: Specifies the strategy to use for multiclass classification. Options include 'auto', 'ovr' (one-vs-rest), and 'multinomial' (softmax). The default is 'auto'.\n",
    "- verbose: Controls the verbosity of the output. Set to an integer value greater than 0 for more verbosity. The default is 0.\n",
    "- random_state: Seed used by the random number generator. It ensures reproducibility of results. Set to an integer for reproducible output. The default is None.\n",
    "- tol: Tolerance for stopping criteria. The default is 1e-4.\n",
    "class_weight: Weights associated with classes. This can be used to handle class imbalance by assigning higher weights to minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rGcEIpvsuDFG",
    "outputId": "e481200d-cc84-4b0f-d141-37a91304ab80"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the  model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'penalty': ['None', 'l2'],\n",
    "    'C': [0.1, 1.0],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'max_iter': [100, 200],\n",
    "    'multi_class': ['auto'],\n",
    "    'random_state': [42]                 #for reproducible results\n",
    "}\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1, verbose=verbosity)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of the model with the best hyperparameters\n",
    "clf = LogisticRegression(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "lr_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "lr_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "lr_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_lr = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_lr_optimized      = Accuracy\n",
    "sensitivity_lr_optimized   = Sensitivity\n",
    "specificity_lr_optimized   = Specificity\n",
    "geometricmean_lr_optimized = GeometricMean\n",
    "precision_lr_optimized     = Precision\n",
    "recall_lr_optimized        = Recall\n",
    "f1_lr_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhO1-d2tWlqC"
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2TuxkDtcywa"
   },
   "source": [
    "### default params\n",
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "PtRUIeblWn7p",
    "outputId": "0ba1a8a9-bf2b-4f88-b0a4-8b1bd6045f9b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of the model\n",
    "#clf = GaussianNB()    # suitable for continuous features\n",
    "#clf = MultinomialNB() # used for discrete data like word counts\n",
    "clf = BernoulliNB()    # suitable for binary data, gives best accuracy for this dataset\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_nb_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_nb_unoptimized      = Accuracy\n",
    "sensitivity_nb_unoptimized   = Sensitivity\n",
    "specificity_nb_unoptimized   = Specificity\n",
    "geometricmean_nb_unoptimized = GeometricMean\n",
    "precision_nb_unoptimized     = Precision\n",
    "recall_nb_unoptimized        = Recall\n",
    "f1_nb_unoptimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWzonkRvWocl"
   },
   "source": [
    "### optimized params\n",
    "NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZY7y9IdjyWX"
   },
   "source": [
    "he BernoulliNB class in scikit-learn represents a naive Bayes classifier for Bernoulli-distributed data. Here are the parameters of the BernoulliNB class:\n",
    "\n",
    "- alpha: (float, default=1.0 or 1e-10)\n",
    "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "- binarize: (float or None, default=None)\n",
    "Threshold for binarizing (mapping to boolean) of sample features. If None, no binarization is performed.\n",
    "- fit_prior: (bool, default=True)\n",
    "Whether to learn class prior probabilities or not. If False, a uniform prior will be used.\n",
    "- class_prior: (array-like of shape (n_classes,), default=None)\n",
    "Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.\n",
    "- min_df: (float or int, default=1)\n",
    "When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature.\n",
    "- max_df: (float or int, default=1.0)\n",
    "When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "- max_features: (int, default=None)\n",
    "If not None, build a vocabulary that only considers the top max_features ordered by term frequency across the corpus.\n",
    "- binary: (bool, default=False)\n",
    "Whether to treat all values greater than zero as 1, and all others as 0.\n",
    "- n_jobs: (int, default=None)\n",
    "The number of parallel jobs to run. -1 means using all processors.\n",
    "\n",
    "\n",
    "These parameters allow you to customize the behavior of the Bernoulli Naive Bayes classifier according to your specific needs and the characteristics of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 898
    },
    "id": "7ySYbKhWWr9H",
    "outputId": "3777cdb1-cebd-43b7-9bb7-3c53d9716943"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "clf = BernoulliNB()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "# skip the sigmoid and poly kernels, rarely used\n",
    "param_grid = {\n",
    "    'alpha': [1.0, 0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1, verbose=verbosity)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "print(\"Performing GridSearchCV\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of model with the best hyperparameters\n",
    "clf = BernoulliNB(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "print(\"Fitting the model\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "nb_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "nb_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "nb_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_nb = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_nb_optimized      = Accuracy\n",
    "sensitivity_nb_optimized   = Sensitivity\n",
    "specificity_nb_optimized   = Specificity\n",
    "geometricmean_nb_optimized = GeometricMean\n",
    "precision_nb_optimized     = Precision\n",
    "recall_nb_optimized        = Recall\n",
    "f1_nb_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAF8RprPSafQ"
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkhDQRIsc8id"
   },
   "source": [
    "### default params\n",
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "hImjfzyoSb4_",
    "outputId": "2b42c61d-9585-49ad-8728-27958918c221"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the model with the desired number of neighbors (you can adjust n_neighbors)\n",
    "clf = KNeighborsClassifier(n_neighbors=5)  # You can change the value of n_neighbors as needed\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_knn_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_knn_unoptimized      = Accuracy\n",
    "sensitivity_knn_unoptimized   = Sensitivity\n",
    "specificity_knn_unoptimized   = Specificity\n",
    "geometricmean_knn_unoptimized = GeometricMean\n",
    "precision_knn_unoptimized     = Precision\n",
    "recall_knn_unoptimized        = Recall\n",
    "f1_knn_unoptimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CsWZNHqSeda"
   },
   "source": [
    "### optimized params\n",
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idmiRTp_SdTr"
   },
   "outputs": [],
   "source": [
    "# # Create an instance of the model\n",
    "# clf = KNeighborsClassifier()\n",
    "\n",
    "# # Define the hyperparameters to tune\n",
    "# param_grid = {\n",
    "#     'n_neighbors': [5,10,15,20,30],\n",
    "#     'weights': ['uniform', 'distance']\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# # Create an instance of GridSearchCV\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1)\n",
    "\n",
    "# # Fit the grid search to the training data\n",
    "# print(f\"Performing grid_search to find optimal parameters\")\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best hyperparameters\n",
    "# best_params = grid_search.best_params_\n",
    "# best_scores = grid_search.best_score_\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# # Create a new instance of the model with the best hyperparameters\n",
    "# clf = KNeighborsClassifier(**best_params)\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict the labels for the test data\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # final cross validation\n",
    "# cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "# print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "# print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "# print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "# knn_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "# knn_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "# knn_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# # Evaluate the model\n",
    "# Accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# # save best parameters for later comparison\n",
    "# best_params_knn = best_params\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# accuracy_knn_optimized      = Accuracy\n",
    "# sensitivity_knn_optimized   = Sensitivity\n",
    "# specificity_knn_optimized   = Specificity\n",
    "# geometricmean_knn_optimized = GeometricMean\n",
    "# precision_knn_optimized     = Precision\n",
    "# recall_knn_optimized        = Recall\n",
    "# f1_knn_optimized            = F1\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfRwkMfoUI9J"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltUX9LdfdFAU"
   },
   "source": [
    "### default params\n",
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQk64zS_UKGr"
   },
   "outputs": [],
   "source": [
    "# # Create an instance of the model\n",
    "# clf = SVC()\n",
    "\n",
    "# default_params = clf.get_params()\n",
    "# print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict the labels for the test data\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = clf.score(X_test, y_test)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # save accuracy for later comparison\n",
    "# accuracy_svm_undersampled_unoptimized = accuracy\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# accuracy_svm_unoptimized      = Accuracy\n",
    "# sensitivity_svm_unoptimized   = Sensitivity\n",
    "# specificity_svm_unoptimized   = Specificity\n",
    "# geometricmean_svm_unoptimized = GeometricMean\n",
    "# precision_svm_unoptimized     = Precision\n",
    "# recall_svm_unoptimized        = Recall\n",
    "# f1_svm_unoptimized            = F1\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0XSUG9jUQ99"
   },
   "source": [
    "### optimized params\n",
    "SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqmV2_5F0p5y"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXh3qfT1UTz0",
    "outputId": "a7862c28-93e9-46f1-8615-eb3d34e5931c"
   },
   "outputs": [],
   "source": [
    "print(\"WARNING: SVM hyperparameter optimization is very CPU-intensive, this will take some time...\")\n",
    "\n",
    "# use the accuracy value prior to hyperparameter optimization so we still have something to copmare to at the end of this notebook\n",
    "accuracy_svm_optimized = accuracy_svm_unoptimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pR5IrihZUXrP"
   },
   "outputs": [],
   "source": [
    "# # Create an instance of the model\n",
    "# clf = SVC()\n",
    "\n",
    "# # Define the hyperparameters to tune\n",
    "# # skip the sigmoid and poly kernels, rarely used\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10],\n",
    "#     'kernel': ['rbf', 'linear'],\n",
    "#     'probability': [True],               #probability=True is required for VotingClassifier\n",
    "#     'random_state': [42]                 #for reproducible results\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# # Create an instance of GridSearchCV\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1, verbose=verbosity)\n",
    "\n",
    "# # Fit the grid search to the training data\n",
    "# print(\"Performing GridSearchCV\")\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best hyperparameters\n",
    "# best_params = grid_search.best_params_\n",
    "# best_scores = grid_search.best_score_\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# # Create a new instance of model with the best hyperparameters\n",
    "# clf = SVC(**best_params)\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# print(\"Fitting the model\")\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict the labels for the test data\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # final cross validation\n",
    "# cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "# print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "# print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "# print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "# svm_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "# svm_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "# svm_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# # Evaluate the model\n",
    "# Accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# # save best parameters for later comparison\n",
    "# best_params_svm = best_params\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# accuracy_svm_optimized      = Accuracy\n",
    "# sensitivity_svm_optimized   = Sensitivity\n",
    "# specificity_svm_optimized   = Specificity\n",
    "# geometricmean_svm_optimized = GeometricMean\n",
    "# precision_svm_optimized     = Precision\n",
    "# recall_svm_optimized        = Recall\n",
    "# f1_svm_optimized            = F1\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8cJRsSbutbf"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kUF62yHdN7E"
   },
   "source": [
    "### default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "Fyj2wUiAuxMD",
    "outputId": "7ae1adc6-a13b-4c16-8da3-8942ee448fb2"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the DecisionTreeClassifier model\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_dt_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_dt_unoptimized      = Accuracy\n",
    "sensitivity_dt_unoptimized   = Sensitivity\n",
    "specificity_dt_unoptimized   = Specificity\n",
    "geometricmean_dt_unoptimized = GeometricMean\n",
    "precision_dt_unoptimized     = Precision\n",
    "recall_dt_unoptimized        = Recall\n",
    "f1_dt_unoptimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqJnOAAduzPS"
   },
   "source": [
    "### optimized params\n",
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "tJfxPRC4uxS9",
    "outputId": "48687701-cee7-4049-f67e-e74e0e5a4e28"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the DecisionTreeClassifier model\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 15, 25],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'random_state': [42]                 #for reproducible results\n",
    "}\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count,n_jobs=-1, verbose=verbosity)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "print(f\"Performing grid_search to find optimal parameters\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of the model with the best hyperparameters\n",
    "clf = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "dt_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "dt_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "dt_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_dt = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_dt_optimized      = Accuracy\n",
    "sensitivity_dt_optimized   = Sensitivity\n",
    "specificity_dt_optimized   = Specificity\n",
    "geometricmean_dt_optimized = GeometricMean\n",
    "precision_dt_optimized     = Precision\n",
    "recall_dt_optimized        = Recall\n",
    "f1_dt_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5r8tytWvC0a"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJqXuHSsdVlf"
   },
   "source": [
    "### default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "q3sDW_5PvEm2",
    "outputId": "f3182a46-0081-4e9a-d9a4-0c80fc2f6b78"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the RandomForestClassifier model\n",
    "clf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_rf_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_rf_unoptimized      = Accuracy\n",
    "sensitivity_rf_unoptimized   = Sensitivity\n",
    "specificity_rf_unoptimized   = Specificity\n",
    "geometricmean_rf_unoptimized = GeometricMean\n",
    "precision_rf_unoptimized     = Precision\n",
    "recall_rf_unoptimized        = Recall\n",
    "f1_rf_unoptimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vXwhEkDvGIz"
   },
   "source": [
    "### optimized params\n",
    "RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_GSVFaYctje"
   },
   "source": [
    "The RandomForestClassifier() class in scikit-learn provides several parameters that can be adjusted to customize the random forest model. Here are some of the commonly used parameters:\n",
    "\n",
    "- n_estimators: The number of trees in the forest. Higher values usually yield better performance, but also increase computational cost. The default is 100.\n",
    "- criterion: The function used to measure the quality of a split. It can be 'gini' for the Gini impurity or 'entropy' for the information gain. The default is 'gini'.\n",
    "- max_depth: The maximum depth of the tree. If None, nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. The default is None.\n",
    "- min_samples_split: The minimum number of samples required to split an internal node. The default is 2.\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node. The default is 1.\n",
    "Vmax_features: The number of features to consider when looking for the best split. It can be 'auto' (sqrt(n_features)), 'sqrt' (sqrt(n_features)), 'log2' (log2(n_features)), or a number between 0 and 1 (fraction of total features). The default is 'auto'.\n",
    "- bootstrap: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. The default is True.\n",
    "- random_state: Seed used by the random number generator. It ensures reproducibility of results. Set to an integer for reproducible output. The default is None.\n",
    "- n_jobs: The number of jobs to run in parallel for both fit and predict. -1 means using all processors. The default is 1.\n",
    "- verbose: Controls the verbosity of the output. Set to an integer value greater than 0 for more verbosity. The default is 0.\n",
    "- class_weight: Weights associated with classes. This can be used to handle class imbalance by assigning higher weights to minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "14cbw0m-vE4i",
    "outputId": "5fb5e26c-6179-497c-88a1-2c3292a62bd7"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the RandomForestClassifier model\n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    #'n_estimators': [100, 200, 300, 500],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    #'max_depth': ['None', 5, 10],\n",
    "    #'class_weight': ['None', 'balanced'],\n",
    "    'random_state': [42]                 #for reproducible results\n",
    "}\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1, verbose=verbosity)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "print(\"Performing grid_search to find optimal parameters\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of the model with the best hyperparameters\n",
    "clf = RandomForestClassifier(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "rf_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "rf_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "rf_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_rf = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_rf_optimized      = Accuracy\n",
    "sensitivity_rf_optimized   = Sensitivity\n",
    "specificity_rf_optimized   = Specificity\n",
    "geometricmean_rf_optimized = GeometricMean\n",
    "precision_rf_optimized     = Precision\n",
    "recall_rf_optimized        = Recall\n",
    "f1_rf_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKzvSno8vROI"
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6dnN3TfQ93e"
   },
   "source": [
    "Gradient Boosting is a popular machine learning technique used for both regression and classification tasks. It is an ensemble learning method that builds a strong predictive model by combining the predictions of multiple weaker models, typically decision trees. Here's how gradient boosting works:\n",
    "\n",
    "1. Base Learners (Weak Models): Gradient Boosting combines the predictions of multiple weak models, often decision trees, to create a strong predictive model. These weak models are referred to as base learners or weak learners.\n",
    "2. Sequential Training: Gradient Boosting trains the weak models sequentially. Each new model is trained to correct the errors made by the previous models.\n",
    "3. Loss Function: During training, Gradient Boosting minimizes a loss function, which measures the difference between the actual target values and the predicted values of the ensemble model. Common loss functions include mean squared error (MSE) for regression tasks and cross-entropy loss for classification tasks.\n",
    "4. Gradient Descent Optimization: Gradient Boosting optimizes the loss function using gradient descent. In each iteration, the algorithm calculates the gradient of the loss function with respect to the current predictions and adjusts the predictions in the direction that minimizes the loss.\n",
    "5. Gradient Boosting Algorithm:\n",
    "- Initialize the ensemble model with a simple base learner (e.g., a decision stump).\n",
    "- Train the base learner on the training data and calculate the residuals (the differences between the actual and predicted values).\n",
    "- Fit a new base learner to the residuals, focusing on the areas where the previous model made errors.\n",
    "- Combine the predictions of all base learners to make the final ensemble prediction.\n",
    "- Repeat the process until a predefined number of base learners have been added, or until the loss function converges.\n",
    "6. Regularization: Gradient Boosting typically includes regularization techniques to prevent overfitting, such as limiting the depth of the trees, adding shrinkage (learning rate), and using subsampling (training on random subsets of the data).\n",
    "7. Hyperparameter Tuning: Gradient Boosting involves tuning several hyperparameters, such as the learning rate, tree depth, number of trees, and regularization parameters, to optimize the performance of the model.\n",
    "8. Scalability: Gradient Boosting can handle large datasets and high-dimensional feature spaces. However, training time and memory usage can increase with the complexity of the model and the size of the dataset.\n",
    "\n",
    "\n",
    "Overall, Gradient Boosting is a powerful and versatile technique that often achieves state-of-the-art performance on a wide range of machine learning tasks. It is widely used in practice due to its effectiveness and ease of implementation. Popular implementations of Gradient Boosting include XGBoost, LightGBM, and CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbJPGo50ddBW"
   },
   "source": [
    "### default params\n",
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "EToyqM-wvSjh",
    "outputId": "48d171d0-4456-48d4-af18-6d234c4948a9"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_gb_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vq1_hvtTvaU1"
   },
   "source": [
    "### optimized params\n",
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gwi8Z5Q7vSrc",
    "outputId": "68f6d8cd-d9fe-43a6-a828-d7b4277d16fc"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "#default_params = clf.get_params()\n",
    "#print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [100],               #10,200 reduced accuracy\n",
    "    'learning_rate': [0.1, 1.0],\n",
    "    'max_depth': [3],                    #add higher numbers reduces accuracy\n",
    "    'random_state': [42]                 #for reproducible results\n",
    "}\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1, verbose=verbosity)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "print(f\"Performing grid_search to find optimal parameters\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of the model with the best hyperparameters\n",
    "print(f\"Fitting the model with these parameters: {best_params}\")\n",
    "clf = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "gb_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "gb_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "gb_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model\n",
    "Accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_gb = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_gb_optimized      = Accuracy\n",
    "sensitivity_gb_optimized   = Sensitivity\n",
    "specificity_gb_optimized   = Specificity\n",
    "geometricmean_gb_optimized = GeometricMean\n",
    "precision_gb_optimized     = Precision\n",
    "recall_gb_optimized        = Recall\n",
    "f1_gb_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vd5hpapcvoUh"
   },
   "source": [
    "# Compare accuracy of LR, NB, KNN, SVM, DT, GB, RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bT87k7Bfvwgy"
   },
   "outputs": [],
   "source": [
    "# this section compares the accuracy of different methods:\n",
    "\n",
    "from tabulate import tabulate\n",
    "# Define headers for the table\n",
    "headers = [\"Model\", \"Accuracy Un-optimized\", \"Accuracy Optimized\"]\n",
    "# Define data for the trable\n",
    "data = [\n",
    "    [\"LR\",  accuracy_lr_unoptimized,   accuracy_lr_optimized],\n",
    "    [\"NB\",  accuracy_nb_unoptimized,   accuracy_nb_optimized],\n",
    "    [\"KNN\", accuracy_knn_unoptimized,  accuracy_knn_optimized],\n",
    "    [\"SVM\", accuracy_svm_unoptimized,  accuracy_svm_optimized],\n",
    "    [\"DT\",  accuracy_dt_unoptimized,   accuracy_dt_optimized],\n",
    "    [\"RF\",  accuracy_rf_unoptimized,   accuracy_rf_optimized],\n",
    "    [\"GB\",  accuracy_gb_unoptimized,   accuracy_gb_optimized]\n",
    "]\n",
    "\n",
    "# Round all floats in the data list to 4 decimal places\n",
    "data_rounded = [[elem if isinstance(elem, str) else round(elem, 4) for elem in row] for row in data]\n",
    "\n",
    "# Generate the table\n",
    "table = tabulate(data_rounded, headers=headers, tablefmt=\"fancy_grid\")\n",
    "\n",
    "# Print the table\n",
    "print(table)\n",
    "\n",
    "\n",
    "# sanity checks to confirm that hyperparameter optimization did not make things worse instead of better\n",
    "if accuracy_lr_unoptimized  > accuracy_lr_optimized:  print(\"WARNING: LR optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_nb_unoptimized  > accuracy_nb_optimized:  print(\"WARNING: NB optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_knn_unoptimized > accuracy_knn_optimized: print(\"WARNING: KNN optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_svm_unoptimized > accuracy_svm_optimized: print(\"WARNING: SVM optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_dt_unoptimized  > accuracy_dt_optimized:  print(\"WARNING: DT optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_rf_unoptimized  > accuracy_rf_optimized:  print(\"WARNING: RF optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_gb_unoptimized  > accuracy_gb_optimized:  print(\"WARNING: GB optimization made the accuracy worse, please tweak hyperparameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eF4b8omCzche"
   },
   "source": [
    "# Model training with Deep Learning classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwSNcKXzhD3H"
   },
   "source": [
    "## MLP Multi-Layer Perceptron\n",
    "\n",
    "MLPClassifier is a class in scikit-learn that represents a Multi-layer Perceptron (MLP) classifier, which is a type of artificial neural network.\n",
    "\n",
    "An MLP is a feedforward neural network that consists of multiple layers of nodes (neurons) and can learn complex patterns and relationships in data.\n",
    "\n",
    "The MLPClassifier is specifically designed for classification tasks.\n",
    "\n",
    "Example of all hyperparameters:\n",
    "- hidden_layer_sizes=(100, 50),  # Architecture of hidden layers\n",
    "- activation='relu',             # Activation function ('relu' is common)\n",
    "- solver='adam',                 # Optimization solver\n",
    "- alpha=0.0001,                  # L2 penalty (regularization)\n",
    "- batch_size='auto',             # Size of mini-batches ('auto' is adaptive)\n",
    "- learning_rate='constant',      # Learning rate schedule\n",
    "- learning_rate_init=0.001,      # Initial learning rate\n",
    "- max_iter=500,                  # Maximum number of iterations\n",
    "- shuffle=True,                  # Shuffle data in each iteration\n",
    "- random_state=42,               # Random seed for reproducibility\n",
    "- verbose=True                   # Print progress during training\n",
    "\n",
    "\n",
    "Multi-Layer Perceptron (MLP) classifier with three or more hidden layers is typically considered a deep learning model. The term \"deep\" in deep learning refers to the presence of multiple layers in the neural network architecture. While there's no strict definition of how many layers constitute a \"deep\" network, models with three or more hidden layers are commonly regarded as deep neural networks.\n",
    "\n",
    "MLP classifiers, being feedforward neural networks (FNN) with multiple layers, can learn complex patterns and representations from data, making them suitable for various classification tasks. The depth of the network allows it to learn hierarchical features and capture intricate relationships within the data, leading to improved performance on tasks with large and complex datasets.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Feedforward_neural_network\n",
    "A feedforward neural network (FNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers.[2] Its flow is uni-directional, meaning that the information in the model flows in only one directionforwardfrom the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops,[2] in contrast to recurrent neural networks,[3] which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method[4][5][6][7][8] and are colloquially referred to as the \"vanilla\" neural networks.[9]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lu5EaAdJ7_Ou"
   },
   "outputs": [],
   "source": [
    "# Sanity check to confirm X_train and y_train have equal number of samples\n",
    "print(f\"X_train has \", len(X_train), \"samples\")\n",
    "print(f\"y_train has \", len(y_train), \"samples\")\n",
    "if ( len(X_train) != len(y_train) ):\n",
    "  raise ValueError (\"X_train and y_train are different lengths, please investigate!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSODW4zhEZAV"
   },
   "source": [
    "### default params\n",
    "MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tG_Awu9z_vDC"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "clf = MLPClassifier(random_state=42)   #hidden_layer_sizes can be added here as tuples, see hyperparameter cell for an example\n",
    "\n",
    "default_params = clf.get_params()\n",
    "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "print(f\"-----------------------------------------\")\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Evaluate the model on val data\n",
    "val_accuracy = clf.score(X_val, y_val)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(f\"-----------------------------------------\")\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "test_accuracy_mlp_unoptimized  = test_accuracy\n",
    "train_accuracy_mlp_unoptimized = train_accuracy\n",
    "val_accuracy_mlp_unoptimized   = val_accuracy\n",
    "\n",
    "# save accuracy for later comparison\n",
    "accuracy_mlp_unoptimized = accuracy\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_mlp_unoptimized      = Accuracy\n",
    "sensitivity_mlp_unoptimized   = Sensitivity\n",
    "specificity_mlp_unoptimized   = Specificity\n",
    "geometricmean_mlp_unoptimized = GeometricMean\n",
    "precision_mlp_unoptimized     = Precision\n",
    "recall_mlp_unoptimized        = Recall\n",
    "f1_mlp_unoptimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM2ojMz4hSwC"
   },
   "source": [
    "### optimized params\n",
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UtoDTXchRKr"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of the model\n",
    "clf = MLPClassifier()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,)],  #also tried (64,32)), (64,32,16), (128,64,32) as tuples for hidden layers, but default (100,) was best\n",
    "    'max_iter': [200],                        # also tried 100, 300\n",
    "    'alpha': [0.0001],                        #also tried 0.001, 0.01\n",
    "    'activation': ['relu'],                   #also tried tanh\n",
    "    'learning_rate': ['constant'],            #also tried adaptive\n",
    "    'random_state': [42]                      #for reproducible results\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1, verbose=verbosity)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "print(f\"Performing grid_search to find optimal parameters\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_scores = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Scores:\", best_scores)\n",
    "\n",
    "# Create a new instance of the model with the best hyperparameters\n",
    "clf = MLPClassifier(**best_params)\n",
    "\n",
    "# Fit the model to the training data\n",
    "print(f\"Fitting the model with best_params {best_params}\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "mlp_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "mlp_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "mlp_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model on training data\n",
    "print(f\"-----------------------------------------\")\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Evaluate the model on val data\n",
    "val_accuracy = clf.score(X_val, y_val)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "print(f\"-----------------------------------------\")\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "test_accuracy_mlp_optimized  = test_accuracy\n",
    "train_accuracy_mlp_optimized = train_accuracy\n",
    "val_accuracy_mlp_optimized   = val_accuracy\n",
    "\n",
    "\n",
    "# save best parameters for later comparison\n",
    "best_params_mlp = best_params\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_mlp_optimized      = Accuracy\n",
    "sensitivity_mlp_optimized   = Sensitivity\n",
    "specificity_mlp_optimized   = Specificity\n",
    "geometricmean_mlp_optimized = GeometricMean\n",
    "precision_mlp_optimized     = Precision\n",
    "recall_mlp_optimized        = Recall\n",
    "f1_mlp_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZiacNkAz5ob"
   },
   "source": [
    "## Sequential FNN\n",
    "(does not require time steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POZIobH6T2NS"
   },
   "source": [
    "\n",
    "In the context of the Keras library, Sequential() is not a classifier itself, but rather a type of model architecture. It is used to create sequential models, which are a linear stack of layers.\n",
    "\n",
    "These models are typically used for building feedforward neural networks (FNNs), where the data flows sequentially from the input layer through one or more hidden layers to the output layer. Each layer in a sequential model has connections only to the layers that follow it in the model.\n",
    "\n",
    "You can use different types of layers such as Dense, Dropout, Conv1D, Conv2D, LSTM, etc., in a Sequential() model depending on the type of problem you are solving. Once the layers are added to the model, you compile it with an optimizer, a loss function, and optionally, performance metrics. After compilation, you can train the model on your data using the fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dr4SGy4fGor2"
   },
   "outputs": [],
   "source": [
    "# row, columns in X_train\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCXa0b49Goud"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TYqNHTR7BWb"
   },
   "outputs": [],
   "source": [
    "# Sanity check to confirm X_train and y_train have equal number of samples\n",
    "print(f\"X_train has \", len(X_train), \"samples\")\n",
    "print(f\"y_train has \", len(y_train), \"samples\")\n",
    "if ( len(X_train) != len(y_train) ):\n",
    "  raise ValueError (\"X_train and y_train are different lengths, please investigate!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQhxSoEyEqkY"
   },
   "source": [
    "### default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4qfa3GSpzC8"
   },
   "outputs": [],
   "source": [
    "# row, columns in X_train\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZFxiUiMrsiL"
   },
   "outputs": [],
   "source": [
    "# # no better than previous cell\n",
    "\n",
    "# # Sequential (prior to optimization) -- New test - Backup stable version - with validation\n",
    "\n",
    "\n",
    "# # Define input shape based on the features in X_train\n",
    "# input_shape = X_train.shape[1]\n",
    "\n",
    "\n",
    "# # Define the model\n",
    "# model = Sequential([                                           #Initializes a sequential neural network model\n",
    "#     Dense(64, activation='relu', input_shape=(input_shape,)),  #Add a fully connected layer (also known as a dense layer) with 64 neurons\n",
    "#     Dropout(0.5),                                              #Optional dropout layer for regularization to randomly sets a fraction of input units to zero during training to prevent overfitting\n",
    "#     Dense(32, activation='tanh'),                              #Adds another fully connected layer with 32 neurons and RtanheLU activation function.\n",
    "#     Dense(1, activation='sigmoid')                             # Output layer with sigmoid activation for binary classification\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Print model summary\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Model Summary\")\n",
    "# print(model.summary())\n",
    "\n",
    "# # Train the model\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Training the model\")\n",
    "# history = model.fit(X_train, y_train, epochs=epoch_count, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# # Evaluate the model on training data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on training data\")\n",
    "# train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "# print(\"Training Loss:\", train_loss)\n",
    "# print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the model on test data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on test data\")\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Test Loss:\", test_loss)\n",
    "# print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# test_accuracy_sequential_unoptimized  = test_accuracy\n",
    "# test_loss_sequential_unoptimized      = test_loss\n",
    "# train_accuracy_sequential_unoptimized = train_accuracy\n",
    "# train_loss_sequential_unoptimized     = train_loss\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3How7jnzGu1"
   },
   "outputs": [],
   "source": [
    "# # no better than previous cell\n",
    "\n",
    "# # Sequential (prior to optimization) -- New test\n",
    "\n",
    "\n",
    "# # Define input shape based on the features in X_train\n",
    "# input_shape = X_train.shape[1]\n",
    "\n",
    "# # Define the model\n",
    "# model = Sequential([\n",
    "#     Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "#     Dropout(0.5),  # Optional dropout layer for regularization\n",
    "#     Dense(64, activation='tanh'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(32, activation='tanh'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(16, activation='tanh'),   # add another hidden layer\n",
    "#     Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Print model summary\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Model Summary\")\n",
    "# print(model.summary())\n",
    "\n",
    "# # Train the model\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Training the model\")\n",
    "# history = model.fit(X_train, y_train, epochs=epoch_count, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# # Evaluate the model on training data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on training data\")\n",
    "# train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "# print(\"Training Loss:\", train_loss)\n",
    "# print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the model on test data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on test data\")\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Test Loss:\", test_loss)\n",
    "# print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# test_accuracy_sequential_unoptimized  = test_accuracy\n",
    "# test_loss_sequential_unoptimized      = test_loss\n",
    "# train_accuracy_sequential_unoptimized = train_accuracy\n",
    "# train_loss_sequential_unoptimized     = train_loss\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MnAyWYiiEbO"
   },
   "outputs": [],
   "source": [
    "# # no better than previous cell\n",
    "\n",
    "# # Test FNN on different activation functions:\n",
    "\n",
    "# # Define a list of activation functions to test\n",
    "# activation_functions = ['sigmoid', 'linear', 'tanh', 'relu']\n",
    "# activation_functions = ['relu']  #after testing, relu was the best\n",
    "\n",
    "# # Dictionary to store results\n",
    "# results = {'Activation Function': [],\n",
    "#            'Train Loss': [],\n",
    "#            'Train Accuracy': [],\n",
    "#            'Test Loss': [],\n",
    "#            'Test Accuracy': []}\n",
    "\n",
    "# # Define input shape based on the features in X_train\n",
    "# input_shape = X_train.shape[1]\n",
    "\n",
    "# for activation_function in activation_functions:\n",
    "#     # Define the model\n",
    "#     model = Sequential([\n",
    "#         Dense(64, activation=activation_function, input_shape=(input_shape,)),\n",
    "#         Dropout(0.5),  # Optional dropout layer for regularization\n",
    "#         Dense(32, activation=activation_function),\n",
    "#         Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "#     ])\n",
    "\n",
    "#     # Compile the model\n",
    "#     model.compile(optimizer='adam',\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "#     # Print model summary\n",
    "#     print(f\"-----------------------------------------\")\n",
    "#     print(f\"Model Summary - Activation Function: {activation_function}\")\n",
    "#     print(model.summary())\n",
    "\n",
    "#     # Train the model\n",
    "#     print(f\"-----------------------------------------\")\n",
    "#     print(f\"Training the model - Activation Function: {activation_function}\")\n",
    "#     history = model.fit(X_train, y_train, epochs=epoch_count, batch_size=32, validation_split=0.2)\n",
    "\n",
    "#     # Evaluate the model on training data\n",
    "#     print(f\"-----------------------------------------\")\n",
    "#     print(f\"Evaluating the model on training data - Activation Function: {activation_function}\")\n",
    "#     train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "#     print(\"Training Loss:\", train_loss)\n",
    "#     print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "#     # Evaluate the model on test data\n",
    "#     print(f\"-----------------------------------------\")\n",
    "#     print(f\"Evaluating the model on test data - Activation Function: {activation_function}\")\n",
    "#     test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "#     print(\"Test Loss:\", test_loss)\n",
    "#     print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "#     # Save results\n",
    "#     results['Activation Function'].append(activation_function)\n",
    "#     results['Train Loss'].append(train_loss)\n",
    "#     results['Train Accuracy'].append(train_accuracy)\n",
    "#     results['Test Loss'].append(test_loss)\n",
    "#     results['Test Accuracy'].append(test_accuracy)\n",
    "\n",
    "# # Convert results to a DataFrame\n",
    "# results_df = pd.DataFrame(results)\n",
    "\n",
    "# # Print results DataFrame\n",
    "# print(results_df)\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5MptzbZE88-"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94CSwoP_zAx0"
   },
   "source": [
    "### optimized params\n",
    "FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lman7Ed5nIW1"
   },
   "outputs": [],
   "source": [
    "# # to-do: add another dropout after dense(32), and another dense layer with 16 neurons\n",
    "\n",
    "# # Sequential (prior to optimization) -- Backup\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# # Define input shape based on the features in X_train\n",
    "# input_shape = X_train.shape[1]\n",
    "\n",
    "# # Define the model\n",
    "# model = Sequential([                                           #Initializes a sequential neural network model\n",
    "#     Dense(64, activation='relu', input_shape=(input_shape,)),  #Add a fully connected layer (also known as a dense layer) with 64 neurons\n",
    "#     Dropout(0.5),                                              #Optional dropout layer for regularization to randomly sets a fraction of input units to zero during training to prevent overfitting\n",
    "#     Dense(32, activation='relu'),                              #Adds another fully connected layer with 32 neurons and ReLU activation function.\n",
    "#     Dense(1, activation='sigmoid')                             # Output layer with sigmoid activation for binary classification\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Print model summary\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Model Summary\")\n",
    "# print(model.summary())\n",
    "\n",
    "# # Train the model\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Training the model\")\n",
    "# history = model.fit(X_train, y_train, epochs=epoch_count, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Evaluate the model on training data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on training data\")\n",
    "# train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "# print(\"Training Loss:\", train_loss)\n",
    "# print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the model on test data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on test data\")\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Test Loss:\", test_loss)\n",
    "# print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the model on validation data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on validation data\")\n",
    "# val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "# print(\"Val Loss:\", val_loss)\n",
    "# print(\"Val Accuracy:\", val_accuracy)\n",
    "# print(f\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# test_accuracy_sequential_unoptimized  = test_accuracy\n",
    "# test_loss_sequential_unoptimized      = test_loss\n",
    "# train_accuracy_sequential_unoptimized = train_accuracy\n",
    "# train_loss_sequential_unoptimized     = train_loss\n",
    "# val_accuracy_sequential_unoptimized   = val_accuracy\n",
    "# val_loss_sequential_unoptimized       = val_loss\n",
    "\n",
    "# # Extracting accuracy and loss history from the training, will be used later for graphing convergence\n",
    "# train_loss_history_unoptimized     = history.history['loss']\n",
    "# train_accuracy_history_unoptimized = history.history['accuracy']\n",
    "# val_loss_history_unoptimized       = history.history['val_loss']\n",
    "# val_accuracy_history_unoptimized   = history.history['val_accuracy']\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# accuracy_sequential_unoptimized      = Accuracy\n",
    "# sensitivity_sequential_unoptimized   = Sensitivity\n",
    "# specificity_sequential_unoptimized   = Specificity\n",
    "# geometricmean_sequential_unoptimized = GeometricMean\n",
    "# precision_sequential_unoptimized     = Precision\n",
    "# recall_sequential_unoptimized        = Recall\n",
    "# f1_sequential_unoptimized            = F1\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x70DkT-W7ENM"
   },
   "outputs": [],
   "source": [
    "# Sanity check to confirm X_train and y_train have equal number of samples\n",
    "print(f\"X_train has \", len(X_train), \"samples\")\n",
    "print(f\"y_train has \", len(y_train), \"samples\")\n",
    "if ( len(X_train) != len(y_train) ):\n",
    "  raise ValueError (\"X_train and y_train are different lengths, please investigate!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xw_AJcC70mlg"
   },
   "outputs": [],
   "source": [
    "# # perform Sequential hyperparameter optimization\n",
    "\n",
    "# # Define a function to create a model\n",
    "# def create_model(units=64, dropout=0.5):\n",
    "#     model = Sequential([\n",
    "#         Dense(units, activation='relu', input_shape=(input_shape,)),\n",
    "#         Dropout(dropout),\n",
    "#         Dense(32, activation='relu'),\n",
    "#         Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Define input shape based on the features in X_train\n",
    "# input_shape = X_train.shape[1]\n",
    "\n",
    "# # Create a wrapper class around the Keras model\n",
    "# class KerasClassifierWrapper:\n",
    "#     def __init__(self, units=64, dropout=0.5, epochs=epoch_count, batch_size=32, verbose=0):\n",
    "#         self.units = units\n",
    "#         self.dropout = dropout\n",
    "#         self.epochs = epochs\n",
    "#         self.batch_size = batch_size\n",
    "#         self.verbose = verbose\n",
    "#         self.model = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         self.model = create_model(units=self.units, dropout=self.dropout)\n",
    "#         self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return (self.model.predict(X) > 0.5).astype(int)\n",
    "\n",
    "#     def get_params(self, deep=True):\n",
    "#         return {\n",
    "#             'units': self.units,\n",
    "#             'dropout': self.dropout,\n",
    "#             'epochs': self.epochs,\n",
    "#             'batch_size': self.batch_size,\n",
    "#             'verbose': self.verbose\n",
    "#         }\n",
    "\n",
    "#     def set_params(self, **params):\n",
    "#         for param, value in params.items():\n",
    "#             setattr(self, param, value)\n",
    "#         return self\n",
    "\n",
    "# # Create an instance of the wrapper class\n",
    "# model = KerasClassifierWrapper()\n",
    "\n",
    "# # Define the hyperparameters grid to search\n",
    "# param_grid = {\n",
    "#     'units': [32],      #also tried 64,128\n",
    "#     'dropout': [0.3],   #also tried 0.5, 0.7\n",
    "#     'activation': ['tanh'] # relu almost as good as tanh ,also tried sigmoid and linear, but accuracy was lower\n",
    "# }\n",
    "# #param_grid = {           #smaller faster version for debugging\n",
    "# #    'units': [32],\n",
    "# #    'dropout': [0.3]\n",
    "# #}\n",
    "\n",
    "# # Create GridSearchCV instance\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_count, scoring=make_scorer(accuracy_score), verbose=verbosity)\n",
    "\n",
    "# # Perform grid search\n",
    "# print(f\"--------------------------------------------------\")\n",
    "# print(f\"Performing GridSearchCV to find optimal parameters\")\n",
    "# grid_search_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print best parameters and results\n",
    "# print(\"Best Parameters:\", grid_search_result.best_params_)\n",
    "# print(\"Best Accuracy:\", grid_search_result.best_score_)\n",
    "# print('\\n')\n",
    "\n",
    "# # final cross validation\n",
    "# cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "# print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "# print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "# sequential_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "# sequential_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "# sequential_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# # Evaluate the best model on training data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on training data\")\n",
    "# best_model = grid_search_result.best_estimator_\n",
    "# train_loss, train_accuracy = best_model.model.evaluate(X_train, y_train)\n",
    "# print(\"Train Loss:\", train_loss)\n",
    "# print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the best model on test data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on test data\")\n",
    "# best_model = grid_search_result.best_estimator_\n",
    "# test_loss, test_accuracy = best_model.model.evaluate(X_test, y_test)\n",
    "# print(\"Test Loss:\", test_loss)\n",
    "# print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# # Evaluate the best model on validation data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on validation data\")\n",
    "# best_model = grid_search_result.best_estimator_\n",
    "# val_loss, val_accuracy = best_model.model.evaluate(X_val, y_val)\n",
    "# print(\"Val Loss:\", val_loss)\n",
    "# print(\"Val Accuracy:\", val_accuracy)\n",
    "# print(f\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# test_accuracy_sequential_optimized  = test_accuracy\n",
    "# test_loss_sequential_optimized      = test_loss\n",
    "# train_accuracy_sequential_optimized = train_accuracy\n",
    "# train_loss_sequential_optimized     = train_loss\n",
    "# val_accuracy_sequential_optimized   = val_accuracy\n",
    "# val_loss_sequential_optimized       = val_loss\n",
    "\n",
    "# # Extracting accuracy and loss history from the training, will be used later for graphing convergence\n",
    "# train_loss_history_optimized     = history.history['loss']\n",
    "# train_accuracy_history_optimized = history.history['accuracy']\n",
    "# val_loss_history_optimized       = history.history['val_loss']\n",
    "# val_accuracy_history_optimized   = history.history['val_accuracy']\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# accuracy_sequential_optimized      = Accuracy\n",
    "# sensitivity_sequential_optimized   = Sensitivity\n",
    "# specificity_sequential_optimized   = Specificity\n",
    "# geometricmean_sequential_optimized = GeometricMean\n",
    "# precision_sequential_optimized     = Precision\n",
    "# recall_sequential_optimized        = Recall\n",
    "# f1_sequential_optimized            = F1\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kB0oblW5itPW"
   },
   "outputs": [],
   "source": [
    "# Call previously defined functions to create graphs that show how Training/Validation loss and accuracy converge\n",
    "# It is important that the lines converge, if they do not, add more data, more epochs, or try different hyperparameter optimizations\n",
    "\n",
    "# print current history values, use end=\" \" to print all output on single line\n",
    "print(f\"\\n\\ntrain_loss_history_unoptimized\", end=\" \") \n",
    "for value in train_loss_history_unoptimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\ntrain_loss_history_optimized\", end=\" \") \n",
    "for value in train_loss_history_optimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\nval_loss_history_unoptimized\", end=\" \") \n",
    "for value in val_loss_history_unoptimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\nval_loss_history_optimized\", end=\" \") \n",
    "for value in val_loss_history_optimized: print(f\"{value:.4f}\", end=\" \")\n",
    "\n",
    "\n",
    "# sanity check to confirm the required values have already been collected, set to zeros if needed to avoid undef errors\n",
    "if len(train_loss_history_unoptimized) != epoch_count: print(\"ERROR: incorrect len() of train_loss_history_unoptimized, resetting to all zeros\") ; train_loss_history_unoptimized = [0] * epoch_count\n",
    "if len(train_loss_history_optimized)   != epoch_count: print(\"ERROR: incorrect len() of train_loss_history_optimized,   resetting to all zeros\") ; train_loss_history_optimized   = [0] * epoch_count\n",
    "if len(val_loss_history_unoptimized)   != epoch_count: print(\"ERROR: incorrect len() of val_loss_history_unoptimized,   resetting to all zeros\") ; val_loss_history_unoptimized   = [0] * epoch_count\n",
    "if len(val_loss_history_optimized)     != epoch_count: print(\"ERROR: incorrect len() of  val_loss_history_optimized,    resetting to all zeros\") ; val_loss_history_optimized     = [0] * epoch_count\n",
    "\n",
    "# create the convergence graphs\n",
    "plot_loss_history(train_loss_history_unoptimized, train_loss_history_optimized, val_loss_history_unoptimized, val_loss_history_optimized, epoch_count)\n",
    "plot_accuracy_history(train_accuracy_history_unoptimized, train_accuracy_history_optimized, val_accuracy_history_unoptimized, val_accuracy_history_optimized, epoch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISQCV20uyOxr"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n498Av_bqlRU"
   },
   "source": [
    "## Sequential FNN + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLZM211BE0Vo"
   },
   "source": [
    "### default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQbRmQpV3X54"
   },
   "outputs": [],
   "source": [
    "# Its important to note that LSTM models can be computationally expensive to train.\n",
    "# Depending on the size of your data and complexity of your model, training may take a significant amount of time.\n",
    "\n",
    "# NOTE: training the model with model.fit()  is ~10x faster when using a GPU!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOrE6RI17c5A"
   },
   "outputs": [],
   "source": [
    "# # FNN-LSTM\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# # Define input shape based on the features in X_train\n",
    "# input_shape = X_train.shape[1]\n",
    "\n",
    "# # Define the model\n",
    "# model = Sequential([\n",
    "#     LSTM(64, activation='relu', input_shape=(input_shape, 1), return_sequences=True),\n",
    "#     Dropout(0.5),  # Optional dropout layer for regularization\n",
    "#     LSTM(32, activation='relu'),\n",
    "#     Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Compiling model\")\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Print model summary\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Model Summary\")\n",
    "# print(model.summary())\n",
    "\n",
    "# # Train the model\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Training the model\")\n",
    "# history = model.fit(X_train, y_train, epochs=epoch_count, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Evaluate the model on training data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on training data\")\n",
    "# train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "# print(\"Training Loss:\", train_loss)\n",
    "# print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the model on test data\n",
    "# # Now that we have trained our LSTM model, its time to evaluate its performance.\n",
    "# # In TensorFlow, we can do this by using the `evaluate()` method of the model object.\n",
    "# #\n",
    "# # First, we need to load the test data and preprocess it in the same way as we did for the training data.\n",
    "# # Once we have preprocessed the test data, we can evaluate the model using the `evaluate()` method.\n",
    "# # This method takes two arguments: the test data (X_test) and its corresponding labels (y_test).\n",
    "# # Evaluate the model on test data\n",
    "\n",
    "\n",
    "# # Evaluate the model on test data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on test data\")\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Test Loss:\", test_loss)\n",
    "# print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the model on validation data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on validation data\")\n",
    "# val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "# print(\"Val Loss:\", val_loss)\n",
    "# print(\"Val Accuracy:\", val_accuracy)\n",
    "# print(f\"-----------------------------------------\")\n",
    "\n",
    "# #\n",
    "# # The `evaluate()` method returns two values: the loss and accuracy of the model on the test data.\n",
    "# # The loss is a measure of how well the model is able to predict the correct output, while the accuracy is a measure of how often the model is correct.\n",
    "# #\n",
    "# # Its important to note that we should only use the test data for evaluation purposes and not for training.\n",
    "# # Using the same data for both training and evaluation can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data.\n",
    "# #\n",
    "# # In addition to evaluating the overall performance of our model, we can also look at individual predictions using the `predict()` method.\n",
    "# # This method takes a single input example and returns its predicted output.\n",
    "# #\n",
    "# ## Make a prediction on a single input example\n",
    "# #example = ...\n",
    "# #prediction = model.predict(preprocess_data(example))\n",
    "# #\n",
    "# # By examining individual predictions, we can gain insights into how our model is making decisions and identify areas where it may be making errors.\n",
    "# # This can help us improve our model and make it more accurate for future predictions.\n",
    "\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# test_accuracy_lstm_unoptimized  = test_accuracy\n",
    "# test_loss_lstm_unoptimized      = test_loss\n",
    "# train_accuracy_lstm_unoptimized = train_accuracy\n",
    "# train_loss_lstm_unoptimized     = train_loss\n",
    "# val_accuracy_lstm_unoptimized   = val_accuracy\n",
    "# val_loss_lstm_unoptimized       = val_loss\n",
    "\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# accuracy_lstm_unoptimized      = Accuracy\n",
    "# sensitivity_lstm_unoptimized   = Sensitivity\n",
    "# specificity_lstm_unoptimized   = Specificity\n",
    "# geometricmean_lstm_unoptimized = GeometricMean\n",
    "# precision_lstm_unoptimized     = Precision\n",
    "# recall_lstm_unoptimized        = Recall\n",
    "# f1_lstm_unoptimized            = F1\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXboS9MhGYDh"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STBh32-tGLEo"
   },
   "source": [
    "### optimized params\n",
    "FNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrypcDrF7dAh"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # perform LSTM hyperparameter optimization  (without GPU, takes approx 60 minutes to run with units=32,64,128 dropout=0.3,0.5,0.7)\n",
    "# # This method is different than Sequential optimization, maybe use the next cell instead for consistency\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Define a function to create a model\n",
    "# def create_model(units=64, dropout=0.5):\n",
    "#     model = Sequential([\n",
    "#         LSTM(units, activation='relu', input_shape=(X_train.shape[1], 1), return_sequences=True),\n",
    "#         Dropout(dropout),\n",
    "#         LSTM(units//2, activation='relu'),\n",
    "#         Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Define hyperparameters to search\n",
    "# #units_list = [32, 64, 128]\n",
    "# #dropout_list = [0.3, 0.5, 0.7]\n",
    "# units_list = [32]              #use smaller list of parameters to speed up debugging phase\n",
    "# dropout_list = [0.3]               #use smaller list of parameters to speed up debugging phase\n",
    "\n",
    "\n",
    "# # initialize variables\n",
    "# best_accuracy = 0\n",
    "# best_params = {}\n",
    "\n",
    "# # Loop through all combinations of hyperparameters\n",
    "# print(f\"\\nLooping through all combinations of hyperparameters\")\n",
    "# for units in units_list:\n",
    "#     for dropout in dropout_list:\n",
    "#         print(f\"Evaluating model with units={units}, dropout={dropout}\")\n",
    "\n",
    "#         # Create and compile the model\n",
    "#         model = create_model(units=units, dropout=dropout)\n",
    "\n",
    "#         # Train the model\n",
    "#         history = model.fit(X_train, y_train, epochs=epoch_count, batch_size=32, validation_split=0.2, verbose=epoch_verbosity)\n",
    "\n",
    "#         # Evaluate the model on validation data\n",
    "#         val_loss, val_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "#         print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "#         # Update best accuracy and parameters if necessary\n",
    "#         if val_accuracy > best_accuracy:\n",
    "#             best_accuracy = val_accuracy\n",
    "#             best_params = {'units': units, 'dropout': dropout}\n",
    "\n",
    "# # Train the final model with the best parameters\n",
    "# print(f\"\\nBest parameters: {best_params}\")\n",
    "# print(f\"Training the final model with the best parameters...\")\n",
    "# model = create_model(**best_params)\n",
    "# history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# # final cross validation\n",
    "# cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "# print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "# print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "# lstm_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "# lstm_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "# lstm_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# # Evaluate the best model on training data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on training data\")\n",
    "# train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "# print(\"Training Loss:\", train_loss)\n",
    "# print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the best model on test data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"\\nEvaluating the model on test data\")\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(\"Test Loss:\", test_loss)\n",
    "# print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the best model on validation data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"\\nEvaluating the model on validation data\")\n",
    "# val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "# print(\"Val Loss:\", val_loss)\n",
    "# print(\"Val Accuracy:\", val_accuracy)\n",
    "# print(f\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# test_accuracy_lstm_optimized  = test_accuracy\n",
    "# test_loss_lstm_optimized      = test_loss\n",
    "# train_accuracy_lstm_optimized = train_accuracy\n",
    "# train_loss_lstm_optimized     = train_loss\n",
    "# val_accuracy_lstm_optimized   = val_accuracy\n",
    "# val_loss_lstm_optimized       = val_loss\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# accuracy_lstm_optimized      = Accuracy\n",
    "# sensitivity_lstm_optimized   = Sensitivity\n",
    "# specificity_lstm_optimized   = Specificity\n",
    "# geometricmean_lstm_optimized = GeometricMean\n",
    "# precision_lstm_optimized     = Precision\n",
    "# recall_lstm_optimized        = Recall\n",
    "# f1_lstm_unoptimized            = F1\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdK8A5_pd2F1"
   },
   "outputs": [],
   "source": [
    "# Call previously defined functions to create graphs that show how Training/Validation loss and accuracy converge\n",
    "# It is important that the lines converge, if they do not, add more data, more epochs, or try different hyperparameter optimizations\n",
    "\n",
    "# print current history values, use end=\" \" to print all output on single line\n",
    "print(f\"\\n\\ntrain_loss_history_unoptimized\", end=\" \") \n",
    "for value in train_loss_history_unoptimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\ntrain_loss_history_optimized\", end=\" \") \n",
    "for value in train_loss_history_optimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\nval_loss_history_unoptimized\", end=\" \") \n",
    "for value in val_loss_history_unoptimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\nval_loss_history_optimized\", end=\" \") \n",
    "for value in val_loss_history_optimized: print(f\"{value:.4f}\", end=\" \")\n",
    "\n",
    "\n",
    "# sanity check to confirm the required values have already been collected, set to zeros if needed to avoid undef errors\n",
    "if len(train_loss_history_unoptimized) != epoch_count: print(\"ERROR: incorrect len() of train_loss_history_unoptimized, resetting to all zeros\") ; train_loss_history_unoptimized = [0] * epoch_count\n",
    "if len(train_loss_history_optimized)   != epoch_count: print(\"ERROR: incorrect len() of train_loss_history_optimized,   resetting to all zeros\") ; train_loss_history_optimized   = [0] * epoch_count\n",
    "if len(val_loss_history_unoptimized)   != epoch_count: print(\"ERROR: incorrect len() of val_loss_history_unoptimized,   resetting to all zeros\") ; val_loss_history_unoptimized   = [0] * epoch_count\n",
    "if len(val_loss_history_optimized)     != epoch_count: print(\"ERROR: incorrect len() of  val_loss_history_optimized,    resetting to all zeros\") ; val_loss_history_optimized     = [0] * epoch_count\n",
    "\n",
    "# create the convergence graphs\n",
    "plot_loss_history(train_loss_history_unoptimized, train_loss_history_optimized, val_loss_history_unoptimized, val_loss_history_optimized, epoch_count)\n",
    "plot_accuracy_history(train_accuracy_history_unoptimized, train_accuracy_history_optimized, val_accuracy_history_unoptimized, val_accuracy_history_optimized, epoch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8PHlZkHIxO9"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH3LYBhB52Tq"
   },
   "source": [
    "## reshape X_train, X_test to include time steps for SimpleRNN and GRU\n",
    "\n",
    "The following model expects sequential (ie time-series) data, so the dataset will need  \"time steps\"  for the SimpleRNN and Gated Recurrent Unit (GRU) models? (which also reshapes X_train,X_test).\n",
    "\n",
    "If the data does not include time steps, you will get an error about the shape being incorrect.\n",
    "\n",
    "The error message indicates that the input to the GRU layer has an incorrect shape. The GRU layer expects input data to have three dimensions: (batch size, time steps, features). In this case, the input data only has two dimensions: (batch size, features).\n",
    "\n",
    "To fix the issue, reshape the input data to have three dimensions. This can be done using the reshape() method.\n",
    "\n",
    "After reshaping the input data, the model can be trained and evaluated successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auX4hG3izG11"
   },
   "outputs": [],
   "source": [
    "# reshape X_train to add time steps (expected by this model)\n",
    "\n",
    "# Assuming X_train has shape (samples, features)\n",
    "# Define the number of time steps\n",
    "time_steps = 1  # Adjust this value based on your data and problem\n",
    "\n",
    "# Reshape X_train to include time steps\n",
    "X_train_with_time_steps = np.zeros((X_train.shape[0] - time_steps + 1, time_steps, X_train.shape[1]))\n",
    "for i in range(len(X_train) - time_steps + 1):\n",
    "    X_train_with_time_steps[i] = X_train[i:i+time_steps]\n",
    "\n",
    "# Now X_train_with_time_steps has shape (samples, time_steps, features)\n",
    "\n",
    "\n",
    "# reshape X_test to add time steps (expected by this model)\n",
    "\n",
    "# Assuming X_test has shape (samples, features)\n",
    "# Define the number of time steps\n",
    "time_steps = 1  # Adjust this value based on your data and problem\n",
    "\n",
    "# Reshape X_test to include time steps\n",
    "X_test_with_time_steps = np.zeros((X_test.shape[0] - time_steps + 1, time_steps, X_test.shape[1]))\n",
    "for i in range(len(X_test) - time_steps + 1):\n",
    "    X_test_with_time_steps[i] = X_test[i:i+time_steps]\n",
    "\n",
    "# Reshape X_val to include time steps\n",
    "X_val_with_time_steps = np.zeros((X_val.shape[0] - time_steps + 1, time_steps, X_val.shape[1]))\n",
    "for i in range(len(X_val) - time_steps + 1):\n",
    "    X_val_with_time_steps[i] = X_val[i:i+time_steps]\n",
    "\n",
    "# Now X_test_with_time_steps has shape (samples, time_steps, features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDBqcaSo1Yyh"
   },
   "source": [
    "## SimpleRNN\n",
    "### (needed reshaping to add time steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEFDLtWsE_Ic"
   },
   "source": [
    "### default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O270ZEpQzGxO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN\n",
    "\n",
    "# Define input shape based on the features in X_train_with_time_steps\n",
    "input_shape = X_train_with_time_steps.shape[1:]\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    SimpleRNN(units=64, input_shape=input_shape),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Model Summary\")\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Training the model\")\n",
    "history = model.fit(X_train_with_time_steps, y_train, epochs=epoch_count, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# Evaluate the model on training data\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Evaluating the model on training data\")\n",
    "train_loss, train_accuracy = model.evaluate(X_train_with_time_steps, y_train)\n",
    "print(\"Training Loss:\", train_loss)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on test data\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Evaluating the model on test data\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test_with_time_steps, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Evaluating the model on validation data\")\n",
    "val_loss, val_accuracy = model.evaluate(X_val_with_time_steps, y_val)\n",
    "print(\"Val Loss:\", val_loss)\n",
    "print(\"Val Accuracy:\", val_accuracy)\n",
    "print(f\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "test_accuracy_simplernn_unoptimized  = test_accuracy\n",
    "test_loss_simplernn_unoptimized      = test_loss\n",
    "train_accuracy_simplernn_unoptimized = train_accuracy\n",
    "train_loss_simplernn_unoptimized     = train_loss\n",
    "val_accuracy_simplernn_unoptimized   = val_accuracy\n",
    "val_loss_simplernn_unoptimized       = val_loss\n",
    "\n",
    "# Extracting accuracy and loss history from the training, will be used later for graphing convergence\n",
    "train_loss_history_unoptimized     = history.history['loss']\n",
    "train_accuracy_history_unoptimized = history.history['accuracy']\n",
    "val_loss_history_unoptimized       = history.history['val_loss']\n",
    "val_accuracy_history_unoptimized   = history.history['val_accuracy']\n",
    "\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_simplernn_unoptimized      = Accuracy\n",
    "sensitivity_simplernn_unoptimized   = Sensitivity\n",
    "specificity_simplernn_unoptimized   = Specificity\n",
    "geometricmean_simplernn_unoptimized = GeometricMean\n",
    "precision_simplernn_unoptimized     = Precision\n",
    "recall_simplernn_unoptimized        = Recall\n",
    "f1_simplernn_unoptimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EW5Of-nMz7Xt"
   },
   "source": [
    "### optimized params\n",
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLO9djUW6o2A"
   },
   "outputs": [],
   "source": [
    "# Sanity check to confirm X_train and y_train have equal number of samples\n",
    "print(f\"X_train_with_time_steps has \", len(X_train_with_time_steps), \"samples\")\n",
    "print(f\"y_train                 has \", len(y_train),                 \"samples\")\n",
    "if ( len(X_train_with_time_steps) != len(y_train) ):\n",
    "  raise ValueError (\"X_train_with_time_steps and y_train are different lengths, please investigate!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4mf9zpLK_nl"
   },
   "outputs": [],
   "source": [
    "# SimpleRNN hyperparameter optimization\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Dropout\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# Define input shape based on the features in X_train\n",
    "input_shape = (X_train_with_time_steps.shape[1], X_train_with_time_steps.shape[2])  # Assuming X_train is 2D\n",
    "\n",
    "# Define a function to create a model\n",
    "def create_model(units=64, dropout=0.5):\n",
    "    model = Sequential([\n",
    "        SimpleRNN(units, input_shape=input_shape),\n",
    "        Dropout(dropout),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a wrapper class around the Keras model\n",
    "class KerasSimpleRNNWrapper:\n",
    "    def __init__(self, units=64, dropout=0.5, epochs=epoch_count, batch_size=32, verbose=0):\n",
    "        self.units = units\n",
    "        self.dropout = dropout\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = create_model(units=self.units, dropout=self.dropout)\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.model.predict(X) > 0.5).astype(int)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'units': self.units,\n",
    "            'dropout': self.dropout,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'verbose': self.verbose\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "# Create an instance of the wrapper class\n",
    "model = KerasSimpleRNNWrapper()\n",
    "\n",
    "# Define the hyperparameters grid to search\n",
    "#param_grid = {\n",
    "#    'units': [32, 64, 128],\n",
    "#    'dropout': [0.3, 0.5, 0.7]\n",
    "#}\n",
    "param_grid = {        #smaller faster version for testing\n",
    "    'units': [32],\n",
    "    'dropout': [0.3]\n",
    "}\n",
    "\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_count, scoring=make_scorer(accuracy_score), verbose=epoch_verbosity)\n",
    "\n",
    "# Perform grid search\n",
    "print(f\"------------------------------------------\")\n",
    "print(f\"Performing GridSearchCV\")\n",
    "grid_search_result = grid_search.fit(X_train_with_time_steps, y_train)\n",
    "\n",
    "# Print best parameters and results\n",
    "print(\"Best Parameters:\", grid_search_result.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search_result.best_score_)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "simplernn_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "simplernn_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "simplernn_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model on training data\n",
    "print(f\"------------------------------------------\")\n",
    "print(f\"Evaluating the model on training data\")\n",
    "best_model = grid_search_result.best_estimator_\n",
    "train_loss, train_accuracy = best_model.model.evaluate(X_train_with_time_steps, y_train)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "# Evaluate the model on test data\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Evaluating the model on test data\")\n",
    "test_loss, test_accuracy = best_model.model.evaluate(X_test_with_time_steps, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Evaluating the model on validation data\")\n",
    "print(f\"-----------------------------------------\")\n",
    "val_loss, val_accuracy = best_model.model.evaluate(X_val_with_time_steps, y_val)\n",
    "print(\"Val Loss:\", val_loss)\n",
    "print(\"Val Accuracy:\", val_accuracy)\n",
    "print(f\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "test_accuracy_simplernn_optimized  = test_accuracy\n",
    "test_loss_simplernn_optimized      = test_loss\n",
    "train_accuracy_simplernn_optimized = train_accuracy\n",
    "train_loss_simplernn_optimized     = train_loss\n",
    "val_accuracy_simplernn_optimized   = val_accuracy\n",
    "val_loss_simplernn_optimized       = val_loss\n",
    "\n",
    "# Extracting accuracy and loss history from the training, will be used later for graphing convergence\n",
    "train_loss_history_optimized     = history.history['loss']\n",
    "train_accuracy_history_optimized = history.history['accuracy']\n",
    "val_loss_history_optimized       = history.history['val_loss']\n",
    "val_accuracy_history_optimized   = history.history['val_accuracy']\n",
    "\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_simplernn_optimized      = Accuracy\n",
    "sensitivity_simplernn_optimized   = Sensitivity\n",
    "specificity_simplernn_optimized   = Specificity\n",
    "geometricmean_simplernn_optimized = GeometricMean\n",
    "precision_simplernn_optimized     = Precision\n",
    "recall_simplernn_optimized        = Recall\n",
    "f1_simplernn_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgJLeTGleDv3"
   },
   "outputs": [],
   "source": [
    "# Call previously defined functions to create graphs that show how Training/Validation loss and accuracy converge\n",
    "# It is important that the lines converge, if they do not, add more data, more epochs, or try different hyperparameter optimizations\n",
    "\n",
    "# print current history values, use end=\" \" to print all output on single line\n",
    "print(f\"\\n\\ntrain_loss_history_unoptimized\", end=\" \") \n",
    "for value in train_loss_history_unoptimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\ntrain_loss_history_optimized\", end=\" \") \n",
    "for value in train_loss_history_optimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\nval_loss_history_unoptimized\", end=\" \") \n",
    "for value in val_loss_history_unoptimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\nval_loss_history_optimized\", end=\" \") \n",
    "for value in val_loss_history_optimized: print(f\"{value:.4f}\", end=\" \")\n",
    "\n",
    "\n",
    "# sanity check to confirm the required values have already been collected, set to zeros if needed to avoid undef errors\n",
    "if len(train_loss_history_unoptimized) != epoch_count: print(\"ERROR: incorrect len() of train_loss_history_unoptimized, resetting to all zeros\") ; train_loss_history_unoptimized = [0] * epoch_count\n",
    "if len(train_loss_history_optimized)   != epoch_count: print(\"ERROR: incorrect len() of train_loss_history_optimized,   resetting to all zeros\") ; train_loss_history_optimized   = [0] * epoch_count\n",
    "if len(val_loss_history_unoptimized)   != epoch_count: print(\"ERROR: incorrect len() of val_loss_history_unoptimized,   resetting to all zeros\") ; val_loss_history_unoptimized   = [0] * epoch_count\n",
    "if len(val_loss_history_optimized)     != epoch_count: print(\"ERROR: incorrect len() of  val_loss_history_optimized,    resetting to all zeros\") ; val_loss_history_optimized     = [0] * epoch_count\n",
    "\n",
    "# create the convergence graphs\n",
    "plot_loss_history(train_loss_history_unoptimized, train_loss_history_optimized, val_loss_history_unoptimized, val_loss_history_optimized, epoch_count)\n",
    "plot_accuracy_history(train_accuracy_history_unoptimized, train_accuracy_history_optimized, val_accuracy_history_unoptimized, val_accuracy_history_optimized, epoch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkjPbEEYEyeT"
   },
   "source": [
    "## SimpleRNN + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm5-pQS2FGtn"
   },
   "source": [
    "### default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ScHi2WBwRc6"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kD5WhHSgE03X"
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, SimpleRNN, LSTM\n",
    "\n",
    "# Define input shape based on the features in X_train_with_time_steps\n",
    "input_shape = X_train_with_time_steps.shape[1:]\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    SimpleRNN(64, input_shape=input_shape, return_sequences=True),  # SimpleRNN layer with 64 units\n",
    "    Dropout(0.5),\n",
    "    LSTM(32),  # LSTM layer with 32 units\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Print model summary\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Model Summary\")\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Training the model\")\n",
    "history = model.fit(X_train_with_time_steps, y_train, epochs=epoch_count, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# Evaluate the model on training data\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Evaluating the model on training data\")\n",
    "train_loss, train_accuracy = model.evaluate(X_train_with_time_steps, y_train)\n",
    "print(\"Training Loss:\", train_loss)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "# Evaluate the model on test data\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Evaluating the model on test data\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test_with_time_steps, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Evaluating the model on validation data\")\n",
    "val_loss, val_accuracy = model.evaluate(X_val_with_time_steps, y_val)\n",
    "print(\"Val Loss:\", val_loss)\n",
    "print(\"Val Accuracy:\", val_accuracy)\n",
    "\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "test_accuracy_simplernn_lstm_unoptimized  = test_accuracy\n",
    "test_loss_simplernn_lstm_unoptimized      = test_loss\n",
    "train_accuracy_simplernn_lstm_unoptimized = train_accuracy\n",
    "train_loss_simplernn_lstm_unoptimized     = train_loss\n",
    "val_accuracy_simplernn_lstm_unoptimized   = val_accuracy\n",
    "val_loss_simplernn_lstm_unoptimized       = val_loss\n",
    "\n",
    "# Extracting accuracy and loss history from the training, will be used later for graphing convergence\n",
    "train_loss_history_unoptimized     = history.history['loss']\n",
    "train_accuracy_history_unoptimized = history.history['accuracy']\n",
    "val_loss_history_unoptimized       = history.history['val_loss']\n",
    "val_accuracy_history_unoptimized   = history.history['val_accuracy']\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_simplernn_lstm_unoptimized      = Accuracy\n",
    "sensitivity_simplernn_lstm_unoptimized   = Sensitivity\n",
    "specificity_simplernn_lstm_unoptimized   = Specificity\n",
    "geometricmean_simplernn_lstm_unoptimized = GeometricMean\n",
    "precision_simplernn_lstm_unoptimized     = Precision\n",
    "recall_simplernn_lstm_unoptimized        = Recall\n",
    "f1_simplernn_lstm_unoptimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUj_U5m2G8Iz"
   },
   "source": [
    "### optimized params\n",
    "RNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x98sxPUVHAef"
   },
   "outputs": [],
   "source": [
    "# SimpleRNN + LSTM hyperparameter optimization\n",
    "\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, SimpleRNN, Dropout\n",
    "#from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# Define input shape based on the features in X_train\n",
    "#input_shape = (X_train_with_time_steps.shape[1], X_train_with_time_steps.shape[2])  # Assuming X_train is 2D\n",
    "input_shape = X_train_with_time_steps.shape[1:]\n",
    "\n",
    "\n",
    "# Define a function to create a model\n",
    "def create_model(units=64, dropout=0.5):\n",
    "    model = Sequential([\n",
    "        SimpleRNN(units, input_shape=input_shape, return_sequences=True),  #input layer\n",
    "        Dropout(dropout),                                                  #Optional dropout layer for regularization to randomly sets a fraction of input units to zero during training to prevent overfitting\n",
    "        LSTM(units, dropout=dropout),                                      #hidden layer\n",
    "        Dropout(dropout),                                                  #Optional dropout layer for regularization to randomly sets a fraction of input units to zero during training to prevent overfitting\n",
    "        #Dense(32, activation='relu'),                                     #add another hidden layer\n",
    "        Dense(1, activation='sigmoid')                                     #output layer\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a wrapper class around the Keras model\n",
    "class KerasSimpleRNNWrapper:\n",
    "    def __init__(self, units=64, dropout=0.5, epochs=epoch_count, batch_size=32, verbose=0):\n",
    "        self.units = units\n",
    "        self.dropout = dropout\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = create_model(units=self.units, dropout=self.dropout)\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.model.predict(X) > 0.5).astype(int)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'units': self.units,\n",
    "            'dropout': self.dropout,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'verbose': self.verbose\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "# Create an instance of the wrapper class\n",
    "model = KerasSimpleRNNWrapper()\n",
    "\n",
    "\n",
    "# Define the hyperparameters grid to search\n",
    "#param_grid = {\n",
    "#    'units': [32, 64, 128],\n",
    "#    'dropout': [0.3, 0.5, 0.7]\n",
    "#}\n",
    "param_grid = {        #smaller faster version for testing\n",
    "    'units': [32],\n",
    "    'dropout': [0.5]\n",
    "}\n",
    "\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_count, scoring=make_scorer(accuracy_score), verbose=epoch_verbosity)\n",
    "\n",
    "# Perform grid search\n",
    "print(f\"------------------------------------------\")\n",
    "print(f\"Performing GridSearchCV\")\n",
    "grid_search_result = grid_search.fit(X_train_with_time_steps, y_train)\n",
    "\n",
    "# Print best parameters and results\n",
    "print(\"Best Parameters:\", grid_search_result.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search_result.best_score_)\n",
    "\n",
    "# final cross validation\n",
    "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "simplernn_lstm_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "simplernn_lstm_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "simplernn_lstm_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# Evaluate the model on training data\n",
    "print(f\"------------------------------------------\")\n",
    "print(f\"Evaluating the model on training data\")\n",
    "best_model = grid_search_result.best_estimator_\n",
    "train_loss, train_accuracy = best_model.model.evaluate(X_train_with_time_steps, y_train)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "# Evaluate the model on test data\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Evaluating the model on test data\")\n",
    "test_loss, test_accuracy = best_model.model.evaluate(X_test_with_time_steps, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "print(f\"-----------------------------------------\")\n",
    "print(f\"Evaluating the model on validation data\")\n",
    "val_loss, val_accuracy = best_model.model.evaluate(X_val_with_time_steps, y_val)\n",
    "print(\"Val Loss:\", val_loss)\n",
    "print(\"Val Accuracy:\", val_accuracy)\n",
    "print(f\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "test_accuracy_simplernn_lstm_optimized  = test_accuracy\n",
    "test_loss_simplernn_lstm_optimized      = test_loss\n",
    "train_accuracy_simplernn_lstm_optimized = train_accuracy\n",
    "train_loss_simplernn_lstm_optimized     = train_loss\n",
    "val_accuracy_simplernn_lstm_optimized   = val_accuracy\n",
    "val_loss_simplernn_lstm_optimized       = val_loss\n",
    "\n",
    "# Extracting accuracy and loss history from the training, will be used later for graphing convergence\n",
    "train_loss_history_optimized     = history.history['loss']\n",
    "train_accuracy_history_optimized = history.history['accuracy']\n",
    "val_loss_history_optimized       = history.history['val_loss']\n",
    "val_accuracy_history_optimized   = history.history['val_accuracy']\n",
    "\n",
    "# call previously defined function to create confusion matrix\n",
    "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# save results calculated for this model for later comparison to other models\n",
    "accuracy_simplernn_lstm_optimized      = Accuracy\n",
    "sensitivity_simplernn_lstm_optimized   = Sensitivity\n",
    "specificity_simplernn_lstm_optimized   = Specificity\n",
    "geometricmean_simplernn_lstm_optimized = GeometricMean\n",
    "precision_simplernn_lstm_optimized     = Precision\n",
    "recall_simplernn_lstm_optimized        = Recall\n",
    "f1_simplernn_lstm_optimized            = F1\n",
    "\n",
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72Bn38OAeI8j"
   },
   "outputs": [],
   "source": [
    "# Call previously defined functions to create graphs that show how Training/Validation loss and accuracy converge\n",
    "# It is important that the lines converge, if they do not, add more data, more epochs, or try different hyperparameter optimizations\n",
    "\n",
    "# print current history values, use end=\" \" to print all output on single line\n",
    "print(f\"\\n\\ntrain_loss_history_unoptimized\", end=\" \") \n",
    "for value in train_loss_history_unoptimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\ntrain_loss_history_optimized\", end=\" \") \n",
    "for value in train_loss_history_optimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\nval_loss_history_unoptimized\", end=\" \") \n",
    "for value in val_loss_history_unoptimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\nval_loss_history_optimized\", end=\" \") \n",
    "for value in val_loss_history_optimized: print(f\"{value:.4f}\", end=\" \")\n",
    "\n",
    "\n",
    "# sanity check to confirm the required values have already been collected, set to zeros if needed to avoid undef errors\n",
    "if len(train_loss_history_unoptimized) != epoch_count: print(\"ERROR: incorrect len() of train_loss_history_unoptimized, resetting to all zeros\") ; train_loss_history_unoptimized = [0] * epoch_count\n",
    "if len(train_loss_history_optimized)   != epoch_count: print(\"ERROR: incorrect len() of train_loss_history_optimized,   resetting to all zeros\") ; train_loss_history_optimized   = [0] * epoch_count\n",
    "if len(val_loss_history_unoptimized)   != epoch_count: print(\"ERROR: incorrect len() of val_loss_history_unoptimized,   resetting to all zeros\") ; val_loss_history_unoptimized   = [0] * epoch_count\n",
    "if len(val_loss_history_optimized)     != epoch_count: print(\"ERROR: incorrect len() of  val_loss_history_optimized,    resetting to all zeros\") ; val_loss_history_optimized     = [0] * epoch_count\n",
    "\n",
    "# create the convergence graphs\n",
    "plot_loss_history(train_loss_history_unoptimized, train_loss_history_optimized, val_loss_history_unoptimized, val_loss_history_optimized, epoch_count)\n",
    "plot_accuracy_history(train_accuracy_history_unoptimized, train_accuracy_history_optimized, val_accuracy_history_unoptimized, val_accuracy_history_optimized, epoch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cvXHP-I1pjZ"
   },
   "source": [
    "## Gated Recurrent Unit (GRU)\n",
    "### (needed reshaping to add time steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EJ2geQFFMb6"
   },
   "source": [
    "Note that it is possible to combine a Gated Recurrent Unit (GRU) layer with a Long Short-Term Memory (LSTM) layer in a neural network architecture. However, it's less common to do so because both GRU and LSTM layers are types of recurrent neural network (RNN) layers designed to address the vanishing gradient problem and capture long-term dependencies in sequential data.\n",
    "\n",
    "Typically, you would choose either GRU or LSTM based on the specific requirements of your task and the characteristics of your data. Both GRU and LSTM have similar capabilities, but they have slightly different architectures and computational properties.\n",
    "\n",
    "If you choose to combine GRU and LSTM layers in a neural network, you would stack them sequentially or in parallel depending on your design choices. However, it's important to consider the computational complexity and potential overfitting when stacking multiple types of RNN layers.\n",
    "\n",
    "In most cases, using either GRU or LSTM layers alone is sufficient to model sequential data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae1Bqj1UFMqW"
   },
   "source": [
    "### default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsgiH50S2wA3"
   },
   "outputs": [],
   "source": [
    "# #from tensorflow.keras.layers import Dense, GRU\n",
    "\n",
    "\n",
    "\n",
    "# # Define input shape based on the features in X_train\n",
    "# input_shape = X_train_with_time_steps.shape[1:]\n",
    "\n",
    "# # Define the model\n",
    "# model = Sequential([\n",
    "#     GRU(units=64, input_shape=input_shape),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Print model summary\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Model Summary\")\n",
    "# print(model.summary())\n",
    "\n",
    "# # Train the model\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Training the model\")\n",
    "# history = model.fit(X_train_with_time_steps, y_train, epochs=epoch_count, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Evaluate the model on training data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on training data\")\n",
    "# train_loss, train_accuracy = model.evaluate(X_train_with_time_steps, y_train)\n",
    "# print(\"Training Loss:\", train_loss)\n",
    "# print(\"Training Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the model on test data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on test data\")\n",
    "# test_loss, test_accuracy = model.evaluate(X_test_with_time_steps, y_test)\n",
    "# print(\"Test Loss:\", test_loss)\n",
    "# print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the model on validation data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on validation data\")\n",
    "# val_loss, val_accuracy = model.evaluate(X_val_with_time_steps, y_val)\n",
    "# print(\"Val Loss:\", val_loss)\n",
    "# print(\"Val Accuracy:\", val_accuracy)\n",
    "# print(f\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# test_accuracy_gru_unoptimized  = test_accuracy\n",
    "# test_loss_gru_unoptimized      = test_loss\n",
    "# train_accuracy_gru_unoptimized = train_accuracy\n",
    "# train_loss_gru_unoptimized     = train_loss\n",
    "# val_accuracy_gru_unoptimized   = val_accuracy\n",
    "# val_loss_gru_unoptimized       = val_loss\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# accuracy_gru_unoptimized      = Accuracy\n",
    "# sensitivity_gru_unoptimized   = Sensitivity\n",
    "# specificity_gru_unoptimized   = Specificity\n",
    "# geometricmean_gru_unoptimized = GeometricMean\n",
    "# precision_gru_unoptimized     = Precision\n",
    "# recall_gru_unoptimized        = Recall\n",
    "# f1_gru_unoptimized            = F1\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fL0p8Bc0_hdA"
   },
   "source": [
    "### optimized params\n",
    "Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkO4kJxJC4J5"
   },
   "outputs": [],
   "source": [
    "# Sanity check to confirm X_train and y_train have equal number of samples\n",
    "print(f\"X_train_with_time_steps has \", len(X_train_with_time_steps), \"samples\")\n",
    "print(f\"y_train                 has \", len(y_train),                 \"samples\")\n",
    "if ( len(X_train_with_time_steps) != len(y_train) ):\n",
    "  raise ValueError (\"X_train_with_time_steps and y_train are different lengths, please investigate!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxSkB7Pw_klo"
   },
   "outputs": [],
   "source": [
    "# # NOTE: Crashed on this step after using all available RAM!  2025-04-19\n",
    "\n",
    "# # GRU hyperparameter optimization\n",
    "\n",
    "# #import numpy as np\n",
    "# #from sklearn.model_selection import GridSearchCV\n",
    "# #from tensorflow.keras.models import Sequential\n",
    "# #from tensorflow.keras.layers import Dense, GRU, Dropout\n",
    "# #from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "# # Define input shape based on the features in X_train\n",
    "# input_shape = (X_train_with_time_steps.shape[1], X_train_with_time_steps.shape[2])  # Assuming X_train is 3D\n",
    "\n",
    "# # Define a function to create a model\n",
    "# def create_model(units=64, dropout=0.5):\n",
    "#     model = Sequential([\n",
    "#         GRU(units, input_shape=input_shape),\n",
    "#         Dropout(dropout),\n",
    "#         Dense(32, activation='relu'),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Create a wrapper class around the Keras model\n",
    "# class KerasGRUWrapper:\n",
    "#     def __init__(self, units=64, dropout=0.5, epochs=epoch_count, batch_size=32, verbose=0):\n",
    "#         self.units = units\n",
    "#         self.dropout = dropout\n",
    "#         self.epochs = epochs\n",
    "#         self.batch_size = batch_size\n",
    "#         self.verbose = verbose\n",
    "#         self.model = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         self.model = create_model(units=self.units, dropout=self.dropout)\n",
    "#         self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return (self.model.predict(X) > 0.5).astype(int)\n",
    "\n",
    "#     def get_params(self, deep=True):\n",
    "#         return {\n",
    "#             'units': self.units,\n",
    "#             'dropout': self.dropout,\n",
    "#             'epochs': self.epochs,\n",
    "#             'batch_size': self.batch_size,\n",
    "#             'verbose': self.verbose\n",
    "#         }\n",
    "\n",
    "#     def set_params(self, **params):\n",
    "#         for param, value in params.items():\n",
    "#             setattr(self, param, value)\n",
    "#         return self\n",
    "\n",
    "# # Create an instance of the wrapper class\n",
    "# model = KerasGRUWrapper()\n",
    "\n",
    "# # Define the hyperparameters grid to search\n",
    "# #param_grid = {\n",
    "# #    'units': [32, 64, 128],\n",
    "# #    'dropout': [0.3, 0.5, 0.7]\n",
    "# #}\n",
    "# param_grid = {          #smaller faster version for testing\n",
    "#     'units': [64],\n",
    "#     'dropout': [0.5]\n",
    "# }\n",
    "\n",
    "# # Create GridSearchCV instance\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_count, scoring=make_scorer(accuracy_score), verbose=epoch_verbosity)\n",
    "\n",
    "# # Perform grid search\n",
    "# print(f\"Performing grid search\")\n",
    "# grid_search_result = grid_search.fit(X_train_with_time_steps, y_train)\n",
    "\n",
    "# # Print best parameters and results\n",
    "# print(\"Best Parameters:\", grid_search_result.best_params_)\n",
    "# print(\"Best Accuracy:\", grid_search_result.best_score_)\n",
    "\n",
    "# # final cross validation\n",
    "# cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Cross validation scores: {cross_val_score_result}\")\n",
    "# print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
    "# print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
    "# gru_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
    "# gru_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
    "# gru_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
    "\n",
    "# # Evaluate the model on training data\n",
    "# print(f\"------------------------------------------\")\n",
    "# print(f\"Evaluating the model on training data\")\n",
    "# best_model = grid_search_result.best_estimator_\n",
    "# train_loss, train_accuracy = best_model.model.evaluate(X_train_with_time_steps, y_train)\n",
    "# print(\"Train Loss:\", train_loss)\n",
    "# print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the model on test data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on test data\")\n",
    "# test_loss, test_accuracy = best_model.model.evaluate(X_test_with_time_steps, y_test)\n",
    "# print(\"Test Loss:\", test_loss)\n",
    "# print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "# # Evaluate the model on validation data\n",
    "# print(f\"-----------------------------------------\")\n",
    "# print(f\"Evaluating the model on validation data\")\n",
    "# val_loss, val_accuracy = best_model.model.evaluate(X_val_with_time_steps, y_val)\n",
    "# print(\"Val Loss:\", val_loss)\n",
    "# print(\"Val Accuracy:\", val_accuracy)\n",
    "# print(f\"-----------------------------------------\")\n",
    "\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# test_accuracy_gru_optimized  = test_accuracy\n",
    "# test_loss_gru_optimized      = test_loss\n",
    "# train_accuracy_gru_optimized = train_accuracy\n",
    "# train_loss_gru_optimized     = train_loss\n",
    "# val_accuracy_gru_optimized   = val_accuracy\n",
    "# val_loss_gru_optimized       = val_loss\n",
    "\n",
    "# # Extracting accuracy and loss history from the training, will be used later for graphing convergence\n",
    "# train_loss_history_optimized     = history.history['loss']\n",
    "# train_accuracy_history_optimized = history.history['accuracy']\n",
    "# val_loss_history_optimized       = history.history['val_loss']\n",
    "# val_accuracy_history_optimized   = history.history['val_accuracy']\n",
    "\n",
    "\n",
    "# # call previously defined function to create confusion matrix\n",
    "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # save results calculated for this model for later comparison to other models\n",
    "# accuracy_gru_optimized      = Accuracy\n",
    "# sensitivity_gru_optimized   = Sensitivity\n",
    "# specificity_gru_optimized   = Specificity\n",
    "# geometricmean_gru_optimized = GeometricMean\n",
    "# precision_gru_optimized     = Precision\n",
    "# recall_gru_optimized        = Recall\n",
    "# f1_gru_optimized            = F1\n",
    "\n",
    "# # show a running total of elapsed time for the entire notebook\n",
    "# show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "036VVgUkeMkw"
   },
   "outputs": [],
   "source": [
    "# Call previously defined functions to create graphs that show how Training/Validation loss and accuracy converge\n",
    "# It is important that the lines converge, if they do not, add more data, more epochs, or try different hyperparameter optimizations\n",
    "\n",
    "# print current history values, use end=\" \" to print all output on single line\n",
    "print(f\"\\n\\ntrain_loss_history_unoptimized\", end=\" \") \n",
    "for value in train_loss_history_unoptimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\ntrain_loss_history_optimized\", end=\" \") \n",
    "for value in train_loss_history_optimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\nval_loss_history_unoptimized\", end=\" \") \n",
    "for value in val_loss_history_unoptimized: print(f\"{value:.4f}\", end=\" \")\n",
    "#\n",
    "print(f\"\\n\\nval_loss_history_optimized\", end=\" \") \n",
    "for value in val_loss_history_optimized: print(f\"{value:.4f}\", end=\" \")\n",
    "\n",
    "\n",
    "#print(f\"\\ntrain_loss_history_optimized\")  ; for value in train_loss_history_unoptimized: print(f\"{value:.4f}\"))\n",
    "#print(f\"\\nval_loss_history_unoptimized\")  ; for value in train_loss_history_unoptimized: print(f\"{value:.4f}\"))\n",
    "#print(f\"\\nval_loss_history_optimized\")    ; for value in train_loss_history_unoptimized: print(f\"{value:.4f}\"))\n",
    "\n",
    "# sanity check to confirm the required values have already been collected, set to zeros if needed to avoid undef errors\n",
    "if len(train_loss_history_unoptimized) != epoch_count: print(\"ERROR: incorrect len() of train_loss_history_unoptimized, resetting to all zeros\") ; train_loss_history_unoptimized = [0] * epoch_count\n",
    "if len(train_loss_history_optimized)   != epoch_count: print(\"ERROR: incorrect len() of train_loss_history_optimized,   resetting to all zeros\") ; train_loss_history_optimized   = [0] * epoch_count\n",
    "if len(val_loss_history_unoptimized)   != epoch_count: print(\"ERROR: incorrect len() of val_loss_history_unoptimized,   resetting to all zeros\") ; val_loss_history_unoptimized   = [0] * epoch_count\n",
    "if len(val_loss_history_optimized)     != epoch_count: print(\"ERROR: incorrect len() of  val_loss_history_optimized,    resetting to all zeros\") ; val_loss_history_optimized     = [0] * epoch_count\n",
    "\n",
    "# create the convergence graphs\n",
    "plot_loss_history(train_loss_history_unoptimized, train_loss_history_optimized, val_loss_history_unoptimized, val_loss_history_optimized, epoch_count)\n",
    "plot_accuracy_history(train_accuracy_history_unoptimized, train_accuracy_history_optimized, val_accuracy_history_unoptimized, val_accuracy_history_optimized, epoch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SMMheYSRsZP"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baeOv-v_xlH7"
   },
   "source": [
    "# Comparison of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e81Nf1Nt3Zhi"
   },
   "outputs": [],
   "source": [
    "# this section compares the accuracy of different methods:\n",
    "\n",
    "from tabulate import tabulate\n",
    "# Define headers for the table\n",
    "headers = [\"Model\", \"Accuracy Un-optimized\", \"Accuracy Optimized\"]\n",
    "# Define data for the trable\n",
    "data = [\n",
    "    [\"LR\",         accuracy_lr_unoptimized,             accuracy_lr_optimized],\n",
    "    [\"NB\",         accuracy_nb_unoptimized,             accuracy_nb_optimized],\n",
    "    [\"KNN\",        accuracy_knn_unoptimized,            accuracy_knn_optimized],\n",
    "    [\"SVM\",        accuracy_svm_unoptimized,            accuracy_svm_optimized],\n",
    "    [\"DT\",         accuracy_dt_unoptimized,             accuracy_dt_optimized],\n",
    "    [\"RF\",         accuracy_rf_unoptimized,             accuracy_rf_optimized],\n",
    "    [\"GB\",         accuracy_gb_unoptimized,             accuracy_gb_optimized],\n",
    "    [\"MLP\",        accuracy_mlp_unoptimized,            accuracy_mlp_optimized],\n",
    "    [\"Sequential\", accuracy_sequential_unoptimized,     accuracy_sequential_optimized],\n",
    "    [\"FNN-LSTM\",   accuracy_lstm_unoptimized,           accuracy_lstm_optimized],\n",
    "    [\"RNN\",        accuracy_simplernn_unoptimized,      accuracy_simplernn_optimized],\n",
    "    [\"RNN-LSTM\",   accuracy_simplernn_lstm_unoptimized, accuracy_simplernn_lstm_optimized],\n",
    "    [\"GRU\",        accuracy_gru_unoptimized,            accuracy_gru_optimized]\n",
    "]\n",
    "\n",
    "# Round all floats in the data list to 4 decimal places\n",
    "data_rounded = [[elem if isinstance(elem, str) else round(elem, 4) for elem in row] for row in data]\n",
    "\n",
    "# Generate the table\n",
    "table = tabulate(data_rounded, headers=headers, tablefmt=\"fancy_grid\")\n",
    "\n",
    "# Print the table\n",
    "print(table)\n",
    "\n",
    "\n",
    "# sanity checks to confirm that hyperparameter optimization did not make things worse instead of better\n",
    "if accuracy_lr_unoptimized              > accuracy_lr_optimized:              print(\"WARNING: LR optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_nb_unoptimized              > accuracy_nb_optimized:              print(\"WARNING: NB optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_knn_unoptimized             > accuracy_knn_optimized:             print(\"WARNING: KNN optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_svm_unoptimized             > accuracy_svm_optimized:             print(\"WARNING: SVM optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_dt_unoptimized              > accuracy_dt_optimized:              print(\"WARNING: DT optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_rf_unoptimized              > accuracy_rf_optimized:              print(\"WARNING: RF optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_gb_unoptimized              > accuracy_gb_optimized:              print(\"WARNING: GB optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_mlp_unoptimized             > accuracy_mlp_optimized:             print(\"WARNING: MLP optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_sequential_unoptimized      > accuracy_sequential_optimized:      print(\"WARNING: FNN Sequential optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_lstm_unoptimized            > accuracy_lstm_optimized:            print(\"WARNING: FNN-LSTM optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_simplernn_unoptimized       > accuracy_simplernn_optimized:       print(\"WARNING: RNN optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_simplernn_lstm_unoptimized  > accuracy_simplernn_lstm_optimized:  print(\"WARNING: RNN-LSTM optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if accuracy_gru_unoptimized             > accuracy_gru_optimized:             print(\"WARNING: GRU optimization made the accuracy worse, please tweak hyperparameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Va-l6ba9MRK1"
   },
   "outputs": [],
   "source": [
    "# sanity check to see if hyperparameter optimizations made accuracy worse instead of better\n",
    "\n",
    "from tabulate import tabulate\n",
    "# Define headers for the table\n",
    "headers = [\"Model\", \"Val Accuracy Un-optimized\", \"Val Accuracy Optimized\"]\n",
    "# Define data for the trable\n",
    "data = [\n",
    "    [\"MLP\",        val_accuracy_mlp_unoptimized,            val_accuracy_mlp_optimized],\n",
    "    [\"Sequential\", val_accuracy_sequential_unoptimized,     val_accuracy_sequential_optimized],\n",
    "    [\"FNN-LSTM\",   val_accuracy_lstm_unoptimized,           val_accuracy_lstm_optimized],\n",
    "    [\"RNN\",        val_accuracy_simplernn_unoptimized,      val_accuracy_simplernn_optimized],\n",
    "    [\"RNN-LSTM\",   val_accuracy_simplernn_lstm_unoptimized, val_accuracy_simplernn_lstm_optimized],\n",
    "    [\"GRU\",        val_accuracy_gru_unoptimized,            val_accuracy_gru_optimized]\n",
    "]\n",
    "\n",
    "# Round all floats in the data list to 4 decimal places\n",
    "data_rounded = [[elem if isinstance(elem, str) else round(elem, 4) for elem in row] for row in data]\n",
    "\n",
    "# Generate the table\n",
    "table = tabulate(data_rounded, headers=headers, tablefmt=\"fancy_grid\")\n",
    "\n",
    "# Print the table\n",
    "print(table)\n",
    "\n",
    "\n",
    "# sanity checks to confirm that hyperparameter optimization did not make things worse instead of better\n",
    "if val_accuracy_mlp_unoptimized             > val_accuracy_mlp_optimized:             print(\"WARNING: MLP optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if val_accuracy_sequential_unoptimized      > val_accuracy_sequential_optimized:      print(\"WARNING: FNN Sequential optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if val_accuracy_lstm_unoptimized            > val_accuracy_lstm_optimized:            print(\"WARNING: FNN-LSTM optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if val_accuracy_simplernn_unoptimized       > val_accuracy_simplernn_optimized:       print(\"WARNING: RNN optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if val_accuracy_simplernn_lstm_unoptimized  > val_accuracy_simplernn_lstm_optimized:  print(\"WARNING: RNN-LSTM optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "if val_accuracy_gru_unoptimized             > val_accuracy_gru_optimized:             print(\"WARNING: GRU optimization made the accuracy worse, please tweak hyperparameters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GoZh0LVTv__"
   },
   "outputs": [],
   "source": [
    "# Create a bar graph that shows the accuracy of the base classifiers and NN classifiers\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following accuracy values will be used for visualization:\")\n",
    "print(f\"   GB       {accuracy_gb_optimized:.4f}\")\n",
    "print(f\"   DT       {accuracy_dt_optimized:.4f}\")\n",
    "print(f\"   RF       {accuracy_rf_optimized:.4f}\")\n",
    "print(f\"   MLP      {accuracy_mlp_optimized:.4f}\")\n",
    "print(f\"   FNN      {accuracy_sequential_optimized:.4f}\")\n",
    "print(f\"   LSTM     {accuracy_lstm_optimized:.4f}\")\n",
    "\n",
    "labels = [\"GB\", \"DT\", \"RF\", \"MLP\", \"FNN\", \"FNN-LSTM\", \"RNN\", \"RNN-LSTM\", \"GRU\"]\n",
    "values = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, accuracy_mlp_optimized*100, accuracy_sequential_optimized*100, accuracy_lstm_optimized*100, accuracy_simplernn_optimized*100, accuracy_simplernn_lstm_optimized*100, accuracy_gru_optimized*100]\n",
    "#values = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, accuracy_mlp_optimized*100, test_accuracy_sequential_optimized*100, test_accuracy_lstm_optimized*100, test_accuracy_simplernn_optimized*100, test_accuracy_simplernn_lstm_optimized*100,test_accuracy_gru_optimized*100]\n",
    "\n",
    "\n",
    "# Increase the width of the graph\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust the figsize as needed\n",
    "\n",
    "# Increase spacing between bars\n",
    "bar_width = 0.6  # Adjust the width as needed\n",
    "bar_positions = range(len(labels))\n",
    "\n",
    "# Create a bar graph\n",
    "#bars = plt.bar(bar_positions, values, width=bar_width, color='blue')\n",
    "bars = plt.bar(bar_positions, values, width=bar_width, color=['lightgreen']*3 + ['darkgreen']*6)  # Last 6 bars are darkgreen\n",
    "\n",
    "# Dynamically set y-axis limits\n",
    "plt.ylim(min(values*100) - 5, max(values) + 5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    plt.title('Model Accuracies for Edge-IIoTset2023 dataset (note identical values for NN models)')\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": plt.title('Model Accuracies for CIC_IOT_Dataset2023 (note identical values for NN models)')\n",
    "\n",
    "# Set x-axis ticks and labels\n",
    "plt.xticks(bar_positions, labels)\n",
    "\n",
    "# Annotate each bar with its respective value\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{value:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "# Save the figure with 600dpi resolution to allow a high-quality image to be imported to a manuscript\n",
    "plt.savefig('model_accuracies.png', dpi=600)\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    plt.savefig('Edge-IIoTset2023_model_accuracies.png'   , dpi=600)\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": plt.savefig('CIC_IOT_Dataset2023_model_accuracies.png', dpi=600)\n",
    "\n",
    "# Display the bar graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phPUgmeIGAjN"
   },
   "outputs": [],
   "source": [
    "# Create a bar graph that shows the accuracy of the classifiers\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following accuracy values will be used for visualization:\")\n",
    "print(f\"   GB       {accuracy_gb_optimized:.4f}\")\n",
    "print(f\"   DT       {accuracy_dt_optimized:.4f}\")\n",
    "print(f\"   RF       {accuracy_rf_optimized:.4f}\")\n",
    "print(f\"   MLP      {accuracy_mlp_optimized:.4f}\")\n",
    "print(f\"   FNN      {accuracy_sequential_optimized:.4f}\")\n",
    "print(f\"   LSTM     {accuracy_lstm_optimized:.4f}\")\n",
    "\n",
    "labels = [\"GB\", \"DT\", \"RF\", \"MLP\", \"FNN\", \"FNN-LSTM\", \"RNN\", \"RNN-LSTM\", \"GRU\"]\n",
    "#values = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, accuracy_mlp_optimized*100, accuracy_sequential_optimized*100, accuracy_lstm_optimized*100, accuracy_simplernn_optimized*100, accuracy_simplernn_lstm_optimized*100, accuracy_gru_optimized*100]\n",
    "values = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, test_accuracy_mlp_optimized*100, test_accuracy_sequential_optimized*100, test_accuracy_lstm_optimized*100, test_accuracy_simplernn_optimized*100, test_accuracy_simplernn_lstm_optimized*100,test_accuracy_gru_optimized*100]\n",
    "\n",
    "\n",
    "# Increase the width of the graph\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust the figsize as needed\n",
    "\n",
    "# Increase spacing between bars\n",
    "bar_width = 0.6  # Adjust the width as needed\n",
    "bar_positions = range(len(labels))\n",
    "\n",
    "# Create a bar graph\n",
    "#bars = plt.bar(bar_positions, values, width=bar_width, color='blue')\n",
    "bars = plt.bar(bar_positions, values, width=bar_width, color=['lightgreen']*3 + ['darkgreen']*6)  # Last 6 bars are darkgreen\n",
    "\n",
    "# Dynamically set y-axis limits\n",
    "plt.ylim(min(values*100) - 5, max(values) + 5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    plt.title('Model Accuracies for Edge-IIoTset2023 dataset (using test_accuracy)')\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": plt.title('Model Accuracies for CIC_IOT_Dataset2023 (using test_accuracy)')\n",
    "\n",
    "# Set x-axis ticks and labels\n",
    "plt.xticks(bar_positions, labels)\n",
    "\n",
    "# Annotate each bar with its respective value\n",
    "# Adjust bar.get_height() + ?? to adjust spacing between bar and label above bar\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f'{value:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "# Save the figure with 600dpi resolution to allow a high-quality image to be imported to a manuscript\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    plt.savefig('Edge-IIoTset2023_model_accuracies.png'   , dpi=600)\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": plt.savefig('CIC_IOT_Dataset2023_model_accuracies.png', dpi=600)\n",
    "\n",
    "# Display the bar graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e5PyMG1CDZv"
   },
   "outputs": [],
   "source": [
    "# Create a bar graph that shows the accuracy of the base classifiers and NN classifiers\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following accuracy values will be used for visualization:\")\n",
    "print(f\"   GB       {accuracy_gb_optimized:.4f}\")\n",
    "print(f\"   DT       {accuracy_dt_optimized:.4f}\")\n",
    "print(f\"   RF       {accuracy_rf_optimized:.4f}\")\n",
    "print(f\"   MLP      {accuracy_mlp_optimized:.4f}\")\n",
    "print(f\"   FNN      {accuracy_sequential_optimized:.4f}\")\n",
    "print(f\"   LSTM     {accuracy_lstm_optimized:.4f}\")\n",
    "\n",
    "labels = [\"GB\", \"DT\", \"RF\", \"MLP\", \"FNN\", \"FNN-LSTM\", \"RNN\", \"RNN-LSTM\", \"GRU\"]\n",
    "#values = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, accuracy_mlp_optimized*100, accuracy_sequential_optimized*100, accuracy_lstm_optimized*100, accuracy_simplernn_optimized*100, accuracy_simplernn_lstm_optimized*100, accuracy_gru_optimized*100]\n",
    "values = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, val_accuracy_mlp_optimized*100, val_accuracy_sequential_optimized*100, val_accuracy_lstm_optimized*100, val_accuracy_simplernn_optimized*100, val_accuracy_simplernn_lstm_optimized*100, val_accuracy_gru_optimized*100]\n",
    "\n",
    "\n",
    "# Increase the width of the graph\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust the figsize as needed\n",
    "\n",
    "# Increase spacing between bars\n",
    "bar_width = 0.6  # Adjust the width as needed\n",
    "bar_positions = range(len(labels))\n",
    "\n",
    "# Create a bar graph\n",
    "#bars = plt.bar(bar_positions, values, width=bar_width, color='blue')\n",
    "bars = plt.bar(bar_positions, values, width=bar_width, color=['lightgreen']*3 + ['darkgreen']*6)  # Last 6 bars are darkgreen\n",
    "\n",
    "# Dynamically set y-axis limits\n",
    "plt.ylim(min(values*100) - 5, max(values) + 5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Model Accuracies for Edge-IIoTset2023 dataset (using val_accuracy)')\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    plt.title('Model Accuracies for Edge-IIoTset2023 dataset')\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": plt.title('Model Accuracies for CIC_IOT_Dataset2023')\n",
    "\n",
    "# Set x-axis ticks and labels\n",
    "plt.xticks(bar_positions, labels)\n",
    "\n",
    "# Annotate each bar with its respective value\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{value:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "# Save the figure with 600dpi resolution to allow a high-quality image to be imported to a manuscript\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    plt.savefig('Edge-IIoTset2023_model_accuracies.png'   , dpi=600)\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": plt.savefig('CIC_IOT_Dataset2023_model_accuracies.png', dpi=600)\n",
    "\n",
    "# Display the bar graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkvqToOlsQ4Y"
   },
   "outputs": [],
   "source": [
    "# Create a bar graph that shows the accuracy of the base classifiers and NN classifiers\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following accuracy values will be used for visualization:\")\n",
    "print(f\"   GB       {accuracy_gb_optimized:.4f}\")\n",
    "print(f\"   DT       {accuracy_dt_optimized:.4f}\")\n",
    "print(f\"   RF       {accuracy_rf_optimized:.4f}\")\n",
    "print(f\"   MLP      {accuracy_mlp_optimized:.4f}\")\n",
    "print(f\"   FNN      {accuracy_sequential_optimized:.4f}\")\n",
    "print(f\"   LSTM     {accuracy_lstm_optimized:.4f}\")\n",
    "\n",
    "labels = [\"GB\", \"DT\", \"RF\", \"MLP\", \"RNN\", \"RNN-LSTM\"]\n",
    "#values = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, accuracy_mlp_optimized*100, accuracy_sequential_optimized*100, accuracy_lstm_optimized*100, accuracy_simplernn_optimized*100, accuracy_simplernn_lstm_optimized*100, accuracy_gru_optimized*100]\n",
    "values = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, val_accuracy_mlp_optimized*100, val_accuracy_simplernn_optimized*100, val_accuracy_simplernn_lstm_optimized*100]\n",
    "\n",
    "\n",
    "# Increase the width of the graph\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust the figsize as needed\n",
    "\n",
    "# Increase spacing between bars\n",
    "bar_width = 0.6  # Adjust the width as needed\n",
    "bar_positions = range(len(labels))\n",
    "\n",
    "# Create a bar graph\n",
    "#bars = plt.bar(bar_positions, values, width=bar_width, color='blue')\n",
    "bars = plt.bar(bar_positions, values, width=bar_width, color=['lightgreen']*3 + ['darkgreen']*3)  # Last 3 bars are darkgreen\n",
    "\n",
    "# Dynamically set y-axis limits\n",
    "plt.ylim(min(values*100) - 5, max(values) + 5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    plt.title('Model Accuracies for Edge-IIoTset2023 dataset')\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": plt.title('Model Accuracies for CIC_IOT_Dataset2023 dataset')\n",
    "\n",
    "# Set x-axis ticks and labels\n",
    "plt.xticks(bar_positions, labels)\n",
    "\n",
    "# Annotate each bar with its respective value\n",
    "# Adjust bar.get_height() + ?? to adjust spacing between bar and label above bar\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f'{value:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "# Save the figure with 600dpi resolution to allow a high-quality image to be imported to a manuscript\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    plt.savefig('Edge-IIoTset2023_model_accuracies.png'   , dpi=600)\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": plt.savefig('CIC_IOT_Dataset2023_model_accuracies.png', dpi=600)\n",
    "\n",
    "# Display the bar graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJuOtqzAsQ4Y"
   },
   "outputs": [],
   "source": [
    "# create a graph that shows model accuracy vs runtime\n",
    "# this gives us an idea of the tradeoff between speed and accuracy\n",
    "\n",
    "# dummy values for runtime to be fixed up\n",
    "print(f\"\\n\\n\\n ---------------\\nIMPORTANT NOTE:\")\n",
    "print(f\"The runtimes need to be manually entered!\")\n",
    "print(f\"Please look at the elapsed time for each section of this notebook and update the runtime variables for graphing!\")\n",
    "print(f\"---------------\")\n",
    "gb_runtime = 9\n",
    "dt_runtime = 3\n",
    "rf_runtime = 5\n",
    "mlp_runtime = 10\n",
    "simplernn_runtime = 35\n",
    "simplernn_lstm_runtime = 55\n",
    "\n",
    "labels = [\"GB\", \"DT\", \"RF\", \"MLP\", \"RNN\", \"RNN-LSTM\"]\n",
    "values_bar  = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, val_accuracy_mlp_optimized*100, val_accuracy_simplernn_optimized*100, val_accuracy_simplernn_lstm_optimized*100]\n",
    "values_line = [gb_runtime, dt_runtime, rf_runtime, mlp_runtime, simplernn_runtime, simplernn_lstm_runtime]\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))  # Adjust the figsize as needed\n",
    "\n",
    "# Increase spacing between bars\n",
    "bar_width = 0.6  # Adjust the width as needed\n",
    "bar_positions = range(len(labels))\n",
    "\n",
    "# Create a bar chart\n",
    "# Adjust the width and align parameters for bar width and spacing\n",
    "ax1.bar(labels, values_bar, label='Accuracy', width=bar_width, align='center', color=['lightgreen']*3 + ['darkgreen']*3)  # Last 3 bars are darkgreen\n",
    "\n",
    "# Dynamically set y-axis limits\n",
    "ax1.set_ylim(min(values_bar) - 2, max(values_bar) + 2)\n",
    "\n",
    "# Set label and ticks for the left vertical axis\n",
    "ax1.set_ylabel('Accuracy (%)', color='g')\n",
    "ax1.tick_params('y', colors='g')\n",
    "\n",
    "# Dynamically set y-axis limits\n",
    "ax1.set_ylim(min(values_bar) - 3, max(values_bar) + 3)\n",
    "\n",
    "# Annotate each bar with its respective value\n",
    "# Adjust bar.get_height() + ?? to adjust spacing between bar and label above bar\n",
    "for bar, value in zip(bars, values_bar):\n",
    "    ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.2, f'{value:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "\n",
    "# Line graph\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(labels, values_line, color='r', marker='o', label='Runtime')\n",
    "\n",
    "# Set label and ticks for the right vertical axis\n",
    "ax2.set_ylabel('Minutes runtime', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "# Dynamically set y-axis limits\n",
    "ax2.set_ylim(min(values_line) - 3, max(values_line) + 3)\n",
    "\n",
    "# Title\n",
    "plt.title('Accuracy vs Runtime')\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    plt.title('Accuracy vs Runtime for Edge-IIoTset2023')\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": plt.title('Accuracy vs Runtime for CIC_IOT_Dataset2023')\n",
    "\n",
    "# Show legend\n",
    "fig.legend(loc='upper right')\n",
    "\n",
    "# Save the figure with 600dpi resolution to allow a high-quality image to be imported to a manuscript\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    plt.savefig('Edge-IIoTset2023_model_accuracies.png'   , dpi=600)\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": plt.savefig('CIC_IOT_Dataset2023_model_accuracies.png', dpi=600)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnhdwatIsQ4Y"
   },
   "outputs": [],
   "source": [
    "# NOTE: these box plots use the cross validation scores,\n",
    "# but perhaps the MLP/RNN/RNN-LSTM models should use their validation accuracy scores instead?\n",
    "# Make some tweaks in the next cell\n",
    "\n",
    "# This code generates a box plot with the given mean and standard deviation values for each classifier.\n",
    "# The showmeans=True option ensures that the means are displayed as red dots, and sym='' removes the outliers for a cleaner representation.\n",
    "# Adjust the code as needed for your specific requirements.\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following Mean and Standard Deviation values will be used for visualization:\")\n",
    "print(f\"   GB       {gb_crossval_score_mean:.4f}               {gb_crossval_score_std:.4f}\")\n",
    "print(f\"   DT       {dt_crossval_score_mean:.4f}               {dt_crossval_score_std:.4f}\")\n",
    "print(f\"   RF       {rf_crossval_score_mean:.4f}               {rf_crossval_score_std:.4f}\")\n",
    "print(f\"   MLP      {mlp_crossval_score_mean:.4f}              {mlp_crossval_score_std:.4f}\")\n",
    "print(f\"   RNN      {simplernn_crossval_score_mean:.4f}        {simplernn_crossval_score_std:.4f}\")\n",
    "print(f\"   RNN-LSTM {simplernn_lstm_crossval_score_mean:.4f}   {simplernn_lstm_crossval_score_std:.4f}\")\n",
    "\n",
    "# Prepare the data\n",
    "labels   = [\"GB\", \"DT\", \"RF\", \"MLP\", \"RNN\", \"RNN-LSTM\"]\n",
    "means    = [gb_crossval_score_mean*100, dt_crossval_score_mean*100, rf_crossval_score_mean*100, mlp_crossval_score_mean*100, simplernn_crossval_score_mean*100, simplernn_lstm_crossval_score_mean*100]\n",
    "std_devs = [gb_crossval_score_std*100,  dt_crossval_score_std*100,  rf_crossval_score_std*100,  mlp_crossval_score_std*100,  simplernn_crossval_score_std*100,  simplernn_lstm_crossval_score_std*100]\n",
    "\n",
    "# Create the box plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the box plots\n",
    "box_data = [np.random.normal(mean, std, 100) for mean, std in zip(means, std_devs)]\n",
    "ax.boxplot(box_data, labels=labels, showmeans=True, meanline=True, sym='')\n",
    "\n",
    "# Rotate the x-axis labels by 45 degrees to make them fit\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Classifiers')\n",
    "ax.set_ylabel('Mean Accuracy % with Std-Dev')\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    ax.set_title('Classifier Performance for Edge-IIoT2023')\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": ax.set_title('Classifier Performance for CIC_IOT_Dataset2023')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Save the figure with 600dpi resolution to allow a high-quality image to be imported to a manuscript\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    plt.savefig('Edge-IIoTset2023_model_performance_boxplots.png', dpi=600)\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": plt.savefig('CIC_IOT_Dataset2023_model_performance_boxplots.png', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6m1zvM1sQ4Z"
   },
   "outputs": [],
   "source": [
    "# NOTE: these box plots use the cross validation scores,\n",
    "# but perhaps the MLP/RNN/RNN-LSTM models should use their validation accuracy scores instead?\n",
    "# This cell tweaks the values from the previous cell, see the commented out lines below\n",
    "\n",
    "# This code generates a box plot with the given mean and standard deviation values for each classifier.\n",
    "# The showmeans=True option ensures that the means are displayed as red dots, and sym='' removes the outliers for a cleaner representation.\n",
    "# Adjust the code as needed for your specific requirements.\n",
    "\n",
    "# Show the values that will be used in the graph\n",
    "print(f\"The following Mean and Standard Deviation values will be used for visualization:\")\n",
    "print(f\"   GB       {gb_crossval_score_mean:.4f}               {gb_crossval_score_std:.4f}\")\n",
    "print(f\"   DT       {dt_crossval_score_mean:.4f}               {dt_crossval_score_std:.4f}\")\n",
    "print(f\"   RF       {rf_crossval_score_mean:.4f}               {rf_crossval_score_std:.4f}\")\n",
    "#print(f\"   MLP      {mlp_crossval_score_mean:.4f}              {mlp_crossval_score_std:.4f}\")\n",
    "#print(f\"   RNN      {simplernn_crossval_score_mean:.4f}        {simplernn_crossval_score_std:.4f}\")\n",
    "#print(f\"   RNN-LSTM {simplernn_lstm_crossval_score_mean:.4f}   {simplernn_lstm_crossval_score_std:.4f}\")\n",
    "print(f\"   MLP      {val_accuracy_mlp_optimized:.4f}            {mlp_crossval_score_std:.4f}\")\n",
    "print(f\"   RNN      {val_accuracy_simplernn_optimized:.4f}      {simplernn_crossval_score_std:.4f}\")\n",
    "print(f\"   RNN-LSTM {val_accuracy_simplernn_lstm_optimized:.4f} {simplernn_lstm_crossval_score_std:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "labels   = [\"GB\", \"DT\", \"RF\", \"MLP\", \"RNN\", \"RNN-LSTM\"]\n",
    "#means    = [gb_crossval_score_mean*100, dt_crossval_score_mean*100, rf_crossval_score_mean*100, mlp_crossval_score_mean*100, simplernn_crossval_score_mean*100, simplernn_lstm_crossval_score_mean*100]\n",
    "means    = [gb_crossval_score_mean*100, dt_crossval_score_mean*100, rf_crossval_score_mean*100, val_accuracy_mlp_optimized*100, val_accuracy_simplernn_optimized*100, val_accuracy_simplernn_lstm_optimized*100]\n",
    "std_devs = [gb_crossval_score_std*100,  dt_crossval_score_std*100,  rf_crossval_score_std*100,  mlp_crossval_score_std*100,  simplernn_crossval_score_std*100,  simplernn_lstm_crossval_score_std*100]\n",
    "\n",
    "# Create the box plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the box plots\n",
    "box_data = [np.random.normal(mean, std, 100) for mean, std in zip(means, std_devs)]\n",
    "ax.boxplot(box_data, labels=labels, showmeans=True, meanline=True, sym='')\n",
    "\n",
    "# Rotate the x-axis labels by 45 degrees to make them fit\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Classifiers')\n",
    "ax.set_ylabel('Mean Accuracy % with Std-Dev')\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    ax.set_title('Classifier Performance for Edge-IIoT2023')\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": ax.set_title('Classifier Performance for CIC_IOT_Dataset2023')\n",
    "\n",
    "# Save the figure with 600dpi resolution to allow a high-quality image to be imported to a manuscript\n",
    "if   dataset_name == \"Edge-IIoTset2023\":    plt.savefig('Edge-IIoTset2023_model_performance_boxplots.png', dpi=600)\n",
    "elif dataset_name == \"CIC_IOT_Dataset2023\": plt.savefig('CIC_IOT_Dataset2023_model_performance_boxplots.png', dpi=600)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_N07vvtFsQ4Z"
   },
   "outputs": [],
   "source": [
    "# create table showing model performance\n",
    "\n",
    "from tabulate import tabulate\n",
    "# Define headers for the table\n",
    "headers = [\"Metric\", \"GB\", \"DT\", \"RF\", \"MLP\", \"RNN\", \"RNN-LSTM\"]\n",
    "# Define data for the trable\n",
    "data = [\n",
    "    [\"Accuracy\",    accuracy_gb_optimized,      accuracy_dt_optimized,      accuracy_rf_optimized,      accuracy_mlp_optimized,      accuracy_simplernn_optimized,      accuracy_simplernn_lstm_optimized],\n",
    "    [\"Sensitivity\", sensitivity_gb_optimized,   sensitivity_dt_optimized,   sensitivity_rf_optimized,   sensitivity_mlp_optimized,   sensitivity_simplernn_optimized,   sensitivity_simplernn_lstm_optimized],\n",
    "    [\"Specificity\", specificity_gb_optimized,   specificity_dt_optimized,   specificity_rf_optimized,   specificity_mlp_optimized,   specificity_simplernn_optimized,   specificity_simplernn_lstm_optimized],\n",
    "    [\"GeoMean\",     geometricmean_gb_optimized, geometricmean_dt_optimized, geometricmean_rf_optimized, geometricmean_mlp_optimized, geometricmean_simplernn_optimized, geometricmean_simplernn_lstm_optimized],\n",
    "    [\"Precision\",   precision_gb_optimized,     precision_dt_optimized,     precision_rf_optimized,     precision_mlp_optimized,     precision_simplernn_optimized,     precision_simplernn_lstm_optimized],\n",
    "    [\"Recall\",      recall_gb_optimized,        recall_dt_optimized,        recall_rf_optimized,        recall_mlp_optimized,        recall_simplernn_optimized,        recall_simplernn_lstm_optimized],\n",
    "    [\"F1-score\",    f1_gb_optimized,            f1_dt_optimized,            f1_rf_optimized,            f1_mlp_optimized,            f1_simplernn_optimized,            f1_simplernn_lstm_optimized]\n",
    "]\n",
    "\n",
    "# Round all floats in the data list to 4 decimal places\n",
    "data_rounded = [[elem if isinstance(elem, str) else round(elem, 4) for elem in row] for row in data]\n",
    "\n",
    "# Generate the table\n",
    "table = tabulate(data_rounded, headers=headers, tablefmt=\"fancy_grid\")\n",
    "\n",
    "# Print the table\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIALcGNpsQ4Z"
   },
   "outputs": [],
   "source": [
    "# create table showing cross-fold validation scores\n",
    "\n",
    "from tabulate import tabulate\n",
    "# Define headers for the table\n",
    "headers = [\"Fold\", \"GB\", \"DT\", \"RF\", \"MLP\", \"RNN\", \"RNN-LSTM\"]\n",
    "# Define data for the trable\n",
    "data = [\n",
    "    [\"1\",      gb_crossval_score_all[0], dt_crossval_score_all[0], rf_crossval_score_all[0], mlp_crossval_score_all[0], simplernn_crossval_score_all[0], simplernn_lstm_crossval_score_all[0] ],\n",
    "    [\"2\",      gb_crossval_score_all[1], dt_crossval_score_all[1], rf_crossval_score_all[1], mlp_crossval_score_all[1], simplernn_crossval_score_all[1], simplernn_lstm_crossval_score_all[1] ],\n",
    "    [\"3\",      gb_crossval_score_all[2], dt_crossval_score_all[2], rf_crossval_score_all[2], mlp_crossval_score_all[2], simplernn_crossval_score_all[2], simplernn_lstm_crossval_score_all[2] ],\n",
    "    [\"4\",      gb_crossval_score_all[3], dt_crossval_score_all[3], rf_crossval_score_all[3], mlp_crossval_score_all[3], simplernn_crossval_score_all[3], simplernn_lstm_crossval_score_all[3] ],\n",
    "    [\"5\",      gb_crossval_score_all[4], dt_crossval_score_all[4], rf_crossval_score_all[4], mlp_crossval_score_all[4], simplernn_crossval_score_all[4], simplernn_lstm_crossval_score_all[4] ],\n",
    "    [\"6\",      gb_crossval_score_all[5], dt_crossval_score_all[5], rf_crossval_score_all[5], mlp_crossval_score_all[5], simplernn_crossval_score_all[5], simplernn_lstm_crossval_score_all[5] ],\n",
    "    [\"7\",      gb_crossval_score_all[6], dt_crossval_score_all[6], rf_crossval_score_all[6], mlp_crossval_score_all[6], simplernn_crossval_score_all[6], simplernn_lstm_crossval_score_all[6] ],\n",
    "    [\"8\",      gb_crossval_score_all[7], dt_crossval_score_all[7], rf_crossval_score_all[7], mlp_crossval_score_all[7], simplernn_crossval_score_all[7], simplernn_lstm_crossval_score_all[7] ],\n",
    "    [\"9\",      gb_crossval_score_all[8], dt_crossval_score_all[8], rf_crossval_score_all[8], mlp_crossval_score_all[8], simplernn_crossval_score_all[8], simplernn_lstm_crossval_score_all[8] ],\n",
    "    [\"10\",     gb_crossval_score_all[9], dt_crossval_score_all[9], rf_crossval_score_all[9], mlp_crossval_score_all[9], simplernn_crossval_score_all[9], simplernn_lstm_crossval_score_all[9] ],\n",
    "    [\"Mean\",   gb_crossval_score_mean,   dt_crossval_score_mean,   rf_crossval_score_mean,   mlp_crossval_score_mean,   simplernn_crossval_score_mean,   simplernn_lstm_crossval_score_mean   ],\n",
    "    [\"StdDev\", gb_crossval_score_std,    dt_crossval_score_std,    rf_crossval_score_std,    mlp_crossval_score_std,    simplernn_crossval_score_std,    simplernn_lstm_crossval_score_std    ]\n",
    "]\n",
    "\n",
    "# Round all floats in the data list to 4 decimal places\n",
    "data_rounded = [[elem if isinstance(elem, str) else round(elem, 4) for elem in row] for row in data]\n",
    "\n",
    "# Generate the table\n",
    "table = tabulate(data_rounded, headers=headers, tablefmt=\"fancy_grid\")\n",
    "\n",
    "# Print the table\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lbu7aAbKVBb"
   },
   "outputs": [],
   "source": [
    "# show a running total of elapsed time for the entire notebook\n",
    "show_elapsed_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxO8qdB_sQ4Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
