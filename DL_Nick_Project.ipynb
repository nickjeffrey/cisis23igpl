{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk48yvUD_Njm"
      },
      "source": [
        "The most recent version of this notebook is at https://github.com/nickjeffrey/cisis23igpl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsQvmAoVOx7Z"
      },
      "source": [
        "# Comments and questions for discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbs_9E80O0_H"
      },
      "source": [
        "\n",
        "- What are \"test loss\" and \"test accuracy\" for the DL models, and how do they compare (if at all) to TP, TN, FP, FN in traditional classifiers?\n",
        "- What metric should be used to compare DL models to traditional classification models?  We don't really have TP,TN,FP,FN on DL models.\n",
        "- DL models typically need larger datasets than traditional classifiers like SVM, KNN, MLP, RF, etc.  Since we are only using 1% of the original dataset (which was fine for traditional classifiers), is the accuracy suffering in the DL models because we do not have enough of the original dataset?\n",
        "[link text](https:// [link text](https://))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIToXy04WPNQ"
      },
      "source": [
        "Try adding the following to Sequential\n",
        "\n",
        "ReLU (Rectified Linear Unit):\n",
        "tf.nn.relu(x)\n",
        "Sigmoid:\n",
        "tf.nn.sigmoid(x)\n",
        "Tanh (Hyperbolic Tangent):\n",
        "tf.nn.tanh(x)\n",
        "Softmax:\n",
        "tf.nn.softmax(x)\n",
        "Softplus:\n",
        "tf.nn.softplus(x)\n",
        "Softsign:\n",
        "tf.nn.softsign(x)\n",
        "ELU (Exponential Linear Unit):\n",
        "tf.nn.elu(x)\n",
        "SELU (Scaled Exponential Linear Unit):\n",
        "tf.nn.selu(x)\n",
        "Swish:\n",
        "tf.nn.swish(x)\n",
        "ReLU6 (ReLU with upper limit of 6):\n",
        "tf.nn.relu6(x)\n",
        "Leaky ReLU:\n",
        "tf.nn.leaky_relu(x)\n",
        "PReLU (Parametric ReLU):\n",
        "tf.keras.layers.PReLU(alpha_initializer='zeros')\n",
        "Thresholded ReLU:\n",
        "tf.keras.layers.ThresholdedReLU(theta=1.0)\n",
        "\n",
        "\n",
        "Optimization algorithms: adam,\n",
        "\n",
        "Try changing activation functions and optimization algorithms in the hyperparameter optimizations\n",
        "\n",
        "\n",
        "MLP\n",
        "FNN feed Forward Neural Network, fully connected neural network (includes sequential but not LSTM)\n",
        "\n",
        "Just focus on MLP and Sequential\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with FNN:\n",
        "1. change activation functions -> calculate metrics for each\n",
        "2. change optimization algorithms in NN\n",
        "3. Regularization Techniques\n",
        "4. Learning Rate\n",
        "5. Number of hidden layers\n",
        "6. Number of neurons in each layer\n",
        "7. Batch Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_uA52cK33Yu3"
      },
      "outputs": [],
      "source": [
        "## WARNING - breaking change with pandas 3.0 for copy on write\n",
        "\n",
        "# https://towardsdatascience.com/deep-dive-into-pandas-copy-on-write-mode-part-iii-c024eaa16ed4\n",
        "\n",
        "# The change described in the above URL is causing this error message to appear:\n",
        "# ValueError: cannot set WRITEABLE flag to True of this array\n",
        "\n",
        "# The error only appears if we run: !pip install scikeras\n",
        "# We run the above command to get the KerasClassifier package so we can do hyperparameter optimization for\n",
        "# Sequential,SimpleRNN,GRU models, but a side-effect is that pandas also gets upgraded to 3.0,\n",
        "# which introduces a breaking change as described in the URL above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1asjRwh3UgM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srj8PAUPQMyi"
      },
      "source": [
        "\n",
        "\n",
        "Table showing accuracy with 10 epochs, notebook runtime 4 minutes\n",
        "\n",
        "10 epochs  | Training Accuracy | Training Loss | Test Accuracy | Test Loss\n",
        "-----------|-------------------|---------------|---------------|-----------\n",
        "Sequential |0.8427             | 0.4011        |0.8952         |0.2912     \n",
        "SimpleRNN  |0.8176             | 0.3603        |0.5796         |0.7341     \n",
        "GRU        |0.8207             | 0.3666        |0.7491         |0.6363     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QH4ItxzR0-p"
      },
      "source": [
        "\n",
        "\n",
        "Table showing accuracy with 100 epochs, notebook runtime 11 minutes\n",
        "\n",
        "100 epochs | Training Accuracy | Training Loss | Test Accuracy | Test Loss\n",
        "-----------|-------------------|---------------|---------------|-----------\n",
        "Sequential |0.8452             | 0.2849        |0.9092         |0.2935     \n",
        "SimpleRNN  |0.7995             | 1.3725        |0.6409         |1.5846     \n",
        "GRU        |0.7927             | 1.5171        |0.4384         |3.6082\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuA1Zf3RTJxL"
      },
      "source": [
        "## Definitions:\n",
        "\n",
        "In the context of neural network models, the test loss and test accuracy are performance metrics used to evaluate the model's performance on unseen data, specifically the test set.\n",
        "\n",
        "Test Loss:\n",
        "\n",
        "- The test loss measures how well the model is performing on the test set. It represents the average loss (e.g., cross-entropy loss) incurred by the model when making predictions on the test data.\n",
        "- Lower test loss indicates better performance, as it means that the model's predictions are closer to the actual labels.\n",
        "However, it's important to consider the scale and nature of the loss function used. For instance, a test loss of 0.1 might be good for one problem but poor for another, depending on the context.\n",
        "\n",
        "\n",
        "Test Accuracy:\n",
        "\n",
        "- The test accuracy measures the proportion of correctly classified samples in the test set.\n",
        "It is calculated by dividing the number of correctly classified samples by the total number of samples in the test set.\n",
        "- Higher test accuracy indicates better performance, as it means that the model is making more correct predictions.\n",
        "However, accuracy alone might not provide a complete picture, especially if the classes are imbalanced or if different types of errors have different costs.\n",
        "\n",
        "\n",
        "In summary, test loss and test accuracy are two important metrics used to assess the performance of a SimpleRNN model on unseen data. Lower test loss and higher test accuracy generally indicate better performance, but it's essential to consider other factors such as the nature of the problem, class imbalance, and potential costs associated with different types of errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAUa5zNamPFO"
      },
      "source": [
        "# Description of Experiment\n",
        "\n",
        "This jupyter notebook builds on previous works at https://github.com/nickjeffrey/ensemble_learning\n",
        "\n",
        "This notebook explores the use of Deep Learning classifiers, which are then fed to an Ensemble Learning model to see if the accuracy can be improved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WgmmRvd78t_"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S5pKfxMf7-VV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing   import LabelEncoder\n",
        "from collections import Counter\n",
        "\n",
        "# Miscellaneous packages\n",
        "import time                                           #for calculating elapsed time for training tasks\n",
        "import os                                             #for checking if file exists\n",
        "import socket                                         #for getting FQDN of local machine\n",
        "import math                                           #square root function\n",
        "import sys\n",
        "\n",
        "\n",
        "# Packages from scikit-learn\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV       #for hyperparameter optimization\n",
        "from sklearn.model_selection import cross_val_score    #for cross fold validation\n",
        "from sklearn.metrics         import make_scorer        #used by GridSearchCV\n",
        "from sklearn.metrics         import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing   import StandardScaler\n",
        "from sklearn.linear_model    import LogisticRegression\n",
        "from sklearn.naive_bayes     import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.svm             import SVC\n",
        "from sklearn.neighbors       import KNeighborsClassifier\n",
        "from sklearn.tree            import DecisionTreeClassifier\n",
        "from sklearn.ensemble        import RandomForestClassifier\n",
        "from sklearn.neural_network  import MLPClassifier      #neural network classifier\n",
        "from sklearn.ensemble        import BaggingClassifier, VotingClassifier, StackingClassifier, AdaBoostClassifier, GradientBoostingClassifier   #Packages for Ensemble Learning\n",
        "\n",
        "# packages for balancing classes\n",
        "from imblearn.under_sampling import RandomUnderSampler  #may need to install with: conda install -c conda-forge imbalanced-learn\n",
        "from imblearn.over_sampling  import SMOTE               #may need to install with: conda install -c conda-forge imbalanced-learn\n",
        "\n",
        "# Deep Learning classifiers\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models     import Sequential\n",
        "from tensorflow.keras.layers     import Dense, Dropout,LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses     import binary_crossentropy\n",
        "from tensorflow.keras.metrics    import Accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "of8Ey5Z34mhL"
      },
      "outputs": [],
      "source": [
        "# # KerasClassifier was moved to scikeras in version 2.13.0, so you will need to install the package, but this will break other things!\n",
        "\n",
        "# import importlib.util\n",
        "\n",
        "# # Check if scikeras is installed\n",
        "# if importlib.util.find_spec(\"scikeras\") is None:\n",
        "#   print(\"scikeras is not installed, attempting installation now.\")\n",
        "#   !pip install scikeras\n",
        "# else:\n",
        "#   print(\"scikeras is already installed.\")\n",
        "\n",
        "\n",
        "# # after confirming the scikeras package was installed, you can now import KerasClassifier,\n",
        "# #  which is used for SimpleRNN hyperparameter optimization\n",
        "# from scikeras.wrappers import KerasClassifier, KerasRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WNNJ8GQLL0oj"
      },
      "outputs": [],
      "source": [
        "# WARNING: do not use tensorflow.keras.wrappers.scikit_learn\n",
        "# DEPRECATED. Use [Sci-Keras](https://github.com/adriangb/scikeras) instead.\n",
        "# See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
        "\n",
        "\n",
        "# import pkg_resources\n",
        "\n",
        "# # Get a list of installed packages and their versions\n",
        "# installed_packages = {package.key: package.version for package in pkg_resources.working_set}\n",
        "\n",
        "# # Uninstall TensorFlow if installed version is greater than 2.12.0\n",
        "# if 'tensorflow' in installed_packages and installed_packages['tensorflow'] > '2.12.0':\n",
        "#   pip.main(['uninstall', '-y', 'tensorflow'])\n",
        "#   print(\"TensorFlow uninstalled successfully\")\n",
        "# else:\n",
        "#   print(\"Did not find a version of TensorFlow greater than 2.12.0\")\n",
        "\n",
        "\n",
        "\n",
        "# # Check if TensorFlow is installed and its version is greater than 2.12.0\n",
        "# if 'tensorflow' in installed_packages and installed_packages['tensorflow'] == '2.12.0':\n",
        "#   print(\"TensorFlow 2.12.0 is already installed\")\n",
        "# else:\n",
        "#   print(\"Installing TensorFlow 2.12.0\")\n",
        "#   pip.main(['install', 'tensorflow==2.12.0'])\n",
        "\n",
        "\n",
        "\n",
        "# # At this point, tensorflow 2.12.0 is installed, so import the package we want\n",
        "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# # Print the installed version of KerasClassifier\n",
        "# #print(\"Installed version of KerasClassifier:\", KerasClassifier.__version__)\n",
        "# KerasClassifier\n",
        "\n",
        "# #!pip uninstall -y tensorflow\n",
        "# #!pip install tensorflow==2.12.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl0E2qi1j-L_"
      },
      "source": [
        "# Define functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zuu4rrIgkAnW"
      },
      "outputs": [],
      "source": [
        "# function to show missing values in dataset\n",
        "\n",
        "def get_type_missing(df):\n",
        "    df_types = pd.DataFrame()\n",
        "    df_types['data_type'] = df.dtypes\n",
        "    df_types['missing_values'] = df.isnull().sum()\n",
        "    return df_types.sort_values(by='missing_values', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vuds3_rnkAvm"
      },
      "outputs": [],
      "source": [
        "# function to create a confusion matrix\n",
        "\n",
        "def visualize_confusion_matrix(y_test, y_pred):\n",
        "    #\n",
        "    ## Calculate accuracy\n",
        "    #accuracy = accuracy_score(y_test, y_pred)\n",
        "    #print(\"Accuracy:\", accuracy)\n",
        "    #\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    #\n",
        "    # visualize confusion matrix with more detailed labels\n",
        "    # https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
        "    #\n",
        "    group_names = ['True Negative','False Positive','False Negative','True Positive']\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    plt.figure(figsize=(3.5, 2.0))  #default figsize is 6.4\" wide x 4.8\" tall, shrink to 3.5\" wide 2.0\" tall\n",
        "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', cbar=False)\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # use the .ravel function to pull out TN,TP,FN,TP\n",
        "    # https://analytics4all.org/2020/05/07/python-confusion-matrix/\n",
        "    TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "    # calculate different metrics\n",
        "    Accuracy = (( TP + TN) / ( TP + TN + FP + FN))\n",
        "    Sensitivity = TP / (TP + FN)\n",
        "    Specificity = TN / (TN + FP)\n",
        "    GeometricMean = math.sqrt(Sensitivity * Specificity)\n",
        "\n",
        "    # Precision is the ratio of true positive predictions to the total number of positive predictions made by the model\n",
        "    # average=binary for  binary classification models, average=micro for multiclass classification, average=weighted to match classification_report\n",
        "    Precision = precision_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Recall is the ratio of true positive predictions to the total number of actual positive instances in the data.\n",
        "    # average=binary for  binary classification models, average=micro for multiclass classification, average=weighted to match classification_report\n",
        "    Recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # F1-score is a metric that considers both precision and recall, providing a balance between the two.\n",
        "    # average=binary for  binary classification models, average=micro for multiclass classification, average=weighted to match classification_report\n",
        "    F1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # add details below graph to help interpret results\n",
        "    print('\\n\\n')\n",
        "    print('Confusion matrix\\n\\n', cm)\n",
        "    print('\\nTrue Negatives  (TN) = ', TN)\n",
        "    print('False Positives (FP) = ', FP)\n",
        "    print('False Negatives (FN) = ', FN)\n",
        "    print('True Positives  (TP) = ', TP)\n",
        "    print ('\\n')\n",
        "    print (\"Accuracy:       \", Accuracy)\n",
        "    print (\"Sensitivity:    \", Sensitivity)\n",
        "    print (\"Specificity:    \", Specificity)\n",
        "    print (\"Geometric Mean: \", GeometricMean)\n",
        "    print ('\\n')\n",
        "    print (\"Precision:       \", Precision)\n",
        "    print (\"Recall:          \", Recall)\n",
        "    print (\"f1-score:        \", F1)\n",
        "\n",
        "    print('\\n------------------------------------------------\\n')\n",
        "    # We want TN and TP to be approximately equal, because this indicates the dataset is well balanced.\n",
        "    # If TN and TP are very different, it indicates imbalanced data, which can lead to low accuracy due to overfitting\n",
        "    #if (TN/TP*100 < 40 or TN/TP*100 > 60):   #we want TN and TP to be approximately 50%, if the values are below 40% or over 60%, generate a warning\n",
        "    #    print(\"WARNING: the confusion matrix shows that TN and TP are very imbalanced, may lead to low accuracy!\")\n",
        "    #\n",
        "    return cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cw4bBmYtkAx-"
      },
      "outputs": [],
      "source": [
        "# function to report on model accuracy (TP, FP, FN, FP), precision, recall, f1-score\n",
        "# this function does not provide anything additional to the results from the previous function\n",
        "\n",
        "def model_classification_report(cm, y_test, y_pred):\n",
        "    report = classification_report(y_test, y_pred, digits=4)\n",
        "    print('\\n')\n",
        "    print(\"Classification Report: \\n\", report)\n",
        "    print('\\n\\n\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHEnelKtkA0V",
        "outputId": "160abd3f-1886-4844-db98-fa9883ff1be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Time: 2024-05-05 17:20:01\n",
            "The entire notebook runtime so far is 0 minutes\n"
          ]
        }
      ],
      "source": [
        "# function to show elapsed time for running notebook\n",
        "\n",
        "# start a timer so we can calculate the total runtime of this notebook\n",
        "notebook_start_time = time.time()  #seconds since epoch\n",
        "\n",
        "def show_elapsed_time():\n",
        "    #\n",
        "    # Get the current time as a struct_time object\n",
        "    current_time_struct = time.localtime()\n",
        "\n",
        "    # Format the struct_time as a string (yyyy-mm-dd HH:MM:SS format)\n",
        "    current_time_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", current_time_struct)\n",
        "\n",
        "    # Display the current time in HH:MM:SS format\n",
        "    print(\"Current Time:\", current_time_str)\n",
        "\n",
        "    # show a running total of elapsed time for the entire notebook\n",
        "    notebook_end_time = time.time()  #seconds since epoch\n",
        "    print(f\"The entire notebook runtime so far is {(notebook_end_time-notebook_start_time)/60:.0f} minutes\")\n",
        "\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhqIJe_Lkbtj"
      },
      "source": [
        "# Initialize variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ioPpjRGmkpVg"
      },
      "outputs": [],
      "source": [
        "# initialize variables to avoid undef errors\n",
        "\n",
        "accuracy_lr_unoptimized               = 0\n",
        "accuracy_lr_optimized                 = 0\n",
        "accuracy_nb_unoptimized               = 0\n",
        "accuracy_nb_optimized                 = 0\n",
        "accuracy_knn_unoptimized              = 0\n",
        "accuracy_knn_optimized                = 0\n",
        "accuracy_svm_unoptimized              = 0\n",
        "accuracy_svm_optimized                = 0\n",
        "accuracy_dt_unoptimized               = 0\n",
        "accuracy_dt_optimized                 = 0\n",
        "accuracy_rf_unoptimized               = 0\n",
        "accuracy_rf_optimized                 = 0\n",
        "accuracy_gb_unoptimized               = 0\n",
        "accuracy_gb_optimized                 = 0\n",
        "accuracy_mlp_unoptimized              = 0\n",
        "accuracy_mlp_optimized                = 0\n",
        "accuracy_fnn_unoptimized              = 0\n",
        "accuracy_fnn_optimized                = 0\n",
        "accuracy_cnn_unoptimized              = 0\n",
        "accuracy_cnn_optimized                = 0\n",
        "accuracy_rnn_unoptimized              = 0\n",
        "accuracy_rnn_optimized                = 0\n",
        "accuracy_lstm_unoptimized              = 0\n",
        "accuracy_lstm_optimized                = 0\n",
        "accuracy_gru_unoptimized              = 0\n",
        "accuracy_gru_optimized                = 0\n",
        "\n",
        "\n",
        "\n",
        "test_accuracy_sequential_unoptimized  = 0\n",
        "test_loss_sequential_unoptimized      = 0\n",
        "train_accuracy_sequential_unoptimized = 0\n",
        "train_loss_sequential_unoptimized     = 0\n",
        "test_accuracy_sequential_optimized    = 0\n",
        "test_loss_sequential_optimized        = 0\n",
        "train_accuracy_sequential_optimized   = 0\n",
        "train_loss_sequential_optimized       = 0\n",
        "\n",
        "test_accuracy_lstm_unoptimized        = 0\n",
        "test_loss_lstm_unoptimized            = 0\n",
        "train_accuracy_lstm_unoptimized       = 0\n",
        "train_loss_lstm_unoptimized           = 0\n",
        "test_accuracy_lstm_optimized          = 0\n",
        "test_loss_lstm_optimized              = 0\n",
        "train_accuracy_lstm_optimized         = 0\n",
        "train_loss_lstm_optimized             = 0\n",
        "\n",
        "test_accuracy_simplernn_unoptimized   = 0\n",
        "test_loss_simplernn_unoptimized       = 0\n",
        "train_accuracy_simplernn_unoptimized  = 0\n",
        "train_loss_simplernn_unoptimized      = 0\n",
        "test_accuracy_simplernn_optimized     = 0\n",
        "test_loss_simplernn_optimized         = 0\n",
        "train_accuracy_simplernn_optimized    = 0\n",
        "train_loss_simplernn_optimized        = 0\n",
        "\n",
        "test_accuracy_gru_unoptimized         = 0\n",
        "test_loss_gru_unoptimized             = 0\n",
        "train_accuracy_gru_unoptimized        = 0\n",
        "train_loss_gru_unoptimized            = 0\n",
        "test_accuracy_gru_optimized           = 0\n",
        "test_loss_gru_optimized               = 0\n",
        "train_accuracy_gru_optimized          = 0\n",
        "train_loss_gru_optimized              = 0\n",
        "\n",
        "\n",
        "best_params_mlp                       = \"\"\n",
        "best_params_sequential                = \"\"\n",
        "best_params_lstm                      = \"\"\n",
        "best_params_simplernn                 = \"\"\n",
        "best_params_gru                       = \"\"\n",
        "\n",
        "\n",
        "accuracy_ensemble_voting              = 0\n",
        "accuracy_ensemble_stacking            = 0\n",
        "accuracy_ensemble_boosting            = 0\n",
        "accuracy_ensemble_bagging             = 0\n",
        "\n",
        "cv_count                              = 10  #number of cross-validation folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rko_Sv99RJoB"
      },
      "outputs": [],
      "source": [
        "# # Load pickled datasets\n",
        "\n",
        "# # Determine the best location to obtain the source *.pkl file\n",
        "\n",
        "# # define *.pkl source file\n",
        "# filename = 'Edge-IIoTset2023_scaled_data_tuple.pkl'\n",
        "# LAN_location = 'http://datasets.nyx.local:80/datasets/Edge-IIoTset2023/Selected_dataset_for_ML_and_DL'  #high speed local copy on LAN\n",
        "# WAN_location = 'http://datasets.nyx.ca:8081/datasets/Edge-IIoTset2023/Selected_dataset_for_ML_and_DL'   #accessible to entire internet\n",
        "\n",
        "# #filename = 'CIC_IOT_Dataset2023_scaled_data_tuple.pkl'\n",
        "# #LAN_location = 'http://datasets.nyx.local:80/datasets/CIC_IOT_Dataset2023/csv'  #high speed local copy on LAN\n",
        "# #WAN_location = 'http://datasets.nyx.ca:8081/datasets/CIC_IOT_Dataset2023/csv'   #accessible to entire internet\n",
        "\n",
        "# # Get the FQDN of the local machine\n",
        "# fqdn = socket.getfqdn()\n",
        "# ipv4_address = socket.gethostbyname(socket.gethostname())\n",
        "# print(f\"Fully Qualified Domain Name (FQDN):{fqdn}, IPv4 address:{ipv4_address}\")\n",
        "# if ( \"nyx.local\" in fqdn ):\n",
        "#     # If inside the LAN, grab the local copy of the dataset\n",
        "#     print(f\"Detected Fully Qualified Domain Name of {fqdn}, dataset source is:\\n{LAN_location}/{filename}\")\n",
        "#     dataset = f\"{LAN_location}/{filename}\"\n",
        "# else:\n",
        "#     # If not inside the LAN, grab the dataset from an internet-accessible URL\n",
        "#     print(f\"Detected Fully Qualified Domain Name of {fqdn}, dataset source is:\\n{WAN_location}/{filename}\")\n",
        "#     dataset = f\"{WAN_location}/{filename}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Load pickle file from dataset that has already been labeled, scaled, randomly undersampled, and split into train/test/val\n",
        "\n",
        "# if not os.path.exists(filename):\n",
        "#   print(f\"Retrieving pickle file\", dataset)\n",
        "#   #!wget {dataset}          #wget typically exists on Linux but not Windows\n",
        "#   !curl -O {dataset}        #curl typically exists on both Linux and Windows\n",
        "# else:\n",
        "#   print(f\"Pickle file {filename} already exists\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Open the pickle file\n",
        "\n",
        "# # uncomment the filename you want to load\n",
        "# pickle_file = \"Edge-IIoTset2023_scaled_data_tuple.pkl\"\n",
        "# #pickle_file = \"CIC_IOT_Dataset2023_scaled_data_tuple.pkl\"\n",
        "\n",
        "# # Load the tuple using pickle\n",
        "# with open(pickle_file, 'rb') as f:\n",
        "#     #data_tuple = pickle.load(f)               #syntax for pandas <  version 2.0\n",
        "#     data_tuple = pd.read_pickle(pickle_file)   #syntax for pandas >= version 2.0\n",
        "\n",
        "# # split the pickle file into the lists from the source dataset\n",
        "# X_train, X_test, X_val, y_train, y_test, y_val = data_tuple\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xTdi1Hwemk4"
      },
      "source": [
        "# Load raw dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbkL_Okyed3m",
        "outputId": "23d066ca-e576-4e0e-b0a9-ee8eebdde2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully Qualified Domain Name (FQDN):1839c8f2e3b3, IPv4 address:172.28.0.12\n",
            "Detected Fully Qualified Domain Name of 1839c8f2e3b3, dataset source is:\n",
            "http://datasets.nyx.ca:8081/datasets/Edge-IIoTset2023/Selected_dataset_for_ML_and_DL/DNN-EdgeIIoT-dataset.csv\n"
          ]
        }
      ],
      "source": [
        "# define CSV source file\n",
        "\n",
        "filename = 'DNN-EdgeIIoT-dataset.csv'\n",
        "LAN_location = 'http://datasets.nyx.local:80/datasets/Edge-IIoTset2023/Selected_dataset_for_ML_and_DL'  #high speed local copy on LAN\n",
        "WAN_location = 'http://datasets.nyx.ca:8081/datasets/Edge-IIoTset2023/Selected_dataset_for_ML_and_DL'   #accessible to entire internet\n",
        "\n",
        "\n",
        "\n",
        "# Get the FQDN of the local machine\n",
        "fqdn = socket.getfqdn()\n",
        "ipv4_address = socket.gethostbyname(socket.gethostname())\n",
        "print(f\"Fully Qualified Domain Name (FQDN):{fqdn}, IPv4 address:{ipv4_address}\")\n",
        "if ( \"nyx.local\" in fqdn ):\n",
        "    # If inside the LAN, grab the local copy of the dataset\n",
        "    print(f\"Detected Fully Qualified Domain Name of {fqdn}, dataset source is:\\n{LAN_location}/{filename}\")\n",
        "    dataset = f\"{LAN_location}/{filename}\"\n",
        "else:\n",
        "    # If not inside the LAN, grab the dataset from an internet-accessible URL\n",
        "    print(f\"Detected Fully Qualified Domain Name of {fqdn}, dataset source is:\\n{WAN_location}/{filename}\")\n",
        "    dataset = f\"{WAN_location}/{filename}\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JbmRPXRiXP5",
        "outputId": "afd3d6f3-6797-4f9a-834a-142b7ccf870e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving dataset http://datasets.nyx.ca:8081/datasets/Edge-IIoTset2023/Selected_dataset_for_ML_and_DL/DNN-EdgeIIoT-dataset.csv\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            " 78 1161M   78  905M    0     0  4683k      0  0:04:13  0:03:18  0:00:55 1372k"
          ]
        }
      ],
      "source": [
        "# check to see if the dataset has already been retrieved from the remote web server\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "  print(f\"Retrieving dataset\", dataset)\n",
        "  #!wget {dataset}          #wget typically exists on Linux but not Windows\n",
        "  !curl -O {dataset}        #curl typically exists on both Linux and Windows\n",
        "else:\n",
        "  print(f\"File {filename} already exists\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWwVGaGqiXU2"
      },
      "outputs": [],
      "source": [
        "# Confirm the source datafile exists locally, just in case the previous cell failed to load the CSV file\n",
        "if not os.path.exists(filename):\n",
        "    raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n",
        "else:\n",
        "    print(f\"Confirmed existence of filename {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_s2qZH5iXjD"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file\n",
        "print(f\"Loading dataset from {filename}\")\n",
        "df = pd.read_csv(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MVMhjiOXdB5"
      },
      "outputs": [],
      "source": [
        "#view dimensions of dataset (rows and columns)\n",
        "print (\"Rows,columns in dataset:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V__jmKh-eeeg"
      },
      "outputs": [],
      "source": [
        "print(f\"Dropping rows from the dataset during debugging to speed up this notebook - turn this off when finished debugging!\")\n",
        "\n",
        "# cut dataset in half if > 2 million rows\n",
        "if ( len(df) > 2000000):\n",
        "    print(f\"Original size of dataset is\", len(df), \" rows\")\n",
        "    df.drop(df.index[::2], inplace=True)\n",
        "    print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
        "\n",
        "# cut dataset in half if > 1 million rows\n",
        "if ( len(df) > 1000000):\n",
        "    print(f\"Original size of dataset is\", len(df), \" rows\")\n",
        "    df.drop(df.index[::2], inplace=True)\n",
        "    print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
        "\n",
        "# cut dataset in half if > 0.5 million rows\n",
        "if ( len(df) > 500000):\n",
        "    print(f\"Original size of dataset is\", len(df), \" rows\")\n",
        "    df.drop(df.index[::2], inplace=True)\n",
        "    print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
        "\n",
        "# cut dataset in half if > 0.5 million rows\n",
        "if ( len(df) > 500000):\n",
        "    print(f\"Original size of dataset is\", len(df), \" rows\")\n",
        "    df.drop(df.index[::2], inplace=True)\n",
        "    print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
        "\n",
        "# cut dataset in half if > 250,000 rows\n",
        "if ( len(df) > 250000):\n",
        "    print(f\"Original size of dataset is\", len(df), \" rows\")\n",
        "    df.drop(df.index[::2], inplace=True)\n",
        "    print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
        "\n",
        "\n",
        "# cut dataset in half if > 100,000 rows\n",
        "if ( len(df) > 100000):\n",
        "    print(f\"Original size of dataset is\", len(df), \" rows\")\n",
        "    df.drop(df.index[::2], inplace=True)\n",
        "    print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
        "\n",
        "\n",
        "# # cut dataset in half if > 50,000 rows\n",
        "# if ( len(df) > 50000):\n",
        "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
        "#     df.drop(df.index[::2], inplace=True)\n",
        "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n",
        "\n",
        "\n",
        "# # cut dataset in half if > 25,000 rows\n",
        "# if ( len(df) > 25000):\n",
        "#     print(f\"Original size of dataset is\", len(df), \" rows\")\n",
        "#     df.drop(df.index[::2], inplace=True)\n",
        "#     print(f\"Dataset size after dropping all the even-numbered rows is\", len(df), \" rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEjUyQMieeid"
      },
      "outputs": [],
      "source": [
        "#view dimensions of dataset (rows and columns)\n",
        "print (\"Rows,columns in dataset:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yCIR_Tweemv"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuvrO_1ae0bY"
      },
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwF-dYr4ezmj"
      },
      "outputs": [],
      "source": [
        "# take a quick look at the data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hEO6Svveeqz"
      },
      "outputs": [],
      "source": [
        "# Display all the data rather than just a portion\n",
        "#pd.set_option('display.max_columns', None)\n",
        "#pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtkIZa6Zeeu1"
      },
      "outputs": [],
      "source": [
        "# check for any missing values in dataset\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSYrTeCWee2b"
      },
      "outputs": [],
      "source": [
        "# check for any missing datatypes\n",
        "get_type_missing(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVqK7ARKee7B"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49MyoC5_ee_k"
      },
      "outputs": [],
      "source": [
        "# look at all the datatypes of that are objects, in case any can be converted to integers\n",
        "df.describe(include='object')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3wccJ22efDi"
      },
      "outputs": [],
      "source": [
        "# look at the values in all of the features\n",
        "\n",
        "feature_names = df.columns.tolist()\n",
        "\n",
        "for feature_name in feature_names:\n",
        "    if feature_name in df.columns:\n",
        "        print('\\n')\n",
        "        print(f\"------------------\")\n",
        "        print(f\"{feature_name}\")\n",
        "        print(f\"------------------\")\n",
        "        print(df[feature_name].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_e5sF2pefHf"
      },
      "outputs": [],
      "source": [
        "#view dimensions of dataset (rows and columns)\n",
        "print (\"Rows,columns in dataset:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4kaTp5defLr"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "698aTJzVfQjH"
      },
      "source": [
        "# Dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR0zfa_qfQvU"
      },
      "source": [
        "## Fix up feature names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl0dAQclfM49"
      },
      "outputs": [],
      "source": [
        "# look at the column names\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GpHzBMXefPW"
      },
      "outputs": [],
      "source": [
        "print(df['frame.time'].value_counts().head())\n",
        "\n",
        "print(\"\\nNull Values:\")\n",
        "print(df['frame.time'].isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94R61fcjefTG"
      },
      "outputs": [],
      "source": [
        "# converting to datetime\n",
        "def convert_to_datetime(value):\n",
        "    try:\n",
        "         return pd.to_datetime(value)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "# skip the time-consuming conversion because we drop this feature later\n",
        "#df['frame.time'] = df['frame.time'].apply(convert_to_datetime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Muw-GgRefW7"
      },
      "outputs": [],
      "source": [
        "# Validating IP address\n",
        "\n",
        "print(df['ip.src_host'].value_counts().head())\n",
        "print('_________________________________________________________')\n",
        "print(df['ip.dst_host'].value_counts().head())\n",
        "print('_________________________________________________________')\n",
        "print(df['arp.src.proto_ipv4'].value_counts().head())\n",
        "print('_________________________________________________________')\n",
        "print(df['arp.dst.proto_ipv4'].value_counts().head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2rlHdGTefav"
      },
      "outputs": [],
      "source": [
        "# just for fun explore these values in the http.file_data column\n",
        "#df[df['Attack_label'] == 1]['http.file_data'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DHn2dUlefei"
      },
      "outputs": [],
      "source": [
        "df['mqtt.topic'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmTH8HwAefiU"
      },
      "outputs": [],
      "source": [
        "df['mqtt.protoname'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2q26stJefme"
      },
      "outputs": [],
      "source": [
        "df['dns.qry.name.len'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZZcjk9uefqd"
      },
      "outputs": [],
      "source": [
        "df['http.request.method'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxGUxjsNefuY"
      },
      "outputs": [],
      "source": [
        "# how many 0 (normal) and 1 (attack) values do we have?\n",
        "df['Attack_label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clxq2KZgf00K"
      },
      "source": [
        "# Visualization of raw dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzcVmv8Gefyb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 6))\n",
        "sns.countplot(data=df, x='Attack_label', hue='Attack_type', edgecolor='black', linewidth=1)\n",
        "plt.title('Attack Label vs Attack Type', fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F58oMBX-fyO7"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.pie(df, names='Attack_label', title='Distribution of Attack Labels')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17hirQgUfyTu"
      },
      "outputs": [],
      "source": [
        "fig = px.pie(df, names='Attack_type', title='Distribution of Attack Type')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEqQvYWcgEyL"
      },
      "source": [
        "- class imbalance issue - this can cause the machine learning model to result in biased results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4stS-wJ8gLBP"
      },
      "source": [
        "# Drop features\n",
        "Now using our domain knowledge we will only select useful features from our dataset and drop the rest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdYvIq0afyYr"
      },
      "outputs": [],
      "source": [
        "#view dimensions of dataset (rows and columns)\n",
        "print (\"Rows,columns in dataset:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CsQ1Yd8fycl"
      },
      "outputs": [],
      "source": [
        "# Identifying columns that are entirely NaN (empty) or have all zero values\n",
        "empty_or_zero_columns = df.columns[(df.isnull().all())\n",
        "| (df == 0).all()   | (df == 1).all() | (df == 1.0).all()\n",
        "| (df == 0.0).all() | (df == 2).all() | (df == 2.0).all()]\n",
        "\n",
        "# Displaying the identified columns\n",
        "empty_features = empty_or_zero_columns.tolist()\n",
        "\n",
        "print(\"These columns are all empty features:\")\n",
        "print(empty_features)\n",
        "\n",
        "\n",
        "for feature in empty_features:\n",
        "  if feature in df.columns:\n",
        "    df.drop(feature, axis=1, inplace=True)\n",
        "    print(\"Dropping empty feature:\", feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dNdzfp3fygz"
      },
      "outputs": [],
      "source": [
        "# show the columns to confirm the features have been dropped\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geVfCA4Ffyk1"
      },
      "outputs": [],
      "source": [
        "#view dimensions of dataset (rows and columns)\n",
        "print (\"Rows,columns in dataset:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29JrX58Cfyor"
      },
      "outputs": [],
      "source": [
        "# drop these features\n",
        "\n",
        "feature_names = [\"frame.time\", \"ip.src_host\", \"ip.dst_host\", \"arp.src.proto_ipv4\",\"arp.dst.proto_ipv4\",\n",
        "                \"http.file_data\",\"http.request.full_uri\",\"icmp.transmit_timestamp\",\n",
        "                \"http.request.uri.query\", \"tcp.options\",\"tcp.payload\",\"tcp.srcport\",\n",
        "                \"tcp.dstport\", \"udp.port\", \"mqtt.msg\", \"icmp.unused\", \"http.tls_port\", 'dns.qry.type',\n",
        "                'dns.retransmit_request_in', \"mqtt.msg_decoded_as\", \"mbtcp.trans_id\", \"mbtcp.unit_id\", \"http.request.method\", \"http.referer\",\n",
        "                \"http.request.version\", \"dns.qry.name.len\", \"mqtt.conack.flags\", \"mqtt.protoname\", \"mqtt.topic\"]\n",
        "\n",
        "# potential_drop_list = ['arp.opcode']\n",
        "\n",
        "for feature_name in feature_names:\n",
        "  if feature_name in df.columns:\n",
        "    df.drop(feature_name, axis=1, inplace=True)\n",
        "    print(\"Dropping feature:\", feature_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHskP5hgfysr"
      },
      "outputs": [],
      "source": [
        "#view dimensions of dataset (rows and columns)\n",
        "print (\"Rows,columns in dataset after dropping features:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1RbX_b0fywk"
      },
      "outputs": [],
      "source": [
        "# print(df[df['tcp.flags.ack'] == 1]['Attack_label'].value_counts(normalize=True))\n",
        "# print(df[df['tcp.flags.ack'] == 0]['Attack_label'].value_counts(normalize=True))\n",
        "\n",
        "df['Attack_label'].groupby(df['tcp.flags.ack']).value_counts(normalize=True)\n",
        "# hence we group by is prefered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q20MJDLDfzVe"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NErKgVOkfzZz"
      },
      "outputs": [],
      "source": [
        "#view dimensions of dataset (rows and columns)\n",
        "print (\"Rows,columns in dataset:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w_HYYW7ghKI"
      },
      "source": [
        "# Label encoding\n",
        "- Problem: if we use a machine learning model to predict the Attack label, it could predict it as 0.1, 0.2 or 0.99 which is not a valid Attack label\n",
        "- Solution: Label Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VBKrY0CnuXT"
      },
      "outputs": [],
      "source": [
        "# The final column in the dataset is Attack_type, and will contain either 0 or 1\n",
        "\n",
        "# Display unique values in the \"Attack_type\" column\n",
        "unique_attack_types = df['Attack_type'].unique()\n",
        "print(\"Unique Attack Types:\")\n",
        "print(unique_attack_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "galIuRn0fzea"
      },
      "outputs": [],
      "source": [
        "# add a column to the dataset called \"Attack_label\"\n",
        "# this column will only contain 0 or 1, and an integer representation of the text-based \"Attack_type\" column\n",
        "# if Attack_type=Normal, then Attack_label=0, otherwise, Attack_level=1\n",
        "\n",
        "le = LabelEncoder()    #assumes \"from sklearn.preprocessing import LabelEncoder\"\n",
        "df['Attack_label'] = le.fit_transform(df['Attack_label'])\n",
        "\n",
        "print(f\"Converting text-based Attack_type feature to integer-baesd Attack_label feature\")\n",
        "df['Attack_label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiCfNraMnBIs"
      },
      "outputs": [],
      "source": [
        "# Now that we have encoded the text-based \"Attack_type\" column into the integer-based \"Attack_label\" column, we can drop the \"Attack_type\" column\n",
        "df.drop('Attack_type', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWmIB3SCmPQy"
      },
      "outputs": [],
      "source": [
        "# confirm that the Attack_label column has been added, and the Attack_type column has been removed\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQz2nMiBgmiT"
      },
      "outputs": [],
      "source": [
        "# separate X and y variables (independent and dependent variables)\n",
        "\n",
        "X = df.drop(['Attack_label'], axis=1)\n",
        "y = df['Attack_label']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USwKs1Q3n6k0"
      },
      "outputs": [],
      "source": [
        "# Sanity check to confirm X and y have equal number of samples\n",
        "print(f\"X has\", len(X), \"samples\")\n",
        "print(f\"y has\", len(y), \"samples\")\n",
        "if ( len(X) != len(y) ):\n",
        "  raise ValueError (\"X and y are different lengths, please investigate!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSEpgbDmg7Cb"
      },
      "source": [
        "# Split data into train / test / validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCZM1U2Vg5lZ"
      },
      "outputs": [],
      "source": [
        "# Split X and y into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyS8i7qBgmnF"
      },
      "outputs": [],
      "source": [
        "# Now further split test set into testing and validation sets because Deep Learning models also have validation data\n",
        "# In this example, the train/test split in the previous cell was 80/20, so the 0.5 split you see in this cell splits the 20% of test data evenly into test and validation\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE-Svztiqen3"
      },
      "outputs": [],
      "source": [
        "# Sanity check to confirm X_train and y_train have equal number of samples\n",
        "print(f\"X_train has\", len(X_train), \"samples\")\n",
        "print(f\"y_train has\", len(y_train), \"samples\")\n",
        "if ( len(X_train) != len(y_train) ):\n",
        "  raise ValueError (\"X_train and y_train are different lengths, please investigate!\")\n",
        "\n",
        "# Sanity check to confirm X_test and y_test have equal number of samples\n",
        "print('\\n')\n",
        "print(f\"X_test has\", len(X_test), \"samples\")\n",
        "print(f\"y_test has\", len(y_test), \"samples\")\n",
        "if ( len(X_test) != len(y_test) ):\n",
        "  raise ValueError (\"X_test and y_test are different lengths, please investigate!\")\n",
        "\n",
        "# Sanity check to confirm X_val and y_val have equal number of samples\n",
        "print('\\n')\n",
        "print(f\"X_val has\", len(X_val), \"samples\")\n",
        "print(f\"y_val has\", len(y_val), \"samples\")\n",
        "if ( len(X_val) != len(y_val) ):\n",
        "  raise ValueError (\"X_val and y_val are different lengths, please investigate!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex3XJ1WF79fA"
      },
      "outputs": [],
      "source": [
        "# create a pie chart showing relative sizes of X_train, X_test, X_val\n",
        "\n",
        "\n",
        "# Labels for the pie chart\n",
        "labels = ['Training', 'Test', 'Validation']\n",
        "\n",
        "# Number of rows in each dataset split\n",
        "sizes = [len(X_train), len(X_test), len(X_val)]\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Dataset Split prior to class balancing')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n",
        "\n",
        "print(f\"X_train contains {len(X_train)} rows, y_train contains {len(y_train)} rows\")\n",
        "print(f\"X_test  contains {len(X_test)} rows, y_test  contains {len(y_test)} rows\")\n",
        "print(f\"X_val   contains {len(X_val)} rows, y_val   contains {len(y_val)} rows\")\n",
        "\n",
        "if (len(X_train) < len(X_test)):\n",
        "  print(f\"\\nWARNING: You will notice in the above chart that X_train has fewer rows than X_test or X_val\")\n",
        "  print(f\"This should not be the case, because the dataset has not yet undergone any reduction in the size of the training set.\")\n",
        "  print(f\"Please confirm that you are working on a clean dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5FCqwO_7n-P"
      },
      "outputs": [],
      "source": [
        "# create a pie chart showing the class balance in the training data\n",
        "\n",
        "print(f\"This pie chart shows the class balance in the training data.\")\n",
        "print(f\"The y_train data is labeled as 0=normal 1=attack \\n\")\n",
        "\n",
        "# Count the occurrences of each unique value\n",
        "normal_class   = sum(1 for value in y_train if value == 0)\n",
        "abnormal_class = sum(1 for value in y_train if value == 1)\n",
        "print(f\"  normal class contains {normal_class} samples\")\n",
        "print(f\"abnormal class contains {abnormal_class} samples\")\n",
        "if (normal_class == abnormal_class): print(\"WARNING: This dataset is not expected to be balanced yet.  Please investigate.\")\n",
        "if (normal_class != abnormal_class): print(\"This dataset is currently imbalanced, will be balanced in next section.\")\n",
        "\n",
        "# Extract labels and sizes for the pie chart\n",
        "labels = [\"Normal class\", \"Abnormal class\"]\n",
        "values = [normal_class, abnormal_class]\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Class distribution prior to balancing')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Balance data classes"
      ],
      "metadata": {
        "id": "okhEEwpeNbIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SMOTE\n",
        "This section is only shown as an example, this notebook balances the classes with random undersampling"
      ],
      "metadata": {
        "id": "ds-pbwdTMt7v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEEuQKj0gmvS"
      },
      "outputs": [],
      "source": [
        "# If you wanted to balance the classes with SMOTE instead, sample code shown below:\n",
        "\n",
        "## Create an instance of the SMOTE class\n",
        "#smote = SMOTE(sampling_strategy='auto')\n",
        "\n",
        "## Apply SMOTE to the training data\n",
        "#X_train_resampled, y_train_type_resampled = smote.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sequential undersampling\n",
        "This section is only shown as an example, this notebook balances the classes with random undersampling"
      ],
      "metadata": {
        "id": "s8vjRUZHM98r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # sample code to perform sequential undersampling instead of random undersampling\n",
        "\n",
        "# def sequential_undersample(X, y, minority_class_label, desired_ratio):\n",
        "#     # Separate majority and minority class samples\n",
        "#     majority_X = X[y != minority_class_label]\n",
        "#     majority_y = y[y != minority_class_label]\n",
        "#     minority_X = X[y == minority_class_label]\n",
        "#     minority_y = y[y == minority_class_label]\n",
        "\n",
        "#     print(f\"Percentage of minority class samples in y: {sum(y == minority_class_label) / len(y) * 100:.2f}%\")\n",
        "#     print(f\"Percentage of minority class samples in minority_y: {sum(minority_y == minority_class_label) / len(minority_y) * 100:.2f}%\")\n",
        "\n",
        "#     # Calculate the number of majority class samples to keep\n",
        "#     num_minority_samples = len(minority_X)\n",
        "#     #num_majority_samples = int(num_minority_samples * desired_ratio)\n",
        "#     num_majority_samples = num_minority_samples\n",
        "\n",
        "#     # Keep a portion of the majority class samples\n",
        "#     majority_X_subset = majority_X[:num_majority_samples]\n",
        "#     majority_y_subset = majority_y[:num_majority_samples]\n",
        "\n",
        "#     # Combine minority and subset of majority class samples\n",
        "#     X_balanced = np.concatenate((minority_X, majority_X_subset))\n",
        "#     y_balanced = np.concatenate((minority_y, majority_y_subset))\n",
        "\n",
        "#     return X_balanced, y_balanced\n",
        "\n",
        "# # Usage example\n",
        "# X_train_balanced, y_train_balanced = sequential_undersample(X_train, y_train, minority_class_label=1, desired_ratio=0.5)\n",
        "\n",
        "\n",
        "# # Count the occurrences of each unique value\n",
        "# normal_class   = sum(1 for value in y_train_balanced if value == 0)\n",
        "# abnormal_class = sum(1 for value in y_train_balanced if value == 1)\n",
        "# print(f\"  normal class contains {normal_class} samples\")\n",
        "# print(f\"abnormal class contains {abnormal_class} samples\")\n",
        "\n",
        "# # save the resampled values back to the original variable names so we can use consistent names throughout this notebook\n",
        "# X_train = X_train_balanced\n",
        "# y_train = y_train_balanced\n"
      ],
      "metadata": {
        "id": "nwQ486-HxXQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1ppbkz_hDOt"
      },
      "source": [
        "## random undersampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLOlGp6DgmrO"
      },
      "outputs": [],
      "source": [
        "# Initialize RandomUnderSampler\n",
        "rus = RandomUnderSampler(sampling_strategy=1, random_state=42)\n",
        "\n",
        "# Apply Random Under Sampling\n",
        "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Class balance before resampling\")\n",
        "print(y_train.value_counts())\n",
        "print('\\n')\n",
        "print(\"Class balance after resampling\")\n",
        "print(y_train_resampled.value_counts())\n",
        "\n",
        "# save the resampled values back to the original variable names so we can use consistent names throughout this notebook\n",
        "X_train = X_train_resampled\n",
        "y_train = y_train_resampled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7efaqGJ8sV-M"
      },
      "outputs": [],
      "source": [
        "# confirm the classes are balanced\n",
        "# Figure out how many rows of each class exist in y_train (0=normal, 1=abnormal)\n",
        "\n",
        "# Count occurrences of 0 and 1\n",
        "normal_class   = sum(1 for value in y_train if value == 0)\n",
        "abnormal_class = sum(1 for value in y_train if value == 1)\n",
        "\n",
        "print(f\"Count of   normal class: {normal_class}\")\n",
        "print(f\"Count of abnormal class: {abnormal_class}\")\n",
        "\n",
        "total_rows = abnormal_class + normal_class\n",
        "print(f\"Total Number of rows (normal+abnormal): {total_rows}\" )\n",
        "\n",
        "balance = abnormal_class / total_rows * 100\n",
        "balance = round(balance,2)\n",
        "\n",
        "print(f\"Percentage of abnormal class in dataset (abnormal/total*100): {balance}%\")\n",
        "if (balance  < 10): print(\"This dataset is very imbalanced, please beware of overfitting.\")\n",
        "if (balance != 50): print(\"WARNING: This dataset is supposed to be balanced.  Please investigate.\")\n",
        "if (balance == 50): print(\"This dataset is perfectly balanced.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG-D5SOlrhX-"
      },
      "outputs": [],
      "source": [
        "# Sanity check to confirm X_train and y_train have equal number of samples\n",
        "print(f\"X_train has\", len(X_train), \"samples\")\n",
        "print(f\"y_train has\", len(y_train), \"samples\")\n",
        "if ( len(X_train) != len(y_train) ):\n",
        "  raise ValueError (\"X_train and y_train are different lengths, please investigate!\")\n",
        "\n",
        "# Sanity check to confirm X_test and y_test have equal number of samples\n",
        "print('\\n')\n",
        "print(f\"X_test has\", len(X_test), \"samples\")\n",
        "print(f\"y_test has\", len(y_test), \"samples\")\n",
        "if ( len(X_test) != len(y_test) ):\n",
        "  raise ValueError (\"X_test and y_test are different lengths, please investigate!\")\n",
        "\n",
        "# Sanity check to confirm X_val and y_val have equal number of samples\n",
        "print('\\n')\n",
        "print(f\"X_val has\", len(X_val), \"samples\")\n",
        "print(f\"y_val has\", len(y_val), \"samples\")\n",
        "if ( len(X_val) != len(y_val) ):\n",
        "  raise ValueError (\"X_val and y_val are different lengths, please investigate!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6fJPskkwu0b"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9g5DPtbhRtm"
      },
      "source": [
        "# Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvpTZEY9gm_4"
      },
      "outputs": [],
      "source": [
        "# perform feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)  # Only transform the test       set, don't fit\n",
        "X_val_scaled   = scaler.transform(X_val)   # Only transform the validation set, don't fit\n",
        "\n",
        "# Save the values under original names so we can use consistent names in subsequent sections\n",
        "X_train = X_train_scaled\n",
        "X_test  = X_test_scaled\n",
        "X_val   = X_val_scaled\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIh3Z-tewywS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXmU6ZD1haWi"
      },
      "source": [
        "# Save progress in a pickle file\n",
        "We don't actually use this pickle file anywhere, but it is nice to have available for debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxvwDfv4gnH9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "output_file = \"Edge-IIoTset2023_scaled_data_tuple.pkl\"\n",
        "print(f\"Saving progress to pickle file: \", output_file)\n",
        "\n",
        "# Create a tuple\n",
        "data_tuple = (X_train, X_test, X_val, y_train, y_test, y_val)\n",
        "\n",
        "# Save the tuple using pickle\n",
        "with open(output_file, 'wb') as f:\n",
        "    pickle.dump(data_tuple, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yND7gPnIgnMM"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HLhMICYUN_O"
      },
      "source": [
        "# Visualization after processing raw dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imfwqbN1Uqzf"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "\n",
        "print(f\"X_train contains {len(X_train)} rows, y_train contains {len(y_train)} rows\")\n",
        "print(f\"X_test  contains {len(X_test)} rows, y_test  contains {len(y_test)} rows\")\n",
        "print(f\"X_val   contains {len(X_val)} rows, y_val   contains {len(y_val)} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6a6o1dtetLU"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pINZyEvogaWW"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEr6tv8Vgag1"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ce8-The9DSF"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7QWVKohE1H9"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZufQNzMjWdr6"
      },
      "outputs": [],
      "source": [
        "# create a pie chart showing relative sizes of X_train, X_test, X_val\n",
        "\n",
        "\n",
        "# Labels for the pie chart\n",
        "labels = ['Training', 'Test', 'Validation']\n",
        "\n",
        "# Number of rows in each dataset split\n",
        "sizes = [len(X_train), len(X_test), len(X_val)]\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Dataset Split after balancing classes by undersampling majority class in X_train,y_train')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n",
        "\n",
        "print(f\"X_train contains\", len(X_train), \"rows, y_train contains\", len(y_train), \" rows\")\n",
        "print(f\"X_test  contains\", len(X_test), \"rows, y_test  contains\", len(y_test), \" rows\")\n",
        "print(f\"X_val   contains\", len(X_val), \"rows, y_val   contains\", len(y_val), \" rows\")\n",
        "print(f\"Please note that this data is after undersampling the majority class for balancing, so it is expected that the 80/10/10 split is changed here.\")\n",
        "\n",
        "if (len(X_train) < len(X_test)):\n",
        "  print(f\"\\nWARNING: You will notice in the above chart that X_train has fewer rows than X_test or X_val\")\n",
        "  print(f\"This should not be the case, because the dataset has not yet undergone any reduction in the size of the training set.\")\n",
        "  print(f\"Please confirm that you are working on a clean dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXIu74uS0E5S"
      },
      "outputs": [],
      "source": [
        "# create a pie chart showing the class balance in the training data\n",
        "\n",
        "print(f\"This pie chart shows the class balance in the training data.\")\n",
        "print(f\"The y_train data is labeled as 0=normal 1=attack \\n\")\n",
        "\n",
        "# Count the occurrences of each unique value\n",
        "normal_class   = sum(1 for value in y_train if value == 0)\n",
        "abnormal_class = sum(1 for value in y_train if value == 1)\n",
        "if (normal_class != abnormal_class): print(\"WARNING: This dataset is supposed to be balanced.  Please investigate.\")\n",
        "if (normal_class == abnormal_class): print(\"This dataset is perfectly balanced.\")\n",
        "\n",
        "# Extract labels and sizes for the pie chart\n",
        "labels = [\"Normal class\", \"Abnormal class\"]\n",
        "values = [normal_class, abnormal_class]\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Class balance in training data labels')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNfsuc9u3PoC"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3c6_ZILWSr3"
      },
      "source": [
        "# Reduce dataset size to speed up analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUK7Pvii9cZC"
      },
      "source": [
        "NOTE: When reducing the size of your dataset to speed up training, it's generally recommended to sample only from the training data and leave the validation and test data untouched. Here's why:\n",
        "\n",
        "Training Data:\n",
        "- Sampling from the training data allows you to create a smaller subset that can be used for training the model.\n",
        "- Since the training data is used to update the model's parameters during training, reducing its size can significantly speed up the training process without affecting the evaluation of the model.\n",
        "\n",
        "Validation Data:\n",
        "- The validation data is used to tune hyperparameters and monitor the model's performance during training.\n",
        "- It's important to keep the validation data separate from the training data to ensure an unbiased evaluation of the model's performance.\n",
        "- Sampling from the validation data could lead to overfitting on the validation set and biased performance estimates.\n",
        "\n",
        "Test Data:\n",
        "- Similarly, the test data serves as an unbiased evaluation of the model's performance on unseen data.\n",
        "- Sampling from the test data could lead to overly optimistic performance estimates, as the model is evaluated on a different distribution than it will encounter in real-world scenarios.\n",
        "\n",
        "In summary, while it's common to reduce the size of the training data to speed up training, it's important to keep the validation and test data separate and unchanged to ensure unbiased evaluation of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rFWvt5AWZN3"
      },
      "outputs": [],
      "source": [
        "# save these values for comparison at the end of this section\n",
        "X_train_len = len(X_train)  #re-calculate after subsampling\n",
        "X_test_len  = len(X_test)   #re-calculate after subsampling\n",
        "X_val_len   = len(X_val)    #re-calculate after subsampling\n",
        "y_train_len = len(y_train)  #re-calculate after subsampling\n",
        "y_test_len  = len(y_test)   #re-calculate after subsampling\n",
        "y_val_len   = len(y_val)    #re-calculate after subsampling\n",
        "\n",
        "\n",
        "print(f\"X_train contains\", len(X_train), \"rows, y_train contains\", len(y_train), \" rows\")\n",
        "print(f\"X_test  contains\", len(X_test), \"rows, y_test  contains\", len(y_test), \" rows\")\n",
        "print(f\"X_val   contains\", len(X_val), \"rows, y_val   contains\", len(y_val), \" rows\")\n",
        "\n",
        "print(f\"\\nThe objective of this section is to see if we can speed up the training process by reducing the size of the dataset, but not losing too much accuracy.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI2ZFmW-WZR2"
      },
      "outputs": [],
      "source": [
        "# Define a list of fractions to keep\n",
        "#fractions_to_keep = [0.01, 0.02, 0.05, 0.10, 0.25, 0.50, 0.75, 1.0]\n",
        "fractions_to_keep = [0.25, 0.50, 0.75, 1.0]\n",
        "\n",
        "\n",
        "#initialize variables\n",
        "best_accuracy         = 0\n",
        "best_fraction_to_keep = 0\n",
        "accuracy_001          = 0\n",
        "accuracy_002          = 0\n",
        "accuracy_005          = 0\n",
        "accuracy_010          = 0\n",
        "accuracy_025          = 0\n",
        "accuracy_050          = 0\n",
        "accuracy_075          = 0\n",
        "accuracy_100          = 0\n",
        "\n",
        "# Iterate through different fractions\n",
        "for fraction_to_keep in fractions_to_keep:\n",
        "    # Randomly subsample the training set\n",
        "    num_samples_to_keep = int(len(X_train) * fraction_to_keep)\n",
        "    random_indices = np.random.choice(len(X_train), num_samples_to_keep, replace=False)\n",
        "\n",
        "    X_train_subsampled = X_train[random_indices]\n",
        "    y_train_subsampled = y_train.iloc[random_indices]   #use .iloc becaue y_train is a 1-dimensional array\n",
        "\n",
        "    # Train your model on the subsampled data\n",
        "    #clf = LogisticRegression(max_iter=800, random_state=42)\n",
        "    clf = MLPClassifier(random_state=42)\n",
        "    clf.fit(X_train_subsampled, y_train_subsampled)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy on the test set\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy on the test set (fraction_to_keep={fraction_to_keep:.4f}): {accuracy:.4f}\")\n",
        "\n",
        "    # Save the accuracy levels for later comparison\n",
        "    if fraction_to_keep == 0.01: accuracy_001 = accuracy\n",
        "    if fraction_to_keep == 0.02: accuracy_002 = accuracy\n",
        "    if fraction_to_keep == 0.05: accuracy_005 = accuracy\n",
        "    if fraction_to_keep == 0.10: accuracy_010 = accuracy\n",
        "    if fraction_to_keep == 0.25: accuracy_025 = accuracy\n",
        "    if fraction_to_keep == 0.50: accuracy_050 = accuracy\n",
        "    if fraction_to_keep == 0.75: accuracy_075 = accuracy\n",
        "    if fraction_to_keep == 1.0:  accuracy_100 = accuracy\n",
        "\n",
        "    # keep track of the best accuracy\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_fraction_to_keep = fraction_to_keep\n",
        "\n",
        "\n",
        "print(f\"The highest accuracy is {best_accuracy:.4f} using the {best_fraction_to_keep} fraction of the dataset\\n\")\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRQyYu9FWZV3"
      },
      "outputs": [],
      "source": [
        "# Visualize the results from the previous cell\n",
        "\n",
        "# Data extracted from the image\n",
        "data = {\n",
        "    'fraction_to_keep': [0.010, 0.020, 0.050, 0.100, 0.250, 0.500, 0.750, 1.000],\n",
        "    'accuracy': [accuracy_001, accuracy_002, accuracy_005, accuracy_010, accuracy_025, accuracy_050, accuracy_075, accuracy_100]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['fraction_to_keep'], df['accuracy'], marker='o')\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Accuracy on the Test Set by Fraction of Data Kept')\n",
        "plt.xlabel('Fraction of Data Kept')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# Adding text for each data point\n",
        "for i in range(len(df)):\n",
        "    plt.text(df['fraction_to_keep'][i], df['accuracy'][i], f\"{df['fraction_to_keep'][i]*100}%\", ha='right')\n",
        "\n",
        "# Adding grid for better readability\n",
        "plt.grid(True)\n",
        "\n",
        "# Save the figure with texts\n",
        "fig_path_with_text = 'accuracy_vs_data_fraction_with_text.png'\n",
        "plt.savefig(fig_path_with_text)\n",
        "\n",
        "# Show the figure\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OyynLMUWZZl"
      },
      "outputs": [],
      "source": [
        "# This cell will programnmatically determine the best_fraction_to_keep, by sacrificing some (small) amount of accuracy for speed.\n",
        "# Exactly how small?  Let's go with an acceptable loss of 1% of accuracy for better speed.\n",
        "\n",
        "acceptable_loss_of_accuracy = 0.0100  # 0.01*100= 1%  Tweak this value depending on how much accuracy you are willing to sacrifice\n",
        "\n",
        "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_100):\n",
        "    print(f\"Using 100% of the dataset gives {accuracy_100*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
        "    best_fraction_to_keep = 1.0\n",
        "\n",
        "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_075):\n",
        "    print(f\"Using  75% of the dataset gives {accuracy_075*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
        "    best_fraction_to_keep = 0.75\n",
        "\n",
        "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_050):\n",
        "    print(f\"Using  50% of the dataset gives {accuracy_050*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
        "    best_fraction_to_keep = 0.50\n",
        "\n",
        "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_025):\n",
        "    print(f\"Using  25% of the dataset gives {accuracy_025*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
        "    best_fraction_to_keep = 0.25\n",
        "\n",
        "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_010):\n",
        "    print(f\"Using  10% of the dataset gives {accuracy_010*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
        "    best_fraction_to_keep = 0.10\n",
        "\n",
        "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_005):\n",
        "    print(f\"Using   5% of the dataset gives {accuracy_005*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
        "    best_fraction_to_keep = 0.05\n",
        "\n",
        "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_002):\n",
        "    print(f\"Using   2% of the dataset gives {accuracy_002*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
        "    best_fraction_to_keep = 0.02\n",
        "\n",
        "if ((best_accuracy - acceptable_loss_of_accuracy) <= accuracy_001):\n",
        "    print(f\"Using   1% of the dataset gives {accuracy_001*100:.2f}% accuracy, which is an acceptable trade-off between accuracy and speed.\")\n",
        "    best_fraction_to_keep = 0.01\n",
        "\n",
        "print(f\"\\nBased on the above calculations, we will keep {best_fraction_to_keep*100:.0f}% of the dataset, which will still provide acceptable accuracy.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61P-uoy8Wxlw"
      },
      "outputs": [],
      "source": [
        "# Based on the accuracy calculations in the previous cell, decide how much of the dataset to keep\n",
        "fraction_to_keep = best_fraction_to_keep\n",
        "\n",
        "# Randomly subsample the training set\n",
        "num_samples_to_keep = int(len(X_train) * fraction_to_keep)\n",
        "random_indices = np.random.choice(len(X_train), num_samples_to_keep, replace=False)\n",
        "\n",
        "#save the sub-sampled data to temporary variable names\n",
        "X_train_subsampled = X_train[random_indices]\n",
        "y_train_subsampled = y_train.iloc[random_indices]   #use .iloc becaue y_train is a 1-dimensional array\n",
        "\n",
        "#save the sub-sampled data back to the original variable names that are used in subsequent sections\n",
        "X_train = X_train_subsampled\n",
        "y_train = y_train_subsampled\n",
        "\n",
        "print(f\"\\nPrior to downsampling the dataset sizes were:\")\n",
        "print(f\"---------------------------------------------\")\n",
        "print(f\"X_train previously contained {X_train_len} rows, y_train previously contained {y_train_len} rows\")  #these values were calculated prior to subsampling\n",
        "print(f\"X_test  previously contained {X_test_len} rows, y_test  previously contained {y_test_len} rows\")\n",
        "print(f\"X_val   previously contained {X_val_len} rows, y_val   previously contained {y_val_len} rows\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nAfter downsampling the training data without losing too much accuracy, the new size of the dataset is:\")\n",
        "print(f\"------------------------------------------------------------------------------------------------------\")\n",
        "X_train_len = len(X_train)  #re-calculate after subsampling\n",
        "X_test_len  = len(X_test)   #re-calculate after subsampling\n",
        "X_val_len   = len(X_val)    #re-calculate after subsampling\n",
        "y_train_len = len(y_train)  #re-calculate after subsampling\n",
        "y_test_len  = len(y_test)   #re-calculate after subsampling\n",
        "y_val_len   = len(y_val)    #re-calculate after subsampling\n",
        "\n",
        "print(f\"X_train now contains {X_train_len} rows, y_train now contains {y_train_len} rows\")  #these values were calculated prior to subsampling\n",
        "print(f\"X_test  now contains {X_test_len} rows, y_test  now contains {y_test_len} rows\")\n",
        "print(f\"X_val   now contains {X_val_len} rows, y_val   now contains {y_val_len} rows\")\n",
        "\n",
        "if (len(X_train) < len(X_test)):\n",
        "  print(f\"\\nWARNING: You have reduced the size of X_train by too much!  X_train should not be smaller than X_test\")\n",
        "  print(f\"This is because the training data was reduced via subsampling to speed up processing, but the test and validation data was not reduced in size.\")\n",
        "  print(f\"Please go back to the dataset reduction setting and adjust the sizes of of the fractions_to_keep list\")\n",
        "  raise ValueError (\"X_train has been reduced by too much, please investigate!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyW0259xJx-3"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCrIItQfIyuA"
      },
      "outputs": [],
      "source": [
        "# create a pie chart showing relative sizes of X_train, X_test, X_val\n",
        "\n",
        "\n",
        "# Labels for the pie chart\n",
        "labels = ['Training', 'Test', 'Validation']\n",
        "\n",
        "# Number of rows in each dataset split\n",
        "sizes = [len(X_train), len(X_test), len(X_val)]\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Dataset Split after subsampling training data')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n",
        "\n",
        "print(f\"X_train contains\", len(X_train), \"rows, y_train contains\", len(y_train), \" rows\")\n",
        "print(f\"X_test  contains\", len(X_test), \"rows, y_test  contains\", len(y_test), \" rows\")\n",
        "print(f\"X_val   contains\", len(X_val), \"rows, y_val   contains\", len(y_val), \" rows\")\n",
        "\n",
        "if (len(X_train) < len(X_test)):\n",
        "  print(f\"\\nWARNING: You have reduced the size of X_train by too much!  X_train should not be smaller than X_test\")\n",
        "  print(f\"This is because the training data was reduced via subsampling to speed up processing, but the test and validation data was not reduced in size.\")\n",
        "  print(f\"Please go back to the dataset reduction setting and adjust the sizes of of the fractions_to_keep list\")\n",
        "  raise ValueError (\"X_train has been reduced by too much, please investigate!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcDY72YWPeME"
      },
      "outputs": [],
      "source": [
        "# create a pie chart showing the class balance in the training data\n",
        "\n",
        "print(f\"This pie chart shows the class balance in the training data.\")\n",
        "print(f\"The y_train data is labeled as 0=normal 1=attack \\n\")\n",
        "\n",
        "# Count the occurrences of each unique value\n",
        "value_counts = Counter(y_train)    #assumes \"from collections import Counter\"\n",
        "\n",
        "# Extract labels and sizes for the pie chart\n",
        "labels = list(value_counts.keys())\n",
        "sizes = list(value_counts.values())\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Class balance in training data labels')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J5L7el-Pexs"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcB2_DRHt5Q9"
      },
      "source": [
        "# Model training with traditional classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBD4TM5Tt_cI"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XROyny8YuC-0"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the LogisticRegression model\n",
        "clf = LogisticRegression()\n",
        "\n",
        "default_params = clf.get_params()\n",
        "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# save accuracy for later comparison\n",
        "accuracy_lr_unoptimized = accuracy\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "# We want to see approximately equal results from TN and TP\n",
        "cm = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_whb7hJuEqL"
      },
      "source": [
        "### LR hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDSWt1gaZ1eH"
      },
      "source": [
        "The LogisticRegression() class in scikit-learn provides several parameters that can be adjusted to customize the logistic regression model. Here are some of the commonly used parameters:\n",
        "- penalty: Specifies the norm used in the penalization. It can take values like 'l1' (L1 regularization), 'l2' (L2 regularization), or 'none' (no regularization). The default is 'l2'.\n",
        "- C: Inverse of regularization strength. Smaller values specify stronger regularization. The default value is 1.0.\n",
        "- solver: Algorithm to use in the optimization problem. Options include 'liblinear', 'newton-cg', 'lbfgs', 'sag', and 'saga'. The default is 'lbfgs'.\n",
        "- max_iter: Maximum number of iterations taken for the solvers to converge. The default is 100.\n",
        "- multi_class: Specifies the strategy to use for multiclass classification. Options include 'auto', 'ovr' (one-vs-rest), and 'multinomial' (softmax). The default is 'auto'.\n",
        "- verbose: Controls the verbosity of the output. Set to an integer value greater than 0 for more verbosity. The default is 0.\n",
        "- random_state: Seed used by the random number generator. It ensures reproducibility of results. Set to an integer for reproducible output. The default is None.\n",
        "- tol: Tolerance for stopping criteria. The default is 1e-4.\n",
        "class_weight: Weights associated with classes. This can be used to handle class imbalance by assigning higher weights to minority classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGcEIpvsuDFG"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the  model\n",
        "clf = LogisticRegression()\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'penalty': ['None', 'l2'],\n",
        "    'C': [0.1, 1, 10],\n",
        "    'solver': ['lbfgs', 'liblinear'],\n",
        "    'max_iter': [100, 200],\n",
        "    'multi_class': ['auto'],\n",
        "    'random_state': [42]                 #for reproducible results\n",
        "}\n",
        "\n",
        "# Create an instance of GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "best_scores = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Scores:\", best_scores)\n",
        "\n",
        "# Create a new instance of the model with the best hyperparameters\n",
        "clf = LogisticRegression(**best_params)\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# final cross validation\n",
        "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
        "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
        "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
        "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
        "lr_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
        "lr_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
        "\n",
        "# Evaluate the model\n",
        "Accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", Accuracy)\n",
        "\n",
        "# save best parameters for later comparison\n",
        "best_params_lr = best_params\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_lr_optimized      = Accuracy\n",
        "sensitivity_lr_optimized   = Sensitivity\n",
        "specificity_lr_optimized   = Specificity\n",
        "geometricmean_lr_optimized = GeometricMean\n",
        "precision_lr_optimized     = Precision\n",
        "recall_lr_optimized        = Recall\n",
        "f1_lr_optimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhO1-d2tWlqC"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtRUIeblWn7p"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create an instance of the model\n",
        "#clf = GaussianNB()    # suitable for continuous features\n",
        "#clf = MultinomialNB() # used for discrete data like word counts\n",
        "clf = BernoulliNB()    # suitable for binary data, gives best accuracy for this dataset\n",
        "\n",
        "default_params = clf.get_params()\n",
        "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# save accuracy for later comparison\n",
        "accuracy_nb_unoptimized = accuracy\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_nb_unoptimized      = Accuracy\n",
        "sensitivity_nb_unoptimized   = Sensitivity\n",
        "specificity_nb_unoptimized   = Specificity\n",
        "geometricmean_nb_unoptimized = GeometricMean\n",
        "precision_nb_unoptimized     = Precision\n",
        "recall_nb_unoptimized        = Recall\n",
        "f1_nb_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWzonkRvWocl"
      },
      "source": [
        "### NB hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZY7y9IdjyWX"
      },
      "source": [
        "he BernoulliNB class in scikit-learn represents a naive Bayes classifier for Bernoulli-distributed data. Here are the parameters of the BernoulliNB class:\n",
        "\n",
        "- alpha: (float, default=1.0 or 1e-10)\n",
        "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
        "- binarize: (float or None, default=None)\n",
        "Threshold for binarizing (mapping to boolean) of sample features. If None, no binarization is performed.\n",
        "- fit_prior: (bool, default=True)\n",
        "Whether to learn class prior probabilities or not. If False, a uniform prior will be used.\n",
        "- class_prior: (array-like of shape (n_classes,), default=None)\n",
        "Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.\n",
        "- min_df: (float or int, default=1)\n",
        "When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature.\n",
        "- max_df: (float or int, default=1.0)\n",
        "When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts.\n",
        "- max_features: (int, default=None)\n",
        "If not None, build a vocabulary that only considers the top max_features ordered by term frequency across the corpus.\n",
        "- binary: (bool, default=False)\n",
        "Whether to treat all values greater than zero as 1, and all others as 0.\n",
        "- n_jobs: (int, default=None)\n",
        "The number of parallel jobs to run. -1 means using all processors.\n",
        "\n",
        "\n",
        "These parameters allow you to customize the behavior of the Bernoulli Naive Bayes classifier according to your specific needs and the characteristics of your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ySYbKhWWr9H"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the model\n",
        "clf = BernoulliNB()\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "# skip the sigmoid and poly kernels, rarely used\n",
        "param_grid = {\n",
        "    'alpha': [1.0, 0.1, 0.01, 0.001]\n",
        "}\n",
        "\n",
        "\n",
        "# Create an instance of GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "print(\"Performing GridSearchCV\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "best_scores = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Scores:\", best_scores)\n",
        "\n",
        "# Create a new instance of model with the best hyperparameters\n",
        "clf = BernoulliNB(**best_params)\n",
        "\n",
        "# Fit the model to the training data\n",
        "print(\"Fitting the model\")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# final cross validation\n",
        "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
        "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
        "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
        "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
        "nb_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
        "nb_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
        "nb_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
        "\n",
        "# Evaluate the model\n",
        "Accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", Accuracy)\n",
        "\n",
        "# save best parameters for later comparison\n",
        "best_params_nb = best_params\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_nb_optimized      = Accuracy\n",
        "sensitivity_nb_optimized   = Sensitivity\n",
        "specificity_nb_optimized   = Specificity\n",
        "geometricmean_nb_optimized = GeometricMean\n",
        "precision_nb_optimized     = Precision\n",
        "recall_nb_optimized        = Recall\n",
        "f1_nb_optimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAF8RprPSafQ"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hImjfzyoSb4_"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the model with the desired number of neighbors (you can adjust n_neighbors)\n",
        "clf = KNeighborsClassifier(n_neighbors=5)  # You can change the value of n_neighbors as needed\n",
        "\n",
        "default_params = clf.get_params()\n",
        "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# save accuracy for later comparison\n",
        "accuracy_knn_unoptimized = accuracy\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_knn_unoptimized      = Accuracy\n",
        "sensitivity_knn_unoptimized   = Sensitivity\n",
        "specificity_knn_unoptimized   = Specificity\n",
        "geometricmean_knn_unoptimized = GeometricMean\n",
        "precision_knn_unoptimized     = Precision\n",
        "recall_knn_unoptimized        = Recall\n",
        "f1_knn_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CsWZNHqSeda"
      },
      "source": [
        "### KNN hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idmiRTp_SdTr"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the model\n",
        "clf = KNeighborsClassifier()\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_neighbors': [5,10,15,20,30],\n",
        "    'weights': ['uniform', 'distance']\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Create an instance of GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "best_scores = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Scores:\", best_scores)\n",
        "\n",
        "# Create a new instance of the model with the best hyperparameters\n",
        "clf = KNeighborsClassifier(**best_params)\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# final cross validation\n",
        "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
        "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
        "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
        "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
        "knn_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
        "knn_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
        "knn_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
        "\n",
        "# Evaluate the model\n",
        "Accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", Accuracy)\n",
        "\n",
        "# save best parameters for later comparison\n",
        "best_params_knn = best_params\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_knn_optimized      = Accuracy\n",
        "sensitivity_knn_optimized   = Sensitivity\n",
        "specificity_knn_optimized   = Specificity\n",
        "geometricmean_knn_optimized = GeometricMean\n",
        "precision_knn_optimized     = Precision\n",
        "recall_knn_optimized        = Recall\n",
        "f1_knn_optimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfRwkMfoUI9J"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQk64zS_UKGr"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the model\n",
        "clf = SVC()\n",
        "\n",
        "default_params = clf.get_params()\n",
        "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# save accuracy for later comparison\n",
        "accuracy_svm_undersampled_unoptimized = accuracy\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_svm_unoptimized      = Accuracy\n",
        "sensitivity_svm_unoptimized   = Sensitivity\n",
        "specificity_svm_unoptimized   = Specificity\n",
        "geometricmean_svm_unoptimized = GeometricMean\n",
        "precision_svm_unoptimized     = Precision\n",
        "recall_svm_unoptimized        = Recall\n",
        "f1_svm_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0XSUG9jUQ99"
      },
      "source": [
        "### SVM hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXh3qfT1UTz0"
      },
      "outputs": [],
      "source": [
        "print(\"WARNING: SVM hyperparameter optimization is very CPU-intensive, this will take some time...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pR5IrihZUXrP"
      },
      "outputs": [],
      "source": [
        "# # Create an instance of the model\n",
        "# clf = SVC()\n",
        "\n",
        "# # Define the hyperparameters to tune\n",
        "# # skip the sigmoid and poly kernels, rarely used\n",
        "# param_grid = {\n",
        "#     'C': [0.1, 1, 10],\n",
        "#     'kernel': ['rbf', 'linear'],\n",
        "#     'probability': [True],               #probability=True is required for VotingClassifier\n",
        "#     'random_state': [42]                 #for reproducible results\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "# # Create an instance of GridSearchCV\n",
        "# grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1, verbose=2)\n",
        "\n",
        "# # Fit the grid search to the training data\n",
        "# print(\"Performing GridSearchCV\")\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# # Get the best hyperparameters\n",
        "# best_params = grid_search.best_params_\n",
        "# best_scores = grid_search.best_score_\n",
        "# print(\"Best Parameters:\", best_params)\n",
        "# print(\"Best Scores:\", best_scores)\n",
        "\n",
        "# # Create a new instance of model with the best hyperparameters\n",
        "# clf = SVC(**best_params)\n",
        "\n",
        "# # Fit the model to the training data\n",
        "# print(\"Fitting the model\")\n",
        "# clf.fit(X_train, y_train)\n",
        "\n",
        "# # Predict the labels for the test data\n",
        "# y_pred = clf.predict(X_test)\n",
        "\n",
        "# # final cross validation\n",
        "# cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
        "# print(f\"Cross validation scores: {cross_val_score_result}\")\n",
        "# print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
        "# print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
        "# svm_crossval_score_all  = cross_val_score_result         #save all folds in a list for later comparison\n",
        "# svm_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
        "# svm_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
        "\n",
        "# # Evaluate the model\n",
        "# Accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"Accuracy:\", Accuracy)\n",
        "\n",
        "# # save best parameters for later comparison\n",
        "# best_params_svm = best_params\n",
        "\n",
        "# # call previously defined function to create confusion matrix\n",
        "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# # save results calculated for this model for later comparison to other models\n",
        "# accuracy_svm_optimized      = Accuracy\n",
        "# sensitivity_svm_optimized   = Sensitivity\n",
        "# specificity_svm_optimized   = Specificity\n",
        "# geometricmean_svm_optimized = GeometricMean\n",
        "# precision_svm_optimized     = Precision\n",
        "# recall_svm_optimized        = Recall\n",
        "# f1_svm_optimized            = F1\n",
        "\n",
        "# # show a running total of elapsed time for the entire notebook\n",
        "# show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8cJRsSbutbf"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fyj2wUiAuxMD"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the DecisionTreeClassifier model\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "default_params = clf.get_params()\n",
        "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# save accuracy for later comparison\n",
        "accuracy_dt_unoptimized = accuracy\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_dt_unoptimized      = Accuracy\n",
        "sensitivity_dt_unoptimized   = Sensitivity\n",
        "specificity_dt_unoptimized   = Specificity\n",
        "geometricmean_dt_unoptimized = GeometricMean\n",
        "precision_dt_unoptimized     = Precision\n",
        "recall_dt_unoptimized        = Recall\n",
        "f1_dt_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqJnOAAduzPS"
      },
      "source": [
        "### DT hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJfxPRC4uxS9"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the DecisionTreeClassifier model\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 5, 10, 15, 25],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'random_state': [42]                 #for reproducible results\n",
        "}\n",
        "\n",
        "# Create an instance of GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=cv_count,n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "best_scores = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Scores:\", best_scores)\n",
        "\n",
        "# Create a new instance of the model with the best hyperparameters\n",
        "clf = DecisionTreeClassifier(**best_params)\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# final cross validation\n",
        "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
        "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
        "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
        "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
        "dt_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
        "dt_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
        "\n",
        "# Evaluate the model\n",
        "Accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", Accuracy)\n",
        "\n",
        "# save best parameters for later comparison\n",
        "best_params_dt = best_params\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_dt_optimized      = Accuracy\n",
        "sensitivity_dt_optimized   = Sensitivity\n",
        "specificity_dt_optimized   = Specificity\n",
        "geometricmean_dt_optimized = GeometricMean\n",
        "precision_dt_optimized     = Precision\n",
        "recall_dt_optimized        = Recall\n",
        "f1_dt_optimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5r8tytWvC0a"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3sDW_5PvEm2"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the RandomForestClassifier model\n",
        "clf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        "\n",
        "default_params = clf.get_params()\n",
        "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# save accuracy for later comparison\n",
        "accuracy_rf_unoptimized = accuracy\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_rf_unoptimized      = Accuracy\n",
        "sensitivity_rf_unoptimized   = Sensitivity\n",
        "specificity_rf_unoptimized   = Specificity\n",
        "geometricmean_rf_unoptimized = GeometricMean\n",
        "precision_rf_unoptimized     = Precision\n",
        "recall_rf_unoptimized        = Recall\n",
        "f1_rf_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vXwhEkDvGIz"
      },
      "source": [
        "### RF hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_GSVFaYctje"
      },
      "source": [
        "The RandomForestClassifier() class in scikit-learn provides several parameters that can be adjusted to customize the random forest model. Here are some of the commonly used parameters:\n",
        "\n",
        "- n_estimators: The number of trees in the forest. Higher values usually yield better performance, but also increase computational cost. The default is 100.\n",
        "- criterion: The function used to measure the quality of a split. It can be 'gini' for the Gini impurity or 'entropy' for the information gain. The default is 'gini'.\n",
        "- max_depth: The maximum depth of the tree. If None, nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. The default is None.\n",
        "- min_samples_split: The minimum number of samples required to split an internal node. The default is 2.\n",
        "min_samples_leaf: The minimum number of samples required to be at a leaf node. The default is 1.\n",
        "Vmax_features: The number of features to consider when looking for the best split. It can be 'auto' (sqrt(n_features)), 'sqrt' (sqrt(n_features)), 'log2' (log2(n_features)), or a number between 0 and 1 (fraction of total features). The default is 'auto'.\n",
        "- bootstrap: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. The default is True.\n",
        "- random_state: Seed used by the random number generator. It ensures reproducibility of results. Set to an integer for reproducible output. The default is None.\n",
        "- n_jobs: The number of jobs to run in parallel for both fit and predict. -1 means using all processors. The default is 1.\n",
        "- verbose: Controls the verbosity of the output. Set to an integer value greater than 0 for more verbosity. The default is 0.\n",
        "- class_weight: Weights associated with classes. This can be used to handle class imbalance by assigning higher weights to minority classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14cbw0m-vE4i"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the RandomForestClassifier model\n",
        "clf = RandomForestClassifier(n_jobs=-1)\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    #'n_estimators': [100, 200, 300, 500],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    #'max_depth': ['None', 5, 10],\n",
        "    #'class_weight': ['None', 'balanced'],\n",
        "    'random_state': [42]                 #for reproducible results\n",
        "}\n",
        "\n",
        "# Create an instance of GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "best_scores = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Scores:\", best_scores)\n",
        "\n",
        "# Create a new instance of the model with the best hyperparameters\n",
        "clf = RandomForestClassifier(**best_params)\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# final cross validation\n",
        "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
        "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
        "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
        "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
        "rf_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
        "rf_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
        "\n",
        "# Evaluate the model\n",
        "Accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", Accuracy)\n",
        "\n",
        "# save best parameters for later comparison\n",
        "best_params_rf = best_params\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_rf_optimized      = Accuracy\n",
        "sensitivity_rf_optimized   = Sensitivity\n",
        "specificity_rf_optimized   = Specificity\n",
        "geometricmean_rf_optimized = GeometricMean\n",
        "precision_rf_optimized     = Precision\n",
        "recall_rf_optimized        = Recall\n",
        "f1_rf_optimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKzvSno8vROI"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6dnN3TfQ93e"
      },
      "source": [
        "Gradient Boosting is a popular machine learning technique used for both regression and classification tasks. It is an ensemble learning method that builds a strong predictive model by combining the predictions of multiple weaker models, typically decision trees. Here's how gradient boosting works:\n",
        "\n",
        "1. Base Learners (Weak Models): Gradient Boosting combines the predictions of multiple weak models, often decision trees, to create a strong predictive model. These weak models are referred to as base learners or weak learners.\n",
        "2. Sequential Training: Gradient Boosting trains the weak models sequentially. Each new model is trained to correct the errors made by the previous models.\n",
        "3. Loss Function: During training, Gradient Boosting minimizes a loss function, which measures the difference between the actual target values and the predicted values of the ensemble model. Common loss functions include mean squared error (MSE) for regression tasks and cross-entropy loss for classification tasks.\n",
        "4. Gradient Descent Optimization: Gradient Boosting optimizes the loss function using gradient descent. In each iteration, the algorithm calculates the gradient of the loss function with respect to the current predictions and adjusts the predictions in the direction that minimizes the loss.\n",
        "5. Gradient Boosting Algorithm:\n",
        "- Initialize the ensemble model with a simple base learner (e.g., a decision stump).\n",
        "- Train the base learner on the training data and calculate the residuals (the differences between the actual and predicted values).\n",
        "- Fit a new base learner to the residuals, focusing on the areas where the previous model made errors.\n",
        "- Combine the predictions of all base learners to make the final ensemble prediction.\n",
        "- Repeat the process until a predefined number of base learners have been added, or until the loss function converges.\n",
        "6. Regularization: Gradient Boosting typically includes regularization techniques to prevent overfitting, such as limiting the depth of the trees, adding shrinkage (learning rate), and using subsampling (training on random subsets of the data).\n",
        "7. Hyperparameter Tuning: Gradient Boosting involves tuning several hyperparameters, such as the learning rate, tree depth, number of trees, and regularization parameters, to optimize the performance of the model.\n",
        "8. Scalability: Gradient Boosting can handle large datasets and high-dimensional feature spaces. However, training time and memory usage can increase with the complexity of the model and the size of the dataset.\n",
        "\n",
        "\n",
        "Overall, Gradient Boosting is a powerful and versatile technique that often achieves state-of-the-art performance on a wide range of machine learning tasks. It is widely used in practice due to its effectiveness and ease of implementation. Popular implementations of Gradient Boosting include XGBoost, LightGBM, and CatBoost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EToyqM-wvSjh"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the model\n",
        "clf = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "default_params = clf.get_params()\n",
        "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# save accuracy for later comparison\n",
        "accuracy_gb_unoptimized = accuracy\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq1_hvtTvaU1"
      },
      "source": [
        "### GB hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwi8Z5Q7vSrc"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the model\n",
        "clf = GradientBoostingClassifier()\n",
        "\n",
        "#default_params = clf.get_params()\n",
        "#print(f\"Training model with default hyperparameters of: {default_params}\")\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [100],               #10,200 reduced accuracy\n",
        "    'learning_rate': [0.1, 1.0],\n",
        "    'max_depth': [3],                    #add higher numbers reduces accuracy\n",
        "    'random_state': [42]                 #for reproducible results\n",
        "}\n",
        "\n",
        "# Create an instance of GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "best_scores = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Scores:\", best_scores)\n",
        "\n",
        "# Create a new instance of the model with the best hyperparameters\n",
        "clf = GradientBoostingClassifier(**best_params)\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# final cross validation\n",
        "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
        "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
        "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
        "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
        "gb_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
        "gb_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
        "\n",
        "# Evaluate the model\n",
        "Accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", Accuracy)\n",
        "\n",
        "# save best parameters for later comparison\n",
        "best_params_gb = best_params\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_gb_optimized      = Accuracy\n",
        "sensitivity_gb_optimized   = Sensitivity\n",
        "specificity_gb_optimized   = Specificity\n",
        "geometricmean_gb_optimized = GeometricMean\n",
        "precision_gb_optimized     = Precision\n",
        "recall_gb_optimized        = Recall\n",
        "f1_gb_optimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd5hpapcvoUh"
      },
      "source": [
        "# Compare accuracy of LR, NB, KNN, SVM, DT, GB, RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT87k7Bfvwgy"
      },
      "outputs": [],
      "source": [
        "# this section compares the accuracy of different methods:\n",
        "\n",
        "print(f\"LR  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_lr_unoptimized*100:.2f}%\")\n",
        "print(f\"LR  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_lr_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"NB  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_nb_unoptimized*100:.2f}%\")\n",
        "print(f\"NB  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_nb_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"KNN accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_knn_unoptimized*100:.2f}%\")\n",
        "print(f\"KNN  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_knn_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"SVM accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_svm_unoptimized*100:.2f}%\")\n",
        "print(f\"SVM accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_svm_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"DT  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_dt_unoptimized*100:.2f}%\")\n",
        "print(f\"DT  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_dt_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"RF  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_rf_unoptimized*100:.2f}%\")\n",
        "print(f\"RF  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_rf_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"GB  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_gb_unoptimized*100:.2f}%\")\n",
        "print(f\"GB  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_gb_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"MLP accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_mlp_unoptimized*100:.2f}%\")\n",
        "print(f\"MLP accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_mlp_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF4b8omCzche"
      },
      "source": [
        "# Model training with Deep Learning classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwSNcKXzhD3H"
      },
      "source": [
        "## MLP Multi-Layer Perceptron\n",
        "\n",
        "MLPClassifier is a class in scikit-learn that represents a Multi-layer Perceptron (MLP) classifier, which is a type of artificial neural network.\n",
        "\n",
        "An MLP is a feedforward neural network that consists of multiple layers of nodes (neurons) and can learn complex patterns and relationships in data.\n",
        "\n",
        "The MLPClassifier is specifically designed for classification tasks.\n",
        "\n",
        "Example of all hyperparameters:\n",
        "- hidden_layer_sizes=(100, 50),  # Architecture of hidden layers\n",
        "- activation='relu',             # Activation function ('relu' is common)\n",
        "- solver='adam',                 # Optimization solver\n",
        "- alpha=0.0001,                  # L2 penalty (regularization)\n",
        "- batch_size='auto',             # Size of mini-batches ('auto' is adaptive)\n",
        "- learning_rate='constant',      # Learning rate schedule\n",
        "- learning_rate_init=0.001,      # Initial learning rate\n",
        "- max_iter=500,                  # Maximum number of iterations\n",
        "- shuffle=True,                  # Shuffle data in each iteration\n",
        "- random_state=42,               # Random seed for reproducibility\n",
        "- verbose=True                   # Print progress during training\n",
        "\n",
        "\n",
        "Multi-Layer Perceptron (MLP) classifier with three or more hidden layers is typically considered a deep learning model. The term \"deep\" in deep learning refers to the presence of multiple layers in the neural network architecture. While there's no strict definition of how many layers constitute a \"deep\" network, models with three or more hidden layers are commonly regarded as deep neural networks.\n",
        "\n",
        "MLP classifiers, being feedforward neural networks (FNN) with multiple layers, can learn complex patterns and representations from data, making them suitable for various classification tasks. The depth of the network allows it to learn hierarchical features and capture intricate relationships within the data, leading to improved performance on tasks with large and complex datasets.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Feedforward_neural_network\n",
        "A feedforward neural network (FNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers.[2] Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops,[2] in contrast to recurrent neural networks,[3] which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method[4][5][6][7][8] and are colloquially referred to as the \"vanilla\" neural networks.[9]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu5EaAdJ7_Ou"
      },
      "outputs": [],
      "source": [
        "# Sanity check to confirm X_train and y_train have equal number of samples\n",
        "print(f\"X_train has \", len(X_train), \"samples\")\n",
        "print(f\"y_train has \", len(y_train), \"samples\")\n",
        "if ( len(X_train) != len(y_train) ):\n",
        "  raise ValueError (\"X_train and y_train are different lengths, please investigate!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG_Awu9z_vDC"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the model\n",
        "clf = MLPClassifier(random_state=42)   #hidden_layer_sizes can be added here as tuples, see hyperparameter cell for an example\n",
        "\n",
        "default_params = clf.get_params()\n",
        "print(f\"Training model with default hyperparameters of: {default_params}\")\n",
        "\n",
        "# Fit the model to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# save accuracy for later comparison\n",
        "accuracy_mlp_unoptimized = accuracy\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_mlp_unoptimized      = Accuracy\n",
        "sensitivity_mlp_unoptimized   = Sensitivity\n",
        "specificity_mlp_unoptimized   = Specificity\n",
        "geometricmean_mlp_unoptimized = GeometricMean\n",
        "precision_mlp_unoptimized     = Precision\n",
        "recall_mlp_unoptimized        = Recall\n",
        "f1_mlp_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just testing\n",
        "\n",
        "# Evaluate the model on training data\n",
        "train_accuracy = clf.score(X_train, y_train)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_accuracy = clf.score(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Evaluate the model on val data\n",
        "val_accuracy = clf.score(X_val, y_val)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "test_accuracy_mlp_unoptimized  = test_accuracy\n",
        "train_accuracy_mlp_unoptimized = train_accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "_WR4nmZCRPiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM2ojMz4hSwC"
      },
      "source": [
        "## MLP hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UtoDTXchRKr"
      },
      "outputs": [],
      "source": [
        "# This cell is commented out during testing because it takes ~25 minutes to run, and produces these results:\n",
        "# Accuracy:        0.90353978113784\n",
        "# Sensitivity:     0.7249120594677109\n",
        "# Specificity:     0.9701365583582067\n",
        "# Geometric Mean:  0.8386081865116538\n",
        "# Precision:        0.9033336255244485\n",
        "# Recall:           0.90353978113784\n",
        "# f1-score:         0.9000212832043374\n",
        "\n",
        "\n",
        "# Create an instance of the model\n",
        "clf = MLPClassifier()\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(100,), (64,32)],  #(64,32)) was the best parameter found, also tried (100,), (64,32), (64,32,16), (128,64,32) as tuples for hidden layers\n",
        "    'max_iter': [200],                        # also tried 100, 300\n",
        "    'alpha': [0.0001],                        #also tried 0.001, 0.01\n",
        "    'activation': ['relu'],                   #also tried tanh\n",
        "    'learning_rate': ['constant'],            #also tried adaptive\n",
        "    'random_state': [42]                      #for reproducible results\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Create an instance of GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=cv_count, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "print(f\"Fitting the model\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "best_scores = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Scores:\", best_scores)\n",
        "\n",
        "# Create a new instance of the model with the best hyperparameters\n",
        "clf = MLPClassifier(**best_params)\n",
        "\n",
        "# Fit the model to the training data\n",
        "print(f\"Fitting the model with best_params {best_params}\")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# final cross validation\n",
        "cross_val_score_result = cross_val_score(clf, X_train, y_train, cv=cv_count)\n",
        "print(f\"Cross validation scores: {cross_val_score_result}\")\n",
        "print(f\"Mean cross validation score: {cross_val_score_result.mean()}\")\n",
        "print(f\"Standard Deviation cross validation score: {cross_val_score_result.std()}\")\n",
        "mlp_crossval_score_mean = cross_val_score_result.mean()  #save mean   crossval score in a variable for later comparison\n",
        "mlp_crossval_score_std  = cross_val_score_result.std()   #save stddev crossval score in a variable for later comparison\n",
        "\n",
        "# Evaluate the model\n",
        "Accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", Accuracy)\n",
        "\n",
        "# save best parameters for later comparison\n",
        "best_params_mlp = best_params\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_mlp_optimized      = Accuracy\n",
        "sensitivity_mlp_optimized   = Sensitivity\n",
        "specificity_mlp_optimized   = Specificity\n",
        "geometricmean_mlp_optimized = GeometricMean\n",
        "precision_mlp_optimized     = Precision\n",
        "recall_mlp_optimized        = Recall\n",
        "f1_mlp_optimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZiacNkAz5ob"
      },
      "source": [
        "## Sequential FNN\n",
        "## (does not require time steps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the context of the Keras library, Sequential() is not a classifier itself, but rather a type of model architecture. It is used to create sequential models, which are a linear stack of layers.\n",
        "\n",
        "These models are typically used for building feedforward neural networks (FNNs), where the data flows sequentially from the input layer through one or more hidden layers to the output layer. Each layer in a sequential model has connections only to the layers that follow it in the model.\n",
        "\n",
        "You can use different types of layers such as Dense, Dropout, Conv1D, Conv2D, LSTM, etc., in a Sequential() model depending on the type of problem you are solving. Once the layers are added to the model, you compile it with an optimizer, a loss function, and optionally, performance metrics. After compilation, you can train the model on your data using the fit() method."
      ],
      "metadata": {
        "id": "POZIobH6T2NS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr4SGy4fGor2"
      },
      "outputs": [],
      "source": [
        "# row, columns in X_train\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCXa0b49Goud"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TYqNHTR7BWb"
      },
      "outputs": [],
      "source": [
        "# Sanity check to confirm X_train and y_train have equal number of samples\n",
        "print(f\"X_train has \", len(X_train), \"samples\")\n",
        "print(f\"y_train has \", len(y_train), \"samples\")\n",
        "if ( len(X_train) != len(y_train) ):\n",
        "  raise ValueError (\"X_train and y_train are different lengths, please investigate!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lman7Ed5nIW1"
      },
      "outputs": [],
      "source": [
        "# to-do: add another dropout after dense(32), and another dense layer with 16 neurons\n",
        "\n",
        "# Sequential (prior to optimization) -- Backup\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Define input shape based on the features in X_train\n",
        "input_shape = X_train.shape[1]\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([                                           #Initializes a sequential neural network model\n",
        "    Dense(64, activation='relu', input_shape=(input_shape,)),  #Add a fully connected layer (also known as a dense layer) with 64 neurons\n",
        "    Dropout(0.5),                                              #Optional dropout layer for regularization to randomly sets a fraction of input units to zero during training to prevent overfitting\n",
        "    Dense(32, activation='relu'),                              #Adds another fully connected layer with 32 neurons and ReLU activation function.\n",
        "    Dense(1, activation='sigmoid')                             # Output layer with sigmoid activation for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Model Summary\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(model.summary())\n",
        "\n",
        "# Train the model\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Training the model\")\n",
        "print(f\"-----------------------------------------\")\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on training data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the model on training data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
        "print(\"Training Loss:\", train_loss)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "\n",
        "# Evaluate the model on test data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the model on test data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "test_accuracy_sequential_unoptimized  = test_accuracy\n",
        "test_loss_sequential_unoptimized      = test_loss\n",
        "train_accuracy_sequential_unoptimized = train_accuracy\n",
        "train_loss_sequential_unoptimized     = train_loss\n",
        "\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_sequential_unoptimized      = Accuracy\n",
        "sensitivity_sequential_unoptimized   = Sensitivity\n",
        "specificity_sequential_unoptimized   = Specificity\n",
        "geometricmean_sequential_unoptimized = GeometricMean\n",
        "precision_sequential_unoptimized     = Precision\n",
        "recall_sequential_unoptimized        = Recall\n",
        "f1_sequential_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4qfa3GSpzC8"
      },
      "outputs": [],
      "source": [
        "# row, columns in X_train\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57nmUvCEocnQ"
      },
      "source": [
        "### FNN ToDo List:\n",
        "1. change the number of neurons for each layer to find the best value\n",
        "\n",
        "2. change the number of layers\n",
        "\n",
        "3. change activation functions -> calculate metrics for each\n",
        "\n",
        "4. change optimization algorithms in NN\n",
        "\n",
        "5. Regularization Techniques\n",
        "\n",
        "6. Learning Rate\n",
        "\n",
        "7. Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZFxiUiMrsiL"
      },
      "outputs": [],
      "source": [
        "# # no better than previous cell\n",
        "\n",
        "# # Sequential (prior to optimization) -- New test - Backup stable version - with validation\n",
        "\n",
        "\n",
        "# # Define input shape based on the features in X_train\n",
        "# input_shape = X_train.shape[1]\n",
        "\n",
        "\n",
        "# # Define the model\n",
        "# model = Sequential([                                           #Initializes a sequential neural network model\n",
        "#     Dense(64, activation='relu', input_shape=(input_shape,)),  #Add a fully connected layer (also known as a dense layer) with 64 neurons\n",
        "#     Dropout(0.5),                                              #Optional dropout layer for regularization to randomly sets a fraction of input units to zero during training to prevent overfitting\n",
        "#     Dense(32, activation='tanh'),                              #Adds another fully connected layer with 32 neurons and RtanheLU activation function.\n",
        "#     Dense(1, activation='sigmoid')                             # Output layer with sigmoid activation for binary classification\n",
        "# ])\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='binary_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# # Print model summary\n",
        "# print(f\"\\n\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# print(f\"Model Summary\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# print(model.summary())\n",
        "\n",
        "# # Train the model\n",
        "# print(f\"\\n\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# print(f\"Training the model\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# # Evaluate the model on training data\n",
        "# print(f\"\\n\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# print(f\"Evaluating the model on training data\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
        "# print(\"Training Loss:\", train_loss)\n",
        "# print(\"Training Accuracy:\", train_accuracy)\n",
        "# print(f\"\\n\")\n",
        "\n",
        "\n",
        "# # Evaluate the model on test data\n",
        "# print(f\"\\n\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# print(f\"Evaluating the model on test data\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "# print(\"Test Loss:\", test_loss)\n",
        "# print(\"Test Accuracy:\", test_accuracy)\n",
        "# print(f\"\\n\")\n",
        "\n",
        "# # save results calculated for this model for later comparison to other models\n",
        "# test_accuracy_sequential_unoptimized  = test_accuracy\n",
        "# test_loss_sequential_unoptimized      = test_loss\n",
        "# train_accuracy_sequential_unoptimized = train_accuracy\n",
        "# train_loss_sequential_unoptimized     = train_loss\n",
        "\n",
        "# # call previously defined function to create confusion matrix\n",
        "# cm = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# # show a running total of elapsed time for the entire notebook\n",
        "# show_elapsed_time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3How7jnzGu1"
      },
      "outputs": [],
      "source": [
        "# # no better than previous cell\n",
        "\n",
        "# # Sequential (prior to optimization) -- New test\n",
        "\n",
        "\n",
        "# # Define input shape based on the features in X_train\n",
        "# input_shape = X_train.shape[1]\n",
        "\n",
        "# # Define the model\n",
        "# model = Sequential([\n",
        "#     Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "#     Dropout(0.5),  # Optional dropout layer for regularization\n",
        "#     Dense(64, activation='tanh'),\n",
        "#     Dropout(0.5),\n",
        "#     Dense(32, activation='tanh'),\n",
        "#     Dropout(0.5),\n",
        "#     Dense(16, activation='tanh'),   # add another hidden layer\n",
        "#     Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
        "# ])\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='binary_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# # Print model summary\n",
        "# print(f\"\\n\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# print(f\"Model Summary\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# print(model.summary())\n",
        "\n",
        "# # Train the model\n",
        "# print(f\"\\n\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# print(f\"Training the model\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# # Evaluate the model on training data\n",
        "# print(f\"\\n\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# print(f\"Evaluating the model on training data\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
        "# print(\"Training Loss:\", train_loss)\n",
        "# print(\"Training Accuracy:\", train_accuracy)\n",
        "# print(f\"\\n\")\n",
        "\n",
        "\n",
        "# # Evaluate the model on test data\n",
        "# print(f\"\\n\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# print(f\"Evaluating the model on test data\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "# print(\"Test Loss:\", test_loss)\n",
        "# print(\"Test Accuracy:\", test_accuracy)\n",
        "# print(f\"\\n\")\n",
        "\n",
        "# # save results calculated for this model for later comparison to other models\n",
        "# test_accuracy_sequential_unoptimized  = test_accuracy\n",
        "# test_loss_sequential_unoptimized      = test_loss\n",
        "# train_accuracy_sequential_unoptimized = train_accuracy\n",
        "# train_loss_sequential_unoptimized     = train_loss\n",
        "\n",
        "# # call previously defined function to create confusion matrix\n",
        "# cm = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# # show a running total of elapsed time for the entire notebook\n",
        "# show_elapsed_time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MnAyWYiiEbO"
      },
      "outputs": [],
      "source": [
        "# # no better than previous cell\n",
        "\n",
        "# # Test FNN on different activation functions:\n",
        "\n",
        "# # Define a list of activation functions to test\n",
        "# activation_functions = ['sigmoid', 'linear', 'tanh', 'relu']\n",
        "# activation_functions = ['relu']  #after testing, relu was the best\n",
        "\n",
        "# # Dictionary to store results\n",
        "# results = {'Activation Function': [],\n",
        "#            'Train Loss': [],\n",
        "#            'Train Accuracy': [],\n",
        "#            'Test Loss': [],\n",
        "#            'Test Accuracy': []}\n",
        "\n",
        "# # Define input shape based on the features in X_train\n",
        "# input_shape = X_train.shape[1]\n",
        "\n",
        "# for activation_function in activation_functions:\n",
        "#     # Define the model\n",
        "#     model = Sequential([\n",
        "#         Dense(64, activation=activation_function, input_shape=(input_shape,)),\n",
        "#         Dropout(0.5),  # Optional dropout layer for regularization\n",
        "#         Dense(32, activation=activation_function),\n",
        "#         Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
        "#     ])\n",
        "\n",
        "#     # Compile the model\n",
        "#     model.compile(optimizer='adam',\n",
        "#                   loss='binary_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "\n",
        "#     # Print model summary\n",
        "#     print(f\"\\n\")\n",
        "#     print(f\"-----------------------------------------\")\n",
        "#     print(f\"Model Summary - Activation Function: {activation_function}\")\n",
        "#     print(f\"-----------------------------------------\")\n",
        "#     print(model.summary())\n",
        "\n",
        "#     # Train the model\n",
        "#     print(f\"\\n\")\n",
        "#     print(f\"-----------------------------------------\")\n",
        "#     print(f\"Training the model - Activation Function: {activation_function}\")\n",
        "#     print(f\"-----------------------------------------\")\n",
        "#     history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "#     # Evaluate the model on training data\n",
        "#     print(f\"\\n\")\n",
        "#     print(f\"-----------------------------------------\")\n",
        "#     print(f\"Evaluating the model on training data - Activation Function: {activation_function}\")\n",
        "#     print(f\"-----------------------------------------\")\n",
        "#     train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
        "#     print(\"Training Loss:\", train_loss)\n",
        "#     print(\"Training Accuracy:\", train_accuracy)\n",
        "#     print(f\"\\n\")\n",
        "\n",
        "#     # Evaluate the model on test data\n",
        "#     print(f\"\\n\")\n",
        "#     print(f\"-----------------------------------------\")\n",
        "#     print(f\"Evaluating the model on test data - Activation Function: {activation_function}\")\n",
        "#     print(f\"-----------------------------------------\")\n",
        "#     test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "#     print(\"Test Loss:\", test_loss)\n",
        "#     print(\"Test Accuracy:\", test_accuracy)\n",
        "#     print(f\"\\n\")\n",
        "\n",
        "#     # Save results\n",
        "#     results['Activation Function'].append(activation_function)\n",
        "#     results['Train Loss'].append(train_loss)\n",
        "#     results['Train Accuracy'].append(train_accuracy)\n",
        "#     results['Test Loss'].append(test_loss)\n",
        "#     results['Test Accuracy'].append(test_accuracy)\n",
        "\n",
        "# # Convert results to a DataFrame\n",
        "# results_df = pd.DataFrame(results)\n",
        "\n",
        "# # Print results DataFrame\n",
        "# print(results_df)\n",
        "\n",
        "# # call previously defined function to create confusion matrix\n",
        "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# # show a running total of elapsed time for the entire notebook\n",
        "# show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYJXrcVoo0wd"
      },
      "source": [
        "### Results of testing FNN on different activation functions\n",
        "\n",
        "| Activation Function | Train Loss | Train Accuracy | Test Loss | Test Accuracy |\n",
        "|---------------------|------------|----------------|-----------|---------------|\n",
        "| relu                | 0.306027   | 0.841822       | 0.287624  | 0.906536      |\n",
        "| tanh                | 0.314723   | 0.840166       | 0.283689  | 0.881940      |\n",
        "| sigmoid             | 0.378268   | 0.818841       | 0.347959  | 0.874153      |\n",
        "| linear              | 0.366655   | 0.826812       | 0.333410  | 0.881313      |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0cf2n_ZZnq3"
      },
      "outputs": [],
      "source": [
        "# Extracting loss history from the training\n",
        "train_loss_history = history.history['loss']\n",
        "val_loss_history = history.history['val_loss']\n",
        "\n",
        "# Plotting the loss history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_loss_history, label='Train Loss', color='blue')\n",
        "plt.plot(val_loss_history, label='Validation Loss', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Extracting accuracy history from the training\n",
        "train_accuracy_history = history.history['accuracy']\n",
        "val_accuracy_history = history.history['val_accuracy']\n",
        "\n",
        "# Plotting the accuracy history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_accuracy_history, label='Train Accuracy', color='blue')\n",
        "plt.plot(val_accuracy_history, label='Validation Accuracy', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5MptzbZE88-"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94CSwoP_zAx0"
      },
      "source": [
        "### Sequential hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x70DkT-W7ENM"
      },
      "outputs": [],
      "source": [
        "# Sanity check to confirm X_train and y_train have equal number of samples\n",
        "print(f\"X_train has \", len(X_train), \"samples\")\n",
        "print(f\"y_train has \", len(y_train), \"samples\")\n",
        "if ( len(X_train) != len(y_train) ):\n",
        "  raise ValueError (\"X_train and y_train are different lengths, please investigate!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw_AJcC70mlg"
      },
      "outputs": [],
      "source": [
        "# perform Sequential hyperparameter optimization\n",
        "\n",
        "# Define a function to create a model\n",
        "def create_model(units=64, dropout=0.5):\n",
        "    model = Sequential([\n",
        "        Dense(units, activation='relu', input_shape=(input_shape,)),\n",
        "        Dropout(dropout),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define input shape based on the features in X_train\n",
        "input_shape = X_train.shape[1]\n",
        "\n",
        "# Create a wrapper class around the Keras model\n",
        "class KerasClassifierWrapper:\n",
        "    def __init__(self, units=64, dropout=0.5, epochs=10, batch_size=32, verbose=0):\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = create_model(units=self.units, dropout=self.dropout)\n",
        "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.model.predict(X) > 0.5).astype(int)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            'units': self.units,\n",
        "            'dropout': self.dropout,\n",
        "            'epochs': self.epochs,\n",
        "            'batch_size': self.batch_size,\n",
        "            'verbose': self.verbose\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for param, value in params.items():\n",
        "            setattr(self, param, value)\n",
        "        return self\n",
        "\n",
        "# Create an instance of the wrapper class\n",
        "model = KerasClassifierWrapper()\n",
        "\n",
        "# Define the hyperparameters grid to search\n",
        "param_grid = {\n",
        "    'units': [32],      #also tried 64,128\n",
        "    'dropout': [0.3],   #also tried 0.5, 0.7\n",
        "    'activation': ['tanh'] # relu almost as good as tanh ,also tried sigmoid and linear, but accuracy was lower\n",
        "}\n",
        "#param_grid = {           #smaller faster version for debugging\n",
        "#    'units': [32],\n",
        "#    'dropout': [0.3]\n",
        "#}\n",
        "\n",
        "# Create GridSearchCV instance\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_count, scoring=make_scorer(accuracy_score), verbose=2)\n",
        "\n",
        "# Perform grid search\n",
        "print(f\"--------------------------------------------------\")\n",
        "print(f\"Performing GridSearchCV to find optimal parameters\")\n",
        "print(f\"--------------------------------------------------\")\n",
        "grid_search_result = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and results\n",
        "print(\"Best Parameters:\", grid_search_result.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search_result.best_score_)\n",
        "print('\\n')\n",
        "\n",
        "# Evaluate the best model on training data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the best model on training data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "best_model = grid_search_result.best_estimator_\n",
        "train_loss, train_accuracy = best_model.model.evaluate(X_train, y_train)\n",
        "print(\"Train Loss:\", train_loss)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "# Evaluate the best model on test data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the best model on test data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "best_model = grid_search_result.best_estimator_\n",
        "test_loss, test_accuracy = best_model.model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "test_accuracy_sequential_optimized  = test_accuracy\n",
        "test_loss_sequential_optimized      = test_loss\n",
        "train_accuracy_sequential_optimized = train_accuracy\n",
        "train_loss_sequential_optimized     = train_loss\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_sequential_optimized      = Accuracy\n",
        "sensitivity_sequential_optimized   = Sensitivity\n",
        "specificity_sequential_optimized   = Specificity\n",
        "geometricmean_sequential_optimized = GeometricMean\n",
        "precision_sequential_optimized     = Precision\n",
        "recall_sequential_optimized        = Recall\n",
        "f1_sequential_optimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDAfFpycFLKC"
      },
      "outputs": [],
      "source": [
        "# Extracting loss history from the training\n",
        "train_loss_history = history.history['loss']\n",
        "val_loss_history = history.history['val_loss']\n",
        "\n",
        "# Plotting the loss history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_loss_history, label='Train Loss', color='blue')\n",
        "plt.plot(val_loss_history, label='Validation Loss', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Extracting accuracy history from the training\n",
        "train_accuracy_history = history.history['accuracy']\n",
        "val_accuracy_history = history.history['val_accuracy']\n",
        "\n",
        "# Plotting the accuracy history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_accuracy_history, label='Train Accuracy', color='blue')\n",
        "plt.plot(val_accuracy_history, label='Validation Accuracy', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISQCV20uyOxr"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n498Av_bqlRU"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQbRmQpV3X54"
      },
      "outputs": [],
      "source": [
        "# It’s important to note that LSTM models can be computationally expensive to train.\n",
        "# Depending on the size of your data and complexity of your model, training may take a significant amount of time.\n",
        "\n",
        "# NOTE: training the model with model.fit()  is ~10x faster when using a GPU!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOrE6RI17c5A"
      },
      "outputs": [],
      "source": [
        "# LSTM\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Define input shape based on the features in X_train\n",
        "input_shape = X_train.shape[1]\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    LSTM(64, activation='relu', input_shape=(input_shape, 1), return_sequences=True),\n",
        "    Dropout(0.5),  # Optional dropout layer for regularization\n",
        "    LSTM(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Compiling model\")\n",
        "print(f\"-----------------------------------------\")\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Model Summary\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(model.summary())\n",
        "\n",
        "# Train the model\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Training the model\")\n",
        "print(f\"-----------------------------------------\")\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on training data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the model on training data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
        "print(\"Training Loss:\", train_loss)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "# Now that we have trained our LSTM model, it’s time to evaluate its performance.\n",
        "# In TensorFlow, we can do this by using the `evaluate()` method of the model object.\n",
        "#\n",
        "# First, we need to load the test data and preprocess it in the same way as we did for the training data.\n",
        "# Once we have preprocessed the test data, we can evaluate the model using the `evaluate()` method.\n",
        "# This method takes two arguments: the test data (X_test) and its corresponding labels (y_test).\n",
        "# Evaluate the model on test data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the model on test data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(f\"\\n\")\n",
        "#\n",
        "# The `evaluate()` method returns two values: the loss and accuracy of the model on the test data.\n",
        "# The loss is a measure of how well the model is able to predict the correct output, while the accuracy is a measure of how often the model is correct.\n",
        "#\n",
        "# It’s important to note that we should only use the test data for evaluation purposes and not for training.\n",
        "# Using the same data for both training and evaluation can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data.\n",
        "#\n",
        "# In addition to evaluating the overall performance of our model, we can also look at individual predictions using the `predict()` method.\n",
        "# This method takes a single input example and returns its predicted output.\n",
        "#\n",
        "## Make a prediction on a single input example\n",
        "#example = ...\n",
        "#prediction = model.predict(preprocess_data(example))\n",
        "#\n",
        "# By examining individual predictions, we can gain insights into how our model is making decisions and identify areas where it may be making errors.\n",
        "# This can help us improve our model and make it more accurate for future predictions.\n",
        "\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "test_accuracy_lstm_unoptimized  = test_accuracy\n",
        "test_loss_lstm_unoptimized      = test_loss\n",
        "train_accuracy_lstm_unoptimized = train_accuracy\n",
        "train_loss_lstm_unoptimized     = train_loss\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_lstm_unoptimized      = Accuracy\n",
        "sensitivity_lstm_unoptimized   = Sensitivity\n",
        "specificity_lstm_unoptimized   = Specificity\n",
        "geometricmean_lstm_unoptimized = GeometricMean\n",
        "precision_lstm_unoptimized     = Precision\n",
        "recall_lstm_unoptimized        = Recall\n",
        "f1_lstm_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNY7zO0OE43L"
      },
      "outputs": [],
      "source": [
        "# During training, we can monitor the loss and visualize it using a graph.\n",
        "# This can help us determine if our model is overfitting or underfitting.\n",
        "\n",
        "# Extracting loss history from the training\n",
        "train_loss_history = history.history['loss']\n",
        "val_loss_history = history.history['val_loss']\n",
        "\n",
        "# Plotting the loss history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_loss_history, label='Train Loss', color='blue')\n",
        "plt.plot(val_loss_history, label='Validation Loss', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Extracting accuracy history from the training\n",
        "train_accuracy_history = history.history['accuracy']\n",
        "val_accuracy_history = history.history['val_accuracy']\n",
        "\n",
        "# Plotting the accuracy history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_accuracy_history, label='Train Accuracy', color='blue')\n",
        "plt.plot(val_accuracy_history, label='Validation Accuracy', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXboS9MhGYDh"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STBh32-tGLEo"
      },
      "source": [
        "### LSTM hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrypcDrF7dAh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# perform LSTM hyperparameter optimization  (without GPU, takes approx 60 minutes to run with units=32,64,128 dropout=0.3,0.5,0.7)\n",
        "# This method is different than Sequential optimization, maybe use the next cell instead for consistency\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define a function to create a model\n",
        "def create_model(units=64, dropout=0.5):\n",
        "    model = Sequential([\n",
        "        LSTM(units, activation='relu', input_shape=(X_train.shape[1], 1), return_sequences=True),\n",
        "        Dropout(dropout),\n",
        "        LSTM(units//2, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define hyperparameters to search\n",
        "#units_list = [32, 64, 128]\n",
        "#dropout_list = [0.3, 0.5, 0.7]\n",
        "units_list = [32]              #use smaller list of parameters to speed up debugging phase\n",
        "dropout_list = [0.3]               #use smaller list of parameters to speed up debugging phase\n",
        "\n",
        "\n",
        "# initialize variables\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "# Loop through all combinations of hyperparameters\n",
        "print(f\"\\nLooping through all combinations of hyperparameters\")\n",
        "for units in units_list:\n",
        "    for dropout in dropout_list:\n",
        "        print(f\"Evaluating model with units={units}, dropout={dropout}\")\n",
        "\n",
        "        # Create and compile the model\n",
        "        model = create_model(units=units, dropout=dropout)\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
        "\n",
        "        # Evaluate the model on validation data\n",
        "        val_loss, val_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "        print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "        # Update best accuracy and parameters if necessary\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            best_params = {'units': units, 'dropout': dropout}\n",
        "\n",
        "# Train the final model with the best parameters\n",
        "print(f\"\\nBest parameters: {best_params}\")\n",
        "print(f\"Training the final model with the best parameters...\")\n",
        "model = create_model(**best_params)\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
        "\n",
        "\n",
        "# Evaluate the best model on training data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the best model on training data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
        "print(\"Training Loss:\", train_loss)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "\n",
        "# Evaluate the best model on test data\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"\\nEvaluating the best model on test data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "test_accuracy_lstm_optimized  = test_accuracy\n",
        "test_loss_lstm_optimized      = test_loss\n",
        "train_accuracy_lstm_optimized = train_accuracy\n",
        "train_loss_lstm_optimized     = train_loss\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_lstm_optimized      = Accuracy\n",
        "sensitivity_lstm_optimized   = Sensitivity\n",
        "specificity_lstm_optimized   = Specificity\n",
        "geometricmean_lstm_optimized = GeometricMean\n",
        "precision_lstm_optimized     = Precision\n",
        "recall_lstm_optimized        = Recall\n",
        "f1_lstm_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdKzfwO4Jn_N"
      },
      "outputs": [],
      "source": [
        "# # NOTE: This cell took 6 hours to run, and has the same accuracy as the previous cell, which only took 5 minutes to run!\n",
        "\n",
        "# # another method of hyperparameter optimization for LSTM\n",
        "# # this one uses the same format as Sequential\n",
        "\n",
        "# # Define input shape based on the features in X_train\n",
        "# input_shape = (X_train.shape[1], 1)  # Assuming X_train is 2D\n",
        "\n",
        "# # Define a function to create a model\n",
        "# def create_model(units=64, dropout=0.5):\n",
        "#     model = Sequential([\n",
        "#         LSTM(units, input_shape=input_shape),\n",
        "#         Dropout(dropout),\n",
        "#         Dense(32, activation='relu'),\n",
        "#         Dense(1, activation='sigmoid')\n",
        "#     ])\n",
        "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # Create a wrapper class around the Keras model\n",
        "# class KerasLSTMWrapper:\n",
        "#     def __init__(self, units=64, dropout=0.5, epochs=10, batch_size=32, verbose=0):\n",
        "#         self.units = units\n",
        "#         self.dropout = dropout\n",
        "#         self.epochs = epochs\n",
        "#         self.batch_size = batch_size\n",
        "#         self.verbose = verbose\n",
        "#         self.model = None\n",
        "\n",
        "#     def fit(self, X, y):\n",
        "#         self.model = create_model(units=self.units, dropout=self.dropout)\n",
        "#         self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
        "\n",
        "#     def predict(self, X):\n",
        "#         return (self.model.predict(X) > 0.5).astype(int)\n",
        "\n",
        "#     def get_params(self, deep=True):\n",
        "#         return {\n",
        "#             'units': self.units,\n",
        "#             'dropout': self.dropout,\n",
        "#             'epochs': self.epochs,\n",
        "#             'batch_size': self.batch_size,\n",
        "#             'verbose': self.verbose\n",
        "#         }\n",
        "\n",
        "#     def set_params(self, **params):\n",
        "#         for param, value in params.items():\n",
        "#             setattr(self, param, value)\n",
        "#         return self\n",
        "\n",
        "# # Create an instance of the wrapper class\n",
        "# model = KerasLSTMWrapper()\n",
        "\n",
        "# # Define the hyperparameters grid to search\n",
        "# param_grid = {\n",
        "#     'units': [32, 64, 128],\n",
        "#     'dropout': [0.3, 0.5, 0.7]\n",
        "# }\n",
        "# #param_grid = {\n",
        "# #    'units': [32],\n",
        "# #    'dropout': [0.3]\n",
        "# #}\n",
        "\n",
        "# # Create GridSearchCV instance\n",
        "# # assumes from sklearn.metrics import make_scorer, accuracy_score\n",
        "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_count, scoring=make_scorer(accuracy_score), verbose=2)\n",
        "\n",
        "# # Perform grid search\n",
        "# grid_search_result = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# # Print best parameters and results\n",
        "# print(\"Best Parameters:\", grid_search_result.best_params_)\n",
        "# print(\"Best Accuracy:\", grid_search_result.best_score_)\n",
        "\n",
        "# # Evaluate the best model on training data\n",
        "# print(f\"\\n\")\n",
        "# print(f\"------------------------------------------\")\n",
        "# print(f\"Evaluating the best model on training data\")\n",
        "# print(f\"------------------------------------------\")\n",
        "# best_model = grid_search_result.best_estimator_\n",
        "# train_loss, train_accuracy = best_model.model.evaluate(X_train, y_train)\n",
        "# print(\"Train Loss:\", train_loss)\n",
        "# print(\"Train Accuracy:\", train_accuracy)\n",
        "# print(f\"\\n\")\n",
        "\n",
        "# # Evaluate the best model on test data\n",
        "# print(f\"\\n\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# print(f\"Evaluating the best model on test data\")\n",
        "# print(f\"-----------------------------------------\")\n",
        "# test_loss, test_accuracy = best_model.model.evaluate(X_test, y_test)\n",
        "# print(\"Test Loss:\", test_loss)\n",
        "# print(\"Test Accuracy:\", test_accuracy)\n",
        "# print(f\"\\n\")\n",
        "\n",
        "# # save results calculated for this model for later comparison to other models\n",
        "# test_accuracy_lstm_optimized  = test_accuracy\n",
        "# test_loss_lstm_optimized      = test_loss\n",
        "# train_accuracy_lstm_optimized = train_accuracy\n",
        "# train_loss_lstm_optimized     = train_loss\n",
        "\n",
        "\n",
        "# # call previously defined function to create confusion matrix\n",
        "# cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# # save results calculated for this model for later comparison to other models\n",
        "# accuracy_lstm_optimized      = Accuracy\n",
        "# sensitivity_lstm_optimized   = Sensitivity\n",
        "# specificity_lstm_optimized   = Specificity\n",
        "# geometricmean_lstm_optimized = GeometricMean\n",
        "# precision_lstm_optimized     = Precision\n",
        "# recall_lstm_optimized        = Recall\n",
        "# f1_lstm_unoptimized            = F1\n",
        "\n",
        "# # show a running total of elapsed time for the entire notebook\n",
        "# show_elapsed_time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZDSrhc7Eh3O"
      },
      "outputs": [],
      "source": [
        "# Extracting loss history from the training\n",
        "train_loss_history = history.history['loss']\n",
        "val_loss_history = history.history['val_loss']\n",
        "\n",
        "# Plotting the loss history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_loss_history, label='Train Loss', color='blue')\n",
        "plt.plot(val_loss_history, label='Validation Loss', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Extracting accuracy history from the training\n",
        "train_accuracy_history = history.history['accuracy']\n",
        "val_accuracy_history = history.history['val_accuracy']\n",
        "\n",
        "# Plotting the accuracy history\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_accuracy_history, label='Train Accuracy', color='blue')\n",
        "plt.plot(val_accuracy_history, label='Validation Accuracy', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8PHlZkHIxO9"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH3LYBhB52Tq"
      },
      "source": [
        "## reshape X_train, X_test to include time steps for SimpleRNN and GRU\n",
        "\n",
        "The following model expects sequential (ie time-series) data, so the dataset will need  \"time steps\"  for the SimpleRNN and Gated Recurrent Unit (GRU) models? (which also reshapes X_train,X_test).\n",
        "\n",
        "If the data does not include time steps, you will get an error about the shape being incorrect.\n",
        "\n",
        "The error message indicates that the input to the GRU layer has an incorrect shape. The GRU layer expects input data to have three dimensions: (batch size, time steps, features). In this case, the input data only has two dimensions: (batch size, features).\n",
        "\n",
        "To fix the issue, reshape the input data to have three dimensions. This can be done using the reshape() method.\n",
        "\n",
        "After reshaping the input data, the model can be trained and evaluated successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auX4hG3izG11"
      },
      "outputs": [],
      "source": [
        "# reshape X_train to add time steps (expected by this model)\n",
        "\n",
        "# Assuming X_train has shape (samples, features)\n",
        "# Define the number of time steps\n",
        "time_steps = 1  # Adjust this value based on your data and problem\n",
        "\n",
        "# Reshape X_train to include time steps\n",
        "X_train_with_time_steps = np.zeros((X_train.shape[0] - time_steps + 1, time_steps, X_train.shape[1]))\n",
        "for i in range(len(X_train) - time_steps + 1):\n",
        "    X_train_with_time_steps[i] = X_train[i:i+time_steps]\n",
        "\n",
        "# Now X_train_with_time_steps has shape (samples, time_steps, features)\n",
        "\n",
        "\n",
        "# reshape X_test to add time steps (expected by this model)\n",
        "\n",
        "# Assuming X_test has shape (samples, features)\n",
        "# Define the number of time steps\n",
        "time_steps = 1  # Adjust this value based on your data and problem\n",
        "\n",
        "# Reshape X_test to include time steps\n",
        "X_test_with_time_steps = np.zeros((X_test.shape[0] - time_steps + 1, time_steps, X_test.shape[1]))\n",
        "for i in range(len(X_test) - time_steps + 1):\n",
        "    X_test_with_time_steps[i] = X_test[i:i+time_steps]\n",
        "\n",
        "# Now X_test_with_time_steps has shape (samples, time_steps, features)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDBqcaSo1Yyh"
      },
      "source": [
        "## SimpleRNN\n",
        "### (needed reshaping to add time steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O270ZEpQzGxO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN\n",
        "\n",
        "# Define input shape based on the features in X_train_with_time_steps\n",
        "input_shape = X_train_with_time_steps.shape[1:]\n",
        "\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    SimpleRNN(units=64, input_shape=input_shape),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Print model summary\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Model Summary\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "# Train the model\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Training the model\")\n",
        "print(f\"-----------------------------------------\")\n",
        "history = model.fit(X_train_with_time_steps, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "\n",
        "# Evaluate the model on training data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the model on training data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "train_loss, train_accuracy = model.evaluate(X_train_with_time_steps, y_train)\n",
        "print(\"Training Loss:\", train_loss)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "\n",
        "# Evaluate the model on test data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the model on test data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "test_loss, test_accuracy = model.evaluate(X_test_with_time_steps, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "test_accuracy_simplernn_unoptimized  = test_accuracy\n",
        "test_loss_simplernn_unoptimized      = test_loss\n",
        "train_accuracy_simplernn_unoptimized = train_accuracy\n",
        "train_loss_simplernn_unoptimized     = train_loss\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_simplernn_unoptimized      = Accuracy\n",
        "sensitivity_simplernn_unoptimized   = Sensitivity\n",
        "specificity_simplernn_unoptimized   = Specificity\n",
        "geometricmean_simplernn_unoptimized = GeometricMean\n",
        "precision_simplernn_unoptimized     = Precision\n",
        "recall_simplernn_unoptimized        = Recall\n",
        "f1_simplernn_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW5Of-nMz7Xt"
      },
      "source": [
        "### SimpleRNN hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLO9djUW6o2A"
      },
      "outputs": [],
      "source": [
        "# Sanity check to confirm X_train and y_train have equal number of samples\n",
        "print(f\"X_train_with_time_steps has \", len(X_train_with_time_steps), \"samples\")\n",
        "print(f\"y_train                 has \", len(y_train),                 \"samples\")\n",
        "if ( len(X_train_with_time_steps) != len(y_train) ):\n",
        "  raise ValueError (\"X_train_with_time_steps and y_train are different lengths, please investigate!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4mf9zpLK_nl"
      },
      "outputs": [],
      "source": [
        "# SimpleRNN hyperparameter optimization\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Dropout\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "\n",
        "# Define input shape based on the features in X_train\n",
        "input_shape = (X_train_with_time_steps.shape[1], X_train_with_time_steps.shape[2])  # Assuming X_train is 2D\n",
        "\n",
        "# Define a function to create a model\n",
        "def create_model(units=64, dropout=0.5):\n",
        "    model = Sequential([\n",
        "        SimpleRNN(units, input_shape=input_shape),\n",
        "        Dropout(dropout),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create a wrapper class around the Keras model\n",
        "class KerasSimpleRNNWrapper:\n",
        "    def __init__(self, units=64, dropout=0.5, epochs=10, batch_size=32, verbose=0):\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = create_model(units=self.units, dropout=self.dropout)\n",
        "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.model.predict(X) > 0.5).astype(int)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            'units': self.units,\n",
        "            'dropout': self.dropout,\n",
        "            'epochs': self.epochs,\n",
        "            'batch_size': self.batch_size,\n",
        "            'verbose': self.verbose\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for param, value in params.items():\n",
        "            setattr(self, param, value)\n",
        "        return self\n",
        "\n",
        "# Create an instance of the wrapper class\n",
        "model = KerasSimpleRNNWrapper()\n",
        "\n",
        "# Define the hyperparameters grid to search\n",
        "#param_grid = {\n",
        "#    'units': [32, 64, 128],\n",
        "#    'dropout': [0.3, 0.5, 0.7]\n",
        "#}\n",
        "param_grid = {        #smaller faster version for testing\n",
        "    'units': [32],\n",
        "    'dropout': [0.3]\n",
        "}\n",
        "\n",
        "\n",
        "# Create GridSearchCV instance\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_count, scoring=make_scorer(accuracy_score), verbose=2)\n",
        "\n",
        "# Perform grid search\n",
        "print(f\"\\n\")\n",
        "print(f\"------------------------------------------\")\n",
        "print(f\"Performing GridSearchCV\")\n",
        "print(f\"------------------------------------------\")\n",
        "grid_search_result = grid_search.fit(X_train_with_time_steps, y_train)\n",
        "\n",
        "# Print best parameters and results\n",
        "print(\"Best Parameters:\", grid_search_result.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search_result.best_score_)\n",
        "\n",
        "# Evaluate the best model on training data\n",
        "print(f\"\\n\")\n",
        "print(f\"------------------------------------------\")\n",
        "print(f\"Evaluating the best model on training data\")\n",
        "print(f\"------------------------------------------\")\n",
        "best_model = grid_search_result.best_estimator_\n",
        "train_loss, train_accuracy = best_model.model.evaluate(X_train_with_time_steps, y_train)\n",
        "print(\"Train Loss:\", train_loss)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "# Evaluate the best model on test data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the best model on test data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "test_loss, test_accuracy = best_model.model.evaluate(X_test_with_time_steps, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "test_accuracy_simplernn_optimized  = test_accuracy\n",
        "test_loss_simplernn_optimized      = test_loss\n",
        "train_accuracy_simplernn_optimized = train_accuracy\n",
        "train_loss_simplernn_optimized     = train_loss\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_simplernn_optimized      = Accuracy\n",
        "sensitivity_simplernn_optimized   = Sensitivity\n",
        "specificity_simplernn_optimized   = Specificity\n",
        "geometricmean_simplernn_optimized = GeometricMean\n",
        "precision_simplernn_optimized     = Precision\n",
        "recall_simplernn_optimized        = Recall\n",
        "f1_simplernn_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SimpleRNN + LSTM"
      ],
      "metadata": {
        "id": "CkjPbEEYEyeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM\n",
        "\n",
        "# Define input shape based on the features in X_train_with_time_steps\n",
        "input_shape = X_train_with_time_steps.shape[1:]\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    SimpleRNN(64, input_shape=input_shape, return_sequences=True),  # SimpleRNN layer with 64 units\n",
        "    LSTM(32),  # LSTM layer with 32 units\n",
        "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Print model summary\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Model Summary\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "# Train the model\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Training the model\")\n",
        "print(f\"-----------------------------------------\")\n",
        "history = model.fit(X_train_with_time_steps, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "\n",
        "# Evaluate the model on training data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the model on training data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "train_loss, train_accuracy = model.evaluate(X_train_with_time_steps, y_train)\n",
        "print(\"Training Loss:\", train_loss)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "\n",
        "# Evaluate the model on test data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the model on test data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "test_loss, test_accuracy = model.evaluate(X_test_with_time_steps, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "test_accuracy_simplernn_lstm_unoptimized  = test_accuracy\n",
        "test_loss_simplernn_lstm_unoptimized      = test_loss\n",
        "train_accuracy_simplernn_lstm_unoptimized = train_accuracy\n",
        "train_loss_simplernn_lstm_unoptimized     = train_loss\n",
        "\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_simplernn_lstm_unoptimized      = Accuracy\n",
        "sensitivity_simplernn_lstm_unoptimized   = Sensitivity\n",
        "specificity_simplernn_lstm_unoptimized   = Specificity\n",
        "geometricmean_simplernn_lstm_unoptimized = GeometricMean\n",
        "precision_simplernn_lstm_unoptimized     = Precision\n",
        "recall_simplernn_lstm_unoptimized        = Recall\n",
        "f1_simplernn_lstm_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ],
      "metadata": {
        "id": "kD5WhHSgE03X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SimpleRNN + LSTM hyperparameter optimization"
      ],
      "metadata": {
        "id": "GUj_U5m2G8Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SimpleRNN + LSTM hyperparameter optimization\n",
        "\n",
        "#import numpy as np\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "#from tensorflow.keras.models import Sequential\n",
        "#from tensorflow.keras.layers import Dense, SimpleRNN, Dropout\n",
        "#from sklearn.metrics import make_scorer, accuracy_score\n",
        "\n",
        "# Define input shape based on the features in X_train\n",
        "input_shape = (X_train_with_time_steps.shape[1], X_train_with_time_steps.shape[2])  # Assuming X_train is 2D\n",
        "input_shape = X_train_with_time_steps.shape[1:]\n",
        "\n",
        "\n",
        "# Define a function to create a model\n",
        "def create_model(units=64, dropout=0.5):\n",
        "    model = Sequential([\n",
        "        SimpleRNN(units, input_shape=input_shape, return_sequences=True),\n",
        "        LSTM(units, dropout=dropout),\n",
        "        Dropout(dropout),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a wrapper class around the Keras model\n",
        "class KerasSimpleRNNWrapper:\n",
        "    def __init__(self, units=64, dropout=0.5, epochs=10, batch_size=32, verbose=0):\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = create_model(units=self.units, dropout=self.dropout)\n",
        "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.model.predict(X) > 0.5).astype(int)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            'units': self.units,\n",
        "            'dropout': self.dropout,\n",
        "            'epochs': self.epochs,\n",
        "            'batch_size': self.batch_size,\n",
        "            'verbose': self.verbose\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for param, value in params.items():\n",
        "            setattr(self, param, value)\n",
        "        return self\n",
        "\n",
        "# Create an instance of the wrapper class\n",
        "model = KerasSimpleRNNWrapper()\n",
        "\n",
        "\n",
        "# Define the hyperparameters grid to search\n",
        "#param_grid = {\n",
        "#    'units': [32, 64, 128],\n",
        "#    'dropout': [0.3, 0.5, 0.7]\n",
        "#}\n",
        "param_grid = {        #smaller faster version for testing\n",
        "    'units': [32],\n",
        "    'dropout': [0.3]\n",
        "}\n",
        "\n",
        "\n",
        "# Create GridSearchCV instance\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_count, scoring=make_scorer(accuracy_score), verbose=2)\n",
        "\n",
        "# Perform grid search\n",
        "print(f\"\\n\")\n",
        "print(f\"------------------------------------------\")\n",
        "print(f\"Performing GridSearchCV\")\n",
        "print(f\"------------------------------------------\")\n",
        "grid_search_result = grid_search.fit(X_train_with_time_steps, y_train)\n",
        "\n",
        "\n",
        "# Print best parameters and results\n",
        "print(\"Best Parameters:\", grid_search_result.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search_result.best_score_)\n",
        "\n",
        "# Evaluate the best model on training data\n",
        "print(f\"\\n\")\n",
        "print(f\"------------------------------------------\")\n",
        "print(f\"Evaluating the best model on training data\")\n",
        "print(f\"------------------------------------------\")\n",
        "best_model = grid_search_result.best_estimator_\n",
        "train_loss, train_accuracy = best_model.model.evaluate(X_train_with_time_steps, y_train)\n",
        "print(\"Train Loss:\", train_loss)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "# Evaluate the best model on test data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the best model on test data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "test_loss, test_accuracy = best_model.model.evaluate(X_test_with_time_steps, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "test_accuracy_simplernn_lstm_optimized  = test_accuracy\n",
        "test_loss_simplernn_lstm_optimized      = test_loss\n",
        "train_accuracy_simplernn_lstm_optimized = train_accuracy\n",
        "train_loss_simplernn_lstm_optimized     = train_loss\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_simplernn_lstm_optimized      = Accuracy\n",
        "sensitivity_simplernn_lstm_optimized   = Sensitivity\n",
        "specificity_simplernn_lstm_optimized   = Specificity\n",
        "geometricmean_simplernn_lstm_optimized = GeometricMean\n",
        "precision_simplernn_lstm_optimized     = Precision\n",
        "recall_simplernn_lstm_optimized        = Recall\n",
        "f1_simplernn_lstm_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ],
      "metadata": {
        "id": "x98sxPUVHAef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cvXHP-I1pjZ"
      },
      "source": [
        "## Gated Recurrent Unit (GRU)\n",
        "### (needed reshaping to add time steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsgiH50S2wA3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dense, GRU\n",
        "\n",
        "\n",
        "\n",
        "# Define input shape based on the features in X_train\n",
        "input_shape = X_train_with_time_steps.shape[1:]\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    GRU(units=64, input_shape=input_shape),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Model Summary\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(model.summary())\n",
        "\n",
        "# Train the model\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Training the model\")\n",
        "print(f\"-----------------------------------------\")\n",
        "history = model.fit(X_train_with_time_steps, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on training data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the model on training data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "train_loss, train_accuracy = model.evaluate(X_train_with_time_steps, y_train)\n",
        "print(\"Training Loss:\", train_loss)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "\n",
        "# Evaluate the model on test data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the model on test data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "test_loss, test_accuracy = model.evaluate(X_test_with_time_steps, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "test_accuracy_gru_unoptimized  = test_accuracy\n",
        "test_loss_gru_unoptimized      = test_loss\n",
        "train_accuracy_gru_unoptimized = train_accuracy\n",
        "train_loss_gru_unoptimized     = train_loss\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_gru_unoptimized      = Accuracy\n",
        "sensitivity_gru_unoptimized   = Sensitivity\n",
        "specificity_gru_unoptimized   = Specificity\n",
        "geometricmean_gru_unoptimized = GeometricMean\n",
        "precision_gru_unoptimized     = Precision\n",
        "recall_gru_unoptimized        = Recall\n",
        "f1_gru_unoptimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL0p8Bc0_hdA"
      },
      "source": [
        "### GRU hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkO4kJxJC4J5"
      },
      "outputs": [],
      "source": [
        "# Sanity check to confirm X_train and y_train have equal number of samples\n",
        "print(f\"X_train_with_time_steps has \", len(X_train_with_time_steps), \"samples\")\n",
        "print(f\"y_train                 has \", len(y_train),                 \"samples\")\n",
        "if ( len(X_train_with_time_steps) != len(y_train) ):\n",
        "  raise ValueError (\"X_train_with_time_steps and y_train are different lengths, please investigate!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxSkB7Pw_klo"
      },
      "outputs": [],
      "source": [
        "# NOTE: Crashed on this steap after using all available RAM!  2025-04-19\n",
        "\n",
        "# GRU hyperparameter optimization\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Dropout\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "\n",
        "# Define input shape based on the features in X_train\n",
        "input_shape = (X_train_with_time_steps.shape[1], X_train_with_time_steps.shape[2])  # Assuming X_train is 3D\n",
        "\n",
        "# Define a function to create a model\n",
        "def create_model(units=64, dropout=0.5):\n",
        "    model = Sequential([\n",
        "        GRU(units, input_shape=input_shape),\n",
        "        Dropout(dropout),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create a wrapper class around the Keras model\n",
        "class KerasGRUWrapper:\n",
        "    def __init__(self, units=64, dropout=0.5, epochs=10, batch_size=32, verbose=0):\n",
        "        self.units = units\n",
        "        self.dropout = dropout\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = create_model(units=self.units, dropout=self.dropout)\n",
        "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.model.predict(X) > 0.5).astype(int)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            'units': self.units,\n",
        "            'dropout': self.dropout,\n",
        "            'epochs': self.epochs,\n",
        "            'batch_size': self.batch_size,\n",
        "            'verbose': self.verbose\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for param, value in params.items():\n",
        "            setattr(self, param, value)\n",
        "        return self\n",
        "\n",
        "# Create an instance of the wrapper class\n",
        "model = KerasGRUWrapper()\n",
        "\n",
        "# Define the hyperparameters grid to search\n",
        "#param_grid = {\n",
        "#    'units': [32, 64, 128],\n",
        "#    'dropout': [0.3, 0.5, 0.7]\n",
        "#}\n",
        "param_grid = {          #smaller faster version for testing\n",
        "    'units': [32],\n",
        "    'dropout': [0.3]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV instance\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv_count, scoring=make_scorer(accuracy_score), verbose=2)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search_result = grid_search.fit(X_train_with_time_steps, y_train)\n",
        "\n",
        "# Print best parameters and results\n",
        "print(\"Best Parameters:\", grid_search_result.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search_result.best_score_)\n",
        "\n",
        "\n",
        "# Evaluate the best model on training data\n",
        "print(f\"\\n\")\n",
        "print(f\"------------------------------------------\")\n",
        "print(f\"Evaluating the best model on training data\")\n",
        "print(f\"------------------------------------------\")\n",
        "best_model = grid_search_result.best_estimator_\n",
        "train_loss, train_accuracy = best_model.model.evaluate(X_train_with_time_steps, y_train)\n",
        "print(\"Train Loss:\", train_loss)\n",
        "print(\"Train Accuracy:\", train_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "# Evaluate the best model on test data\n",
        "print(f\"\\n\")\n",
        "print(f\"-----------------------------------------\")\n",
        "print(f\"Evaluating the best model on test data\")\n",
        "print(f\"-----------------------------------------\")\n",
        "test_loss, test_accuracy = best_model.model.evaluate(X_test_with_time_steps, y_test)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(f\"\\n\")\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "test_accuracy_gru_optimized  = test_accuracy\n",
        "test_loss_gru_optimized      = test_loss\n",
        "train_accuracy_gru_optimized = train_accuracy\n",
        "train_loss_gru_optimized     = train_loss\n",
        "\n",
        "# call previously defined function to create confusion matrix\n",
        "cm, Accuracy, Sensitivity, Specificity, GeometricMean, Precision, Recall, F1 = visualize_confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# save results calculated for this model for later comparison to other models\n",
        "accuracy_gru_optimized      = Accuracy\n",
        "sensitivity_gru_optimized   = Sensitivity\n",
        "specificity_gru_optimized   = Specificity\n",
        "geometricmean_gru_optimized = GeometricMean\n",
        "precision_gru_optimized     = Precision\n",
        "recall_gru_optimized        = Recall\n",
        "f1_gru_optimized            = F1\n",
        "\n",
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SMMheYSRsZP"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baeOv-v_xlH7"
      },
      "source": [
        "# Comparison of all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpaZ3DR-xn1d"
      },
      "outputs": [],
      "source": [
        "# from tabulate import tabulate\n",
        "\n",
        "# # round to 4 decimal places\n",
        "# accuracy_mlp_unoptimized              = round(accuracy_mlp_unoptimized,4)\n",
        "# accuracy_mlp_optimized                = round(accuracy_mlp_optimized,4)\n",
        "\n",
        "\n",
        "# train_accuracy_sequential_unoptimized = round(train_accuracy_sequential_unoptimized,4)\n",
        "# train_loss_sequential_unoptimized     = round(train_loss_sequential_unoptimized,4)\n",
        "# test_accuracy_sequential_unoptimized  = round(test_accuracy_sequential_unoptimized,4)\n",
        "# test_loss_sequential_unoptimized      = round(test_loss_sequential_unoptimized,4)\n",
        "# #\n",
        "# train_accuracy_sequential_optimized   = round(train_accuracy_sequential_optimized,4)\n",
        "# train_loss_sequential_optimized       = round(train_loss_sequential_optimized,4)\n",
        "# test_accuracy_sequential_optimized    = round(test_accuracy_sequential_optimized,4)\n",
        "# test_loss_sequential_optimized        = round(test_loss_sequential_optimized,4)\n",
        "\n",
        "# train_accuracy_lstm_unoptimized       = round(train_accuracy_lstm_unoptimized,4)\n",
        "# train_loss_lstm_unoptimized           = round(train_loss_lstm_unoptimized,4)\n",
        "# test_accuracy_lstm_unoptimized        = round(test_accuracy_lstm_unoptimized,4)\n",
        "# test_loss_lstm_unoptimized            = round(test_loss_lstm_unoptimized,4)\n",
        "# #\n",
        "# train_accuracy_lstm_optimized         = round(train_accuracy_lstm_optimized,4)\n",
        "# train_loss_lstm_optimized             = round(train_loss_lstm_optimized,4)\n",
        "# test_accuracy_lstm_optimized          = round(test_accuracy_lstm_optimized,4)\n",
        "# test_loss_lstm_optimized              = round(test_loss_lstm_optimized,4)\n",
        "\n",
        "# #train_accuracy_simplernn_unoptimized  = round(train_accuracy_simplernn_unoptimized,4)\n",
        "# #train_loss_simplernn_unoptimized      = round(train_loss_simplernn_unoptimized,4)\n",
        "# #test_accuracy_simplernn_unoptimized   = round(test_accuracy_simplernn_unoptimized,4)\n",
        "# #test_loss_simplernn_unoptimized       = round(test_loss_simplernn_unoptimized,4)\n",
        "# ##\n",
        "# #train_accuracy_simplernn_optimized    = round(train_accuracy_simplernn_optimized,4)\n",
        "# #train_loss_simplernn_optimized        = round(train_loss_simplernn_optimized,4)\n",
        "# #test_accuracy_simplernn_optimized     = round(test_accuracy_simplernn_optimized,4)\n",
        "# #test_loss_simplernn_optimized         = round(test_loss_simplernn_optimized,4)\n",
        "# #\n",
        "# #train_accuracy_gru_unoptimized        = round(train_accuracy_gru_unoptimized,4)\n",
        "# #train_loss_gru_unoptimized            = round(train_loss_gru_unoptimized,4)\n",
        "# #test_accuracy_gru_unoptimized         = round(test_accuracy_gru_unoptimized,4)\n",
        "# #test_loss_gru_unoptimized             = round(test_loss_gru_unoptimized,4)\n",
        "# #\n",
        "# #train_accuracy_gru_optimized          = round(train_accuracy_gru_optimized,4)\n",
        "# #train_loss_gru_optimized              = round(train_loss_gru_optimized,4)\n",
        "# #test_accuracy_gru_optimized           = round(test_accuracy_gru_optimized,4)\n",
        "# #test_loss_gru_optimized               = round(test_loss_gru_optimized,4)\n",
        "\n",
        "\n",
        "\n",
        "# # Create a list of lists to represent the table showing un-optimized values before hyperparameter optimization\n",
        "# table = [\n",
        "#     [\"Model\", \"Train Accuracy Un-optimized\", \"Train Loss Un-optimized\",  \"Test Accuracy Un-optimized\", \"Test Loss Un-optimized\"],\n",
        "#     [\"MLP\"       , \"N/A\"                                , \"N/A\"                            , accuracy_mlp_unoptimized          , \"N/A\"],\n",
        "#     [\"Sequential\", train_accuracy_sequential_unoptimized, train_loss_sequential_unoptimized, test_accuracy_sequential_unoptimized, test_loss_sequential_unoptimized],\n",
        "#     [\"LSTM\"      , train_accuracy_lstm_unoptimized      , train_loss_lstm_unoptimized      , test_accuracy_lstm_unoptimized      , test_loss_lstm_unoptimized]\n",
        "# ]\n",
        "# # Print the table with numbers formatted to 4 decimal places\n",
        "# print(tabulate(table, headers=\"firstrow\", floatfmt=\".4f\", tablefmt=\"fancy_grid\"))\n",
        "# print('\\n\\n')\n",
        "\n",
        "\n",
        "# # Create a list of lists to represent the table showing optimized values after hyperparameter optimization\n",
        "# table = [\n",
        "#     [\"Model\", \"Train Accuracy Optimized\", \"Train Loss Optimized\",  \"Test Accuracy Optimized\", \"Test Loss Optimized\"],\n",
        "#     [\"MLP\"       , \"N/A\"                              , \"N/A\"                          , accuracy_mlp_optimized          , \"N/A\"],\n",
        "#     [\"Sequential\", train_accuracy_sequential_optimized, train_loss_sequential_optimized, test_accuracy_sequential_optimized, test_loss_sequential_optimized],\n",
        "#     [\"LSTM\"      , train_accuracy_lstm_optimized      , train_loss_lstm_optimized      , test_accuracy_lstm_optimized      , test_loss_lstm_optimized]\n",
        "#  ]\n",
        "# # Print the table with numbers formatted to 4 decimal places\n",
        "# print(tabulate(table, headers=\"firstrow\", floatfmt=\".4f\", tablefmt=\"fancy_grid\"))\n",
        "# print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e81Nf1Nt3Zhi"
      },
      "outputs": [],
      "source": [
        "# this section compares the accuracy of different methods:\n",
        "\n",
        "print(f\"LR  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_lr_unoptimized*100:.2f}%\")\n",
        "print(f\"LR  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_lr_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"NB  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_nb_unoptimized*100:.2f}%\")\n",
        "print(f\"NB  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_nb_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"KNN accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_knn_unoptimized*100:.2f}%\")\n",
        "print(f\"KNN  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_knn_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"SVM accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_svm_unoptimized*100:.2f}%\")\n",
        "print(f\"SVM accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_svm_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"DT  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_dt_unoptimized*100:.2f}%\")\n",
        "print(f\"DT  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_dt_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"RF  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_rf_unoptimized*100:.2f}%\")\n",
        "print(f\"RF  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_rf_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"GB  accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_gb_unoptimized*100:.2f}%\")\n",
        "print(f\"GB  accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_gb_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"MLP accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_mlp_unoptimized*100:.2f}%\")\n",
        "print(f\"MLP accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_mlp_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"SEQ accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_sequential_unoptimized*100:.2f}%\")\n",
        "print(f\"SEQ accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_sequential_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"FNN+LSTM accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_lstm_unoptimized*100:.2f}%\")\n",
        "print(f\"FNN+LSTM accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_lstm_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"RNN accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_simplernn_unoptimized*100:.2f}%\")\n",
        "print(f\"RNN accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_simplernn_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "print(f\"RNN+LSTM accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_simplernn_lstm_unoptimized*100:.2f}%\")\n",
        "print(f\"RNN+LSTM accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_simplernn_lstm_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "\n",
        "print(f\"GRU accuracy on undersampled balanced data, before hyperparameter optimimization: {accuracy_gru_unoptimized*100:.2f}%\")\n",
        "print(f\"GRU accuracy on undersampled balanced data, after  hyperparameter optimimization: {accuracy_gru_optimized*100:.2f}%\")\n",
        "print('\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(f\"test_accuracy_mlp_unoptimized             {test_accuracy_mlp_unoptimized}\")\n",
        "#print(f\"test_accuracy_mlp_optimized               {test_accuracy_mlp_optimized}\")\n",
        "print(f\"test_accuracy_sequential_unoptimized       {test_accuracy_sequential_unoptimized}\")\n",
        "print(f\"test_accuracy_sequential_optimized         {test_accuracy_sequential_optimized}\")\n",
        "print(f\"test_accuracy_lstm_unoptimized             {test_accuracy_lstm_unoptimized}\")\n",
        "print(f\"test_accuracy_lstm_optimized               {test_accuracy_lstm_optimized}\")\n",
        "print(f\"test_accuracy_simplernn_unoptimized        {test_accuracy_simplernn_unoptimized}\")\n",
        "print(f\"test_accuracy_simplernn_optimized          {test_accuracy_simplernn_optimized}\")\n",
        "print(f\"test_accuracy_simplernn_lstm_optimized     {test_accuracy_simplernn_lstm_optimized}\")\n",
        "print(f\"test_accuracy_gru_unoptimized              {test_accuracy_gru_unoptimized}\")\n",
        "print(f\"test_accuracy_gru_optimized                {test_accuracy_gru_optimized}\")\n"
      ],
      "metadata": {
        "id": "Va-l6ba9MRK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar graph that shows the accuracy of the base classifiers and ensemble classifiers\n",
        "\n",
        "# Show the values that will be used in the graph\n",
        "print(f\"The following accuracy values will be used for visualization:\")\n",
        "print(f\"   GB       {accuracy_gb_optimized:.4f}\")\n",
        "print(f\"   DT       {accuracy_dt_optimized:.4f}\")\n",
        "print(f\"   RF       {accuracy_rf_optimized:.4f}\")\n",
        "print(f\"   MLP      {accuracy_mlp_optimized:.4f}\")\n",
        "print(f\"   FNN      {accuracy_sequential_optimized:.4f}\")\n",
        "print(f\"   LSTM     {accuracy_lstm_optimized:.4f}\")\n",
        "\n",
        "labels = [\"GB\", \"DT\", \"RF\", \"MLP\", \"FNN\", \"FNN-LSTM\", \"RNN\", \"RNN-LSTM\", \"GRU\"]\n",
        "values = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, accuracy_mlp_optimized*100, accuracy_sequential_optimized*100, accuracy_lstm_optimized*100, accuracy_simplernn_optimized*100, accuracy_simplernn_lstm_optimized*100, accuracy_gru_optimized*100]\n",
        "#values = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, accuracy_mlp_optimized*100, test_accuracy_sequential_optimized*100, test_accuracy_lstm_optimized*100, test_accuracy_simplernn_optimized*100, test_accuracy_simplernn_lstm_optimized*100,test_accuracy_gru_optimized*100]\n",
        "\n",
        "\n",
        "# Increase the width of the graph\n",
        "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust the figsize as needed\n",
        "\n",
        "# Increase spacing between bars\n",
        "bar_width = 0.6  # Adjust the width as needed\n",
        "bar_positions = range(len(labels))\n",
        "\n",
        "# Create a bar graph\n",
        "#bars = plt.bar(bar_positions, values, width=bar_width, color='blue')\n",
        "bars = plt.bar(bar_positions, values, width=bar_width, color=['lightgreen']*3 + ['darkgreen']*6)  # Last 6 bars are darkgreen\n",
        "\n",
        "# Dynamically set y-axis limits\n",
        "plt.ylim(min(values*100) - 5, max(values) + 5)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Model Accuracies for Edge-IIoTset2023 dataset (note identical values for NN models)')\n",
        "\n",
        "# Set x-axis ticks and labels\n",
        "plt.xticks(bar_positions, labels)\n",
        "\n",
        "# Annotate each bar with its respective value\n",
        "for bar, value in zip(bars, values):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{value:.2f}%', ha='center', va='bottom')\n",
        "\n",
        "# Save the figure with 600dpi resolution to allow a high-quality image to be imported to a manuscript\n",
        "plt.savefig('model_accuracies.png', dpi=600)\n",
        "\n",
        "# Display the bar graph\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1GoZh0LVTv__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bar graph that shows the accuracy of the base classifiers and ensemble classifiers\n",
        "\n",
        "# Show the values that will be used in the graph\n",
        "print(f\"The following accuracy values will be used for visualization:\")\n",
        "print(f\"   GB       {accuracy_gb_optimized:.4f}\")\n",
        "print(f\"   DT       {accuracy_dt_optimized:.4f}\")\n",
        "print(f\"   RF       {accuracy_rf_optimized:.4f}\")\n",
        "print(f\"   MLP      {accuracy_mlp_optimized:.4f}\")\n",
        "print(f\"   FNN      {accuracy_sequential_optimized:.4f}\")\n",
        "print(f\"   LSTM     {accuracy_lstm_optimized:.4f}\")\n",
        "\n",
        "labels = [\"GB\", \"DT\", \"RF\", \"MLP\", \"FNN\", \"FNN-LSTM\", \"RNN\", \"RNN-LSTM\", \"GRU\"]\n",
        "#values = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, accuracy_mlp_optimized*100, accuracy_sequential_optimized*100, accuracy_lstm_optimized*100, accuracy_simplernn_optimized*100, accuracy_simplernn_lstm_optimized*100, accuracy_gru_optimized*100]\n",
        "values = [accuracy_gb_optimized*100, accuracy_dt_optimized*100, accuracy_rf_optimized*100, accuracy_mlp_optimized*100, test_accuracy_sequential_optimized*100, test_accuracy_lstm_optimized*100, test_accuracy_simplernn_optimized*100, test_accuracy_simplernn_lstm_optimized*100,test_accuracy_gru_optimized*100]\n",
        "\n",
        "\n",
        "# Increase the width of the graph\n",
        "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust the figsize as needed\n",
        "\n",
        "# Increase spacing between bars\n",
        "bar_width = 0.6  # Adjust the width as needed\n",
        "bar_positions = range(len(labels))\n",
        "\n",
        "# Create a bar graph\n",
        "#bars = plt.bar(bar_positions, values, width=bar_width, color='blue')\n",
        "bars = plt.bar(bar_positions, values, width=bar_width, color=['lightgreen']*3 + ['darkgreen']*6)  # Last 6 bars are darkgreen\n",
        "\n",
        "# Dynamically set y-axis limits\n",
        "plt.ylim(min(values*100) - 5, max(values) + 5)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Model Accuracies for Edge-IIoTset2023 dataset (using test_accuracy)')\n",
        "\n",
        "# Set x-axis ticks and labels\n",
        "plt.xticks(bar_positions, labels)\n",
        "\n",
        "# Annotate each bar with its respective value\n",
        "for bar, value in zip(bars, values):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{value:.2f}%', ha='center', va='bottom')\n",
        "\n",
        "# Save the figure with 600dpi resolution to allow a high-quality image to be imported to a manuscript\n",
        "plt.savefig('model_accuracies.png', dpi=600)\n",
        "\n",
        "# Display the bar graph\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "phPUgmeIGAjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lbu7aAbKVBb"
      },
      "outputs": [],
      "source": [
        "# show a running total of elapsed time for the entire notebook\n",
        "show_elapsed_time()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}